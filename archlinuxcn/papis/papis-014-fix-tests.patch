diff --git a/papis/bibtex.py b/papis/bibtex.py
index 761a64a9..d6abd889 100644
--- a/papis/bibtex.py
+++ b/papis/bibtex.py
@@ -512,7 +512,8 @@ def create_reference(doc: Dict[str, Any], force: bool = False) -> str:
     return ref_cleanup(ref)
 
 
-def author_list_to_author(author_list: List[Dict[str, Any]]) -> str:
+def author_list_to_author(doc: papis.document.Document,
+                          author_list: List[Dict[str, Any]]) -> str:
     if not author_list:
         return ""
 
@@ -520,6 +521,11 @@ def author_list_to_author(author_list: List[Dict[str, Any]]) -> str:
     fmt = "{au[family]}, {au[given]}"
 
     for author in author_list:
+        if not isinstance(author, dict):
+            logger.error("Incorrect 'author_list' type (author is not a 'dict'): %s",
+                         papis.document.describe(doc))
+            continue
+
         family = author.get("family")
         given = author.get("given")
         if family and given:
@@ -624,7 +630,7 @@ def to_bibtex(document: papis.document.Document, *, indent: int = 2) -> str:
                     "'journal-key' key '%s' is not present for ref '%s'.",
                     journal_key, document["ref"])
         elif bib_key == "author" and "author_list" in document:
-            bib_value = author_list_to_author(document["author_list"])
+            bib_value = author_list_to_author(document, document["author_list"])
 
         override_key = f"{bib_key}_latex"
         if override_key in document:
diff --git a/papis/commands/add.py b/papis/commands/add.py
index 75a66e72..9a645ece 100644
--- a/papis/commands/add.py
+++ b/papis/commands/add.py
@@ -283,16 +283,24 @@ def run(paths: List[str],
     # rename all the given file names
     from papis.paths import symlink, rename_document_files
 
-    new_file_list = rename_document_files(tmp_document, in_document_paths)
+    renamed_file_list = rename_document_files(tmp_document, in_document_paths)
 
     import shutil
 
-    for in_file_path, out_file_name in zip(in_document_paths, new_file_list):
+    document_file_list = []
+    for in_file_path, out_file_name in zip(in_document_paths, renamed_file_list):
         out_file_path = os.path.join(temp_dir, out_file_name)
         if os.path.exists(out_file_path):
             logger.warning("File '%s' already exists. Skipping...", out_file_path)
             continue
 
+        if not batch and open_file:
+            papis.utils.open_file(in_file_path)
+
+        if not batch and confirm and not papis.tui.utils.confirm(
+                f"Add file '{os.path.basename(in_file_path)}' to document?"):
+            continue
+
         if link:
             logger.info("[LN] '%s' to '%s'.", in_file_path, out_file_name)
             symlink(in_file_path, out_file_path)
@@ -303,7 +311,9 @@ def run(paths: List[str],
             logger.info("[CP] '%s' to '%s'.", in_file_path, out_file_name)
             shutil.copy(in_file_path, out_file_path)
 
-    tmp_document["files"] = new_file_list
+        document_file_list.append(out_file_name)
+
+    tmp_document["files"] = document_file_list
     tmp_document.save()
 
     from papis.paths import get_document_unique_folder
@@ -314,7 +324,7 @@ def run(paths: List[str],
         folder_name_format=folder_name)
 
     logger.info("Document folder is '%s'.", out_folder_path)
-    logger.debug("Document includes files: '%s'.", "', '".join(in_document_paths))
+    logger.debug("Document includes files: '%s'.", "', '".join(document_file_list))
 
     # Check if the user wants to edit before submitting the doc
     # to the library
@@ -371,10 +381,6 @@ def run(paths: List[str],
             title=f"This{dup_text}document will be added to your library",
             lexer_name="yaml")
 
-    if open_file:
-        for d_path in tmp_document.get_files():
-            papis.utils.open_file(d_path)
-
     if confirm:
         if not papis.tui.utils.confirm("Do you want to add the new document?"):
             return
@@ -441,7 +447,7 @@ def run(paths: List[str],
     default=lambda: papis.config.getboolean("add-confirm"))
 @papis.cli.bool_flag(
     "--open/--no-open", "open_file",
-    help="Open file before adding document",
+    help="Open files before adding them to the document",
     default=lambda: papis.config.getboolean("add-open"))
 @papis.cli.bool_flag(
     "--edit/--no-edit",
@@ -521,7 +527,8 @@ def cli(files: List[str],
 
     # merge importer data + commandline data into a single set
     imported = papis.utils.collect_importer_data(
-        matching_importers, batch=batch, use_files=download_files)
+        matching_importers, batch=batch, use_files=download_files
+    )
 
     ctx = papis.importer.Context()
     ctx.data = imported.data
diff --git a/papis/commands/default.py b/papis/commands/default.py
index e1d0f5ed..9a0a5b6c 100644
--- a/papis/commands/default.py
+++ b/papis/commands/default.py
@@ -83,16 +83,15 @@ class ScriptLoaderGroup(click.Group):
             matches = list(map(
                 str, difflib.get_close_matches(name, self.scripts, n=2)))
 
-            if matches:
-                click.echo("Command '{name}' is unknown! Did you mean '{matches}'?"
-                           .format(name=name, matches="' or '".join(matches)))
-            else:
-                click.echo(f"Command '{name}' is unknown!")
-
-            # return the match if there was only one match
+            click.echo("Command '{name}' is unknown!".format(name=name))
             if len(matches) == 1:
+                # return the match if there was only one match
                 click.echo(f"I suppose you meant: '{matches[0]}'")
                 script = self.scripts[matches[0]]
+            elif matches:
+                click.echo("Did you mean '{matches}'?"
+                           .format(matches="' or '".join(matches)))
+                return None
             else:
                 return None
 
diff --git a/papis/commands/update.py b/papis/commands/update.py
index 86d0fd17..f0b7d9cd 100644
--- a/papis/commands/update.py
+++ b/papis/commands/update.py
@@ -531,7 +531,7 @@ def cli(
                             matching_importers.append(importer)
 
             imported = papis.utils.collect_importer_data(
-                matching_importers, batch=batch, only_data=True
+                matching_importers, batch=batch, use_files=False
             )
             if "ref" in imported.data:
                 logger.debug(
diff --git a/papis/config.py b/papis/config.py
index 23622059..c800a367 100644
--- a/papis/config.py
+++ b/papis/config.py
@@ -134,7 +134,12 @@ class Configuration(configparser.ConfigParser):
         configpy = get_configpy_file()
         if os.path.exists(configpy):
             with open(configpy) as fd:
-                exec(fd.read())
+                # NOTE: this includes the `globals()` so that the user config.py
+                # can add entries to the global namespace. This was motivated
+                # by adding filters to `Jinja2Formatter.env`, which may be separated
+                # into multiple functions that would not be found otherwise.
+                #   https://github.com/papis/papis/pull/930
+                exec(fd.read(), globals())
 
 
 def get_default_settings() -> PapisConfigType:
diff --git a/papis/downloaders/__init__.py b/papis/downloaders/__init__.py
index 46355cc1..1ab63956 100644
--- a/papis/downloaders/__init__.py
+++ b/papis/downloaders/__init__.py
@@ -12,6 +12,7 @@ import papis.logging
 
 if TYPE_CHECKING:
     import bs4
+    import requests
 
 logger = papis.logging.get_logger(__name__)
 
@@ -99,6 +100,7 @@ class Downloader(papis.importer.Importer):
         # NOTE: used to cache data
         self._soup: Optional[bs4.BeautifulSoup] = None
         self.bibtex_data: Optional[str] = None
+        self.document_filename: Optional[str] = None
         self.document_data: Optional[bytes] = None
         self.document_extension: Optional[str] = None
 
@@ -205,18 +207,29 @@ class Downloader(papis.importer.Importer):
         except NotImplementedError:
             pass
         else:
-            doc_rawdata = self.get_document_data()
-            if doc_rawdata and self.check_document_format():
-                extension = self.get_document_extension()
-                if extension:
-                    extension = f".{extension}"
-
-                with tempfile.NamedTemporaryFile(
-                        mode="wb+", delete=False,
-                        suffix=extension) as f:
-                    f.write(doc_rawdata)
-                    self.logger.info("Saving downloaded file in '%s'.", f.name)
-                    self.ctx.files.append(f.name)
+            rawdata = self.get_document_data()
+            if rawdata and self.check_document_format():
+                if self.document_filename:
+                    from papis.paths import _make_unique_file
+
+                    filename = _make_unique_file(
+                        os.path.join(tempfile.gettempdir(), self.document_filename)
+                    )
+                    with open(filename, mode="wb") as f:
+                        f.write(rawdata)
+                else:
+                    extension = self.get_document_extension()
+                    if extension:
+                        extension = f".{extension}"
+
+                    with tempfile.NamedTemporaryFile(
+                            mode="wb+", delete=False,
+                            suffix=extension) as f:
+                        f.write(rawdata)
+                        filename = f.name
+
+                self.logger.info("Saving downloaded file in '%s'.", filename)
+                self.ctx.files.append(filename)
 
     def _get_body(self) -> bytes:
         """Download the content (body) at the given URL.
@@ -317,16 +330,22 @@ class Downloader(papis.importer.Importer):
             uses magic file signatures to determine the type. If no guess is valid,
             an empty string is returned.
         """
+        ext = None
+
+        if self.document_filename is not None:
+            _, ext = os.path.splitext(self.document_filename)
+            if ext.startswith("."):
+                ext = ext[1:]
+
+            self.document_extension = ext
+
         if self.document_extension is None:
             data = self.get_document_data()
-            document_extension = None
             if data is not None:
                 from papis.filetype import guess_content_extension
-                document_extension = guess_content_extension(data)
+                ext = guess_content_extension(data)
 
-            self.document_extension = (
-                document_extension if document_extension is not None else ""
-            )
+            self.document_extension = ext if ext is not None else ""
 
         return self.document_extension
 
@@ -341,6 +360,7 @@ class Downloader(papis.importer.Importer):
         self.logger.info("Downloading file from '%s'.", url)
 
         response = self.session.get(url, cookies=self.cookies)
+        self.document_filename = _get_filename_from_response(response)
         self.document_data = response.content
 
     def check_document_format(self) -> bool:
@@ -436,6 +456,43 @@ def get_info_from_url(
     return down.ctx
 
 
+def _get_filename_from_response(response: "requests.Response") -> Optional[str]:
+    filename = None
+
+    # NOTE: we can guess the filename from the response headers
+    #   Content-Disposition: inline; filename="some_file_name.ext"
+    #   Content-Disposition: attachement; filename="some_file_name.ext"
+    key = "Content-Disposition"
+    if not filename and key in response.headers:
+        from email.message import EmailMessage
+
+        msg = EmailMessage()
+        msg[key] = response.headers[key]
+        filename = msg.get_filename()
+
+    key = "Content-Type"
+    if not filename and key in response.headers:
+        from email.message import EmailMessage
+
+        msg = EmailMessage()
+        msg[key] = response.headers[key]
+
+        from mimetypes import guess_extension
+
+        ext = guess_extension(msg.get_content_type())
+
+        from urllib.parse import urlsplit
+
+        result = urlsplit(response.url)
+        if result.path.strip("/"):
+            basename = os.path.basename(result.path)
+        else:
+            basename = result.netloc
+        filename = f"{basename}{ext}"
+
+    return filename
+
+
 def download_document(
         url: str,
         expected_document_extension: Optional[str] = None,
@@ -471,36 +528,8 @@ def download_document(
                      url, response.reason, response.status_code)
         return None
 
-    # NOTE: we can guess the filename from the response headers
-    #   Content-Disposition: inline; filename="some_file_name.ext"
-    #   Content-Disposition: attachement; filename="some_file_name.ext"
-    key = "Content-Disposition"
-    if not filename and key in response.headers:
-        from email.message import EmailMessage
-
-        msg = EmailMessage()
-        msg[key] = response.headers[key]
-        filename = msg.get_filename()
-
-    key = "Content-Type"
-    if not filename and key in response.headers:
-        from email.message import EmailMessage
-
-        msg = EmailMessage()
-        msg[key] = response.headers[key]
-
-        from mimetypes import guess_extension
-
-        ext = guess_extension(msg.get_content_type())
-
-        from urllib.parse import urlsplit
-
-        result = urlsplit(url)
-        if result.path.strip("/"):
-            basename = os.path.basename(result.path)
-        else:
-            basename = result.netloc
-        filename = f"{basename}{ext}"
+    if not filename:
+        filename = _get_filename_from_response(response)
 
     # try go guess an extension
     ext = expected_document_extension
diff --git a/papis/format.py b/papis/format.py
index 0308bef2..707a97ae 100644
--- a/papis/format.py
+++ b/papis/format.py
@@ -1,5 +1,5 @@
 import string
-from typing import Any, Dict, Optional
+from typing import Any, ClassVar, Dict, Optional
 
 import papis.config
 import papis.plugin
@@ -202,18 +202,36 @@ class Jinja2Formatter(Formatter):
         "{{ doc.isbn | default('ISBN-NONE', true) }}"
     """
 
+    env: ClassVar[Any] = None
+    """The ``jinja2`` Environment used by the formatter. This should be obtained
+    with :meth:`~Jinja2Formatter.get_environment()` (cached) and modified as
+    required (e.g. by adding filters).
+    """
+
     def __init__(self) -> None:
         super().__init__()
 
         try:
-            import jinja2       # noqa: F401
+            import jinja2  # noqa: F401
         except ImportError as exc:
             logger.error(
-                "The 'jinja2' formatter requires the 'jinja' library. "
+                "The 'jinja2' formatter requires the 'jinja2' library. "
                 "To use this functionality install it using e.g. "
                 "'pip install jinja2'.", exc_info=exc)
             raise exc
 
+    @classmethod
+    def get_environment(cls, *, force: bool = False) -> Any:
+        if cls.env is None or force:
+            from jinja2 import Environment
+
+            # NOTE: this will kindly autoescape apostrophes otherwise
+            env = Environment(autoescape=False)
+            env.shared = True
+            cls.env = env
+
+        return cls.env
+
     def format(self,
                fmt: str,
                doc: papis.document.DocumentLike,
@@ -223,15 +241,15 @@ class Jinja2Formatter(Formatter):
         if additional is None:
             additional = {}
 
-        from jinja2 import Template
-
         fmt = unescape(fmt)
         if not isinstance(doc, papis.document.Document):
             doc = papis.document.from_data(doc)
 
         doc_name = doc_key or self.default_doc_name
+        env = self.get_environment()
+
         try:
-            return str(Template(fmt).render(**{doc_name: doc}, **additional))
+            return str(env.from_string(fmt).render(**{doc_name: doc}, **additional))
         except Exception as exc:
             if default is not None:
                 logger.warning("Could not format string '%s' for document '%s'",
diff --git a/papis/paths.py b/papis/paths.py
index 9ca29666..4fe073e0 100644
--- a/papis/paths.py
+++ b/papis/paths.py
@@ -296,6 +296,20 @@ def _make_unique_folder(out_folder_path: PathLike) -> str:
     return out_folder_path_suffix
 
 
+def _make_unique_file(filename: PathLike) -> str:
+    if not os.path.exists(filename):
+        return str(filename)
+
+    suffix = unique_suffixes()
+    basename, ext = os.path.splitext(filename)
+
+    out_file_name = f"{basename}-{next(suffix)}{ext}"
+    while os.path.exists(out_file_name):
+        out_file_name = f"{basename}-{next(suffix)}{ext}"
+
+    return out_file_name
+
+
 def get_document_unique_folder(
         doc: DocumentLike,
         dirname: PathLike, *,
diff --git a/papis/utils.py b/papis/utils.py
index dad25e04..65acd06c 100644
--- a/papis/utils.py
+++ b/papis/utils.py
@@ -518,12 +518,11 @@ def collect_importer_data(
     only do the aggregation.
 
     :param batch: if *True*, overwrite data from previous importers, otherwise
-        ask the user to manually merge.
+        ask the user to manually merge. Note that files are always kept, even
+        if they potentially contain duplicates.
     :param use_files: if *True*, both metadata and files are collected
         from the importers.
     """
-    from papis.tui.utils import confirm
-
     # FIXME: this is here for backwards compatibility and should be removed
     # before we release the next version
     if only_data is not None and use_files is not None:
@@ -559,11 +558,7 @@ def collect_importer_data(
                         importer.name,
                         "\n\t".join(importer.ctx.files))
 
-            msg = f"Use this file? (from {importer.name})"
-            for f in importer.ctx.files:
-                open_file(f)
-                if batch or confirm(msg):
-                    ctx.files.append(f)
+            ctx.files.extend(importer.ctx.files)
 
     return ctx
 
diff --git a/papis/web/epubjs.py b/papis/web/epubjs.py
index 2adeaae4..7f449aea 100644
--- a/papis/web/epubjs.py
+++ b/papis/web/epubjs.py
@@ -1,15 +1,45 @@
 import os.path
+import urllib.parse
 
 import dominate.tags as t
 
 import papis.web.html as wh
 import papis.web.static
 
-VIEWER_PATH = "epubjs-reader/reader/index.html"
 EPUBJS_URL = "https://github.com/futurepress/epubjs-reader"
+VIEWER_PATH = "epubjs-reader/reader/index.html"
+
+
+def widget(unquoted_file_path: str) -> None:
+    """
+    Widget for epub files.
+    """
+
+    _file_path = urllib.parse.quote(unquoted_file_path, safe="")
 
+    viewer_path = f"/static/{VIEWER_PATH}?bookPath={_file_path}"
+
+    with wh.flex("center"):
+        with t.div(cls="btn-group", role="group"):
+            with t.a(href=viewer_path,
+                     cls="btn btn-outline-success",
+                     target="_blank"):
+                wh.icon_span("square-arrow-up-right", "Open in new window")
+            with t.a(href=unquoted_file_path,
+                     cls="btn btn-outline-success",
+                     target="_blank"):
+                wh.icon_span("download", "Download")
+
+    if detect_epubjs():
+        t.iframe(src=viewer_path,
+                 style="resize: vertical",
+                 width="100%",
+                 height="800")
+    else:
+        t.pre(error_message(), cls="alert alert-warning")
 
-def detect_local_installation() -> bool:
+
+def detect_epubjs() -> bool:
     for path in papis.web.static.static_paths():
         viewer = os.path.join(path, VIEWER_PATH)
         if os.path.exists(viewer):
@@ -37,30 +67,3 @@ On linux and mac you can simply run the following lines
     mkdir -p ~/.config/papis/web/
     git clone {EPUBJS_URL} ~/.config/papis/web/epubjs-reader
     """
-
-
-def widget(unquoted_file_path: str) -> None:
-    """
-    Widget for epub files.
-    """
-
-    viewer_path = f"/static/{VIEWER_PATH}?bookPath={unquoted_file_path}"
-
-    with wh.flex("center"):
-        with t.div(cls="btn-group", role="group"):
-            with t.a(href=viewer_path,
-                     cls="btn btn-outline-success",
-                     target="_blank"):
-                wh.icon_span("square-arrow-up-right", "Open in new window")
-            with t.a(href=unquoted_file_path,
-                     cls="btn btn-outline-success",
-                     target="_blank"):
-                wh.icon_span("download", "Download")
-
-    if detect_local_installation():
-        t.iframe(src=viewer_path,
-                 style="resize: vertical",
-                 width="100%",
-                 height="800")
-    else:
-        t.pre(error_message(), cls="alert alert-warning")
diff --git a/papis/web/pdfjs.py b/papis/web/pdfjs.py
index 715ada80..cee31976 100644
--- a/papis/web/pdfjs.py
+++ b/papis/web/pdfjs.py
@@ -1,4 +1,4 @@
-import os
+import os.path
 import urllib.parse
 
 import dominate.tags as t
@@ -11,8 +11,12 @@ PDFJS_URL = ("https://github.com/mozilla/pdf.js/releases/download/"
 VIEWER_PATH = "pdfjs/web/viewer.html"
 
 
-def widget(_unquoted_file_path: str) -> None:
-    _file_path = urllib.parse.quote(_unquoted_file_path, safe="")
+def widget(unquoted_file_path: str) -> None:
+    """
+    Widget for pdf files.
+    """
+
+    _file_path = urllib.parse.quote(unquoted_file_path, safe="")
 
     viewer_path = (f"/static/{VIEWER_PATH}?file={_file_path}")
 
@@ -21,22 +25,19 @@ def widget(_unquoted_file_path: str) -> None:
             with t.a(href=viewer_path,
                      cls="btn btn-outline-success",
                      target="_blank"):
-                wh.icon_span(
-                    "square-arrow-up-right",
-                    "Open in new window")
-            with t.a(href=_unquoted_file_path,
+                wh.icon_span("square-arrow-up-right", "Open in new window")
+            with t.a(href=unquoted_file_path,
                      cls="btn btn-outline-success",
                      target="_blank"):
-                wh.icon_span("download",
-                             "Download")
+                wh.icon_span("download", "Download")
+
     if detect_pdfjs():
         t.iframe(src=viewer_path,
                  style="resize: vertical",
                  width="100%",
                  height="800")
     else:
-        t.pre(papis.web.pdfjs.error_message(),
-              cls="alert alert-warning")
+        t.pre(error_message(), cls="alert alert-warning")
 
 
 def detect_pdfjs() -> bool:
@@ -48,6 +49,7 @@ def detect_pdfjs() -> bool:
 
 
 def error_message() -> str:
+    """Error message for when there is no pdfjs installation"""
     return f"""
 No installation of pdfjs found.
 
diff --git a/papis/zenodo.py b/papis/zenodo.py
index c487e804..fea57dc0 100644
--- a/papis/zenodo.py
+++ b/papis/zenodo.py
@@ -31,14 +31,12 @@ def get_author_info(authors: List[Dict[str, str]]) -> List[Dict[str, str]]:
     return out
 
 
-def get_text_from_html(html: str) -> str:
-    """
-    Processes the HTML and returns it in markdown format if a dependency is found,
-    or raw HTML otherwise.
+def _get_text_from_html(html: str) -> str:
+    # NOTE: this needs to be a separate function for testing purposes. It is
+    # getting monkeypatched in `test_zenodo`, which isn't possible with
+    # `get_text_from_html` because it's already bound to variables in
+    # the global `key_conversion` mapping below.
 
-    :param html: The raw HTML as embedded in the incoming Zenodo JSON data.
-    :return: Either the raw HTML as text, or the markdown-annotated plain text.
-    """
     try:
         import markdownify
     except ImportError:
@@ -51,11 +49,24 @@ def get_text_from_html(html: str) -> str:
     if hasattr(markdownify.MarkdownConverter.Options, "escape_misc"):
         # NOTE: markdownify 0.13 introduced some more escaping that we do not
         # want for now, so we turn it off!
+        # NOTE: this is set to False by default in 0.14
         options["escape_misc"] = False
 
     return str(markdownify.markdownify(html, **options))
 
 
+def get_text_from_html(html: str) -> str:
+    """
+    Processes the HTML and returns it in markdown format if a dependency is found,
+    or raw HTML otherwise.
+
+    :param html: The raw HTML as embedded in the incoming Zenodo JSON data.
+    :return: Either the raw HTML as text, or the markdown-annotated plain text.
+    """
+
+    return _get_text_from_html(html)
+
+
 KeyConversionPair = papis.document.KeyConversionPair
 key_conversion = [
     # Fields from biblatex-software and biblatex docs
diff --git a/tests/commands/test_add.py b/tests/commands/test_add.py
index 0c86eca1..9cb8d66b 100644
--- a/tests/commands/test_add.py
+++ b/tests/commands/test_add.py
@@ -1,5 +1,6 @@
 import os
 import pytest
+import shutil
 import sys
 
 import papis.config
@@ -31,6 +32,9 @@ def make_document(name: str, dir: str, nfiles: int = 0) -> papis.document.Docume
     return doc
 
 
+@pytest.mark.skipif(
+    not shutil.which("git"),
+    reason="Test requires 'git' executable to be in the PATH")
 @pytest.mark.library_setup(use_git=True)
 def test_add_run(tmp_library: TemporaryLibrary, nfiles: int = 5) -> None:
     from papis.commands.add import run
diff --git a/tests/commands/test_git.py b/tests/commands/test_git.py
index b9bf31e2..39cd518e 100644
--- a/tests/commands/test_git.py
+++ b/tests/commands/test_git.py
@@ -1,10 +1,16 @@
 import os
+import shutil
+
+import pytest
 
 import papis.config
 
 from papis.testing import TemporaryLibrary, PapisRunner
 
 
+@pytest.mark.skipif(
+    not shutil.which("git"),
+    reason="Test requires 'git' executable to be in the PATH")
 def test_git_cli(tmp_library: TemporaryLibrary) -> None:
     from papis.commands.git import cli
     cli_runner = PapisRunner()
diff --git a/tests/commands/test_rm.py b/tests/commands/test_rm.py
index af84773e..28ac2f6e 100644
--- a/tests/commands/test_rm.py
+++ b/tests/commands/test_rm.py
@@ -1,5 +1,6 @@
 import os
 import pytest
+import shutil
 from _pytest.monkeypatch import MonkeyPatch
 
 import papis.database
@@ -19,6 +20,9 @@ def test_rm_run(tmp_library: TemporaryLibrary) -> None:
     assert not os.path.exists(folder)
 
 
+@pytest.mark.skipif(
+    not shutil.which("git"),
+    reason="Test requires 'git' executable to be in the PATH")
 @pytest.mark.library_setup(use_git=True)
 def test_rm_files_run(tmp_library: TemporaryLibrary) -> None:
     from papis.commands.rm import run
diff --git a/tests/resources/zenodo/10794563.json b/tests/resources/zenodo/10794563.json
index ec9026fc..db994757 100644
--- a/tests/resources/zenodo/10794563.json
+++ b/tests/resources/zenodo/10794563.json
@@ -1 +1 @@
-{"created": "2024-03-07T16:52:48.717997+00:00", "modified": "2024-03-07T16:52:49.174340+00:00", "id": 10794563, "conceptrecid": "10794562", "doi": "10.5061/dryad.66t1g1k7m", "doi_url": "https://doi.org/10.5061/dryad.66t1g1k7m", "metadata": {"title": "Directional epistasis is common in morphological divergence", "doi": "10.5061/dryad.66t1g1k7m", "publication_date": "2024-03-07", "description": "<p>Epistasis is often portrayed as unimportant in evolution. While random patterns of epistasis may have limited effects on the response to selection, systematic directional epistasis can have substantial effects on evolutionary dynamics. Directional epistasis occurs when allelic substitutions that change a trait also modify the effects of allelic substitutions at other loci in a systematic direction. In this case, trait evolution may induce correlated changes in allelic effects and effective genetic variance (evolvability) that modify further evolution. Although theory thus suggests a potentially important role for directional epistasis in evolution, we still lack empirical evidence about its prevalence and magnitude. Using a new framework to estimate systematic patterns of epistasis from line-crosses experiments, we quantify its effects on 197 size-related traits from diverging natural populations in 24 animal and 17 plant species. We show that directional epistasis is common and tends to become stronger with increasing morphological divergence. In animals, most traits displayed negative directionality toward larger size, suggesting that epistasis constraints reducing evolvability toward larger size may be common. Dominance was also common and did not systematically alter the effects of epistasis.</p>", "access_right": "open", "creators": [{"name": "Bourg, Salom\u00e9", "affiliation": "Norwegian University of Science and Technology", "orcid": "0000-0003-1215-0308"}, {"name": "Bolstad, Geir H.", "affiliation": "Norwegian Institute for Nature Research"}, {"name": "Griffin, Donald V.", "affiliation": "Florida State University"}, {"name": "P\u00e9labon, Christophe", "affiliation": "Norwegian University of Science and Technology"}, {"name": "Hansen, Thomas F.", "affiliation": "University of Oslo"}], "keywords": ["linecross", "Epistasis", "Genetic dominance", "evolvability", "Evolutionary biology"], "related_identifiers": [{"identifier": "https://github.com/salomebo/linecross", "relation": "isDerivedFrom", "scheme": "url"}, {"identifier": "https://github.com/GHBolstad/lmmHeterosced", "relation": "isDerivedFrom", "scheme": "url"}], "resource_type": {"title": "Dataset", "type": "dataset"}, "license": {"id": "cc-zero"}, "communities": [{"id": "dryad"}], "relations": {"version": [{"index": 0, "is_last": true, "parent": {"pid_type": "recid", "pid_value": "10794562"}}]}, "notes": "<p>Funding provided by: Norwegian research council (RCN)*<br>Crossref Funder Registry ID: <br>Award Number: 287214</p><p>Funding provided by: Centre for Advanced Study<br>Crossref Funder Registry ID: https://ror.org/05rbhea42<br>Award Number: 223257</p>", "method": "<p>The data were collected through a literature search.</p>"}, "title": "Directional epistasis is common in morphological divergence", "links": {"self": "https://zenodo.org/api/records/10794563", "self_html": "https://zenodo.org/records/10794563", "self_doi": "https://zenodo.org/doi/10.5061/dryad.66t1g1k7m", "doi": "https://doi.org/10.5061/dryad.66t1g1k7m", "parent": "https://zenodo.org/api/records/10794562", "parent_html": "https://zenodo.org/records/10794562", "parent_doi": "https://zenodo.org/doi/", "self_iiif_manifest": "https://zenodo.org/api/iiif/record:10794563/manifest", "self_iiif_sequence": "https://zenodo.org/api/iiif/record:10794563/sequence/default", "files": "https://zenodo.org/api/records/10794563/files", "media_files": "https://zenodo.org/api/records/10794563/media-files", "archive": "https://zenodo.org/api/records/10794563/files-archive", "archive_media": "https://zenodo.org/api/records/10794563/media-files-archive", "latest": "https://zenodo.org/api/records/10794563/versions/latest", "latest_html": "https://zenodo.org/records/10794563/latest", "draft": "https://zenodo.org/api/records/10794563/draft", "versions": "https://zenodo.org/api/records/10794563/versions", "access_links": "https://zenodo.org/api/records/10794563/access/links", "access_users": "https://zenodo.org/api/records/10794563/access/users", "access_request": "https://zenodo.org/api/records/10794563/access/request", "access": "https://zenodo.org/api/records/10794563/access", "reserve_doi": "https://zenodo.org/api/records/10794563/draft/pids/doi", "communities": "https://zenodo.org/api/records/10794563/communities", "communities-suggestions": "https://zenodo.org/api/records/10794563/communities-suggestions", "requests": "https://zenodo.org/api/records/10794563/requests"}, "updated": "2024-03-07T16:52:49.174340+00:00", "recid": "10794563", "revision": 5, "files": [{"id": "c0d9f0f6-9f89-45f1-a1ce-6970cf8c167c", "key": "Paperlist_-_epistasis.xls", "size": 63488, "checksum": "md5:7c7d75c3d74c076b6a7aa13da9724eb0", "links": {"self": "https://zenodo.org/api/records/10794563/files/Paperlist_-_epistasis.xls/content"}}, {"id": "41eb3479-7886-405c-aa48-e431fd8dd3a6", "key": "Full_list_of_studies_-_epistasis.xls", "size": 252416, "checksum": "md5:b4dbcbe8680d8e949c740beeb7980df7", "links": {"self": "https://zenodo.org/api/records/10794563/files/Full_list_of_studies_-_epistasis.xls/content"}}, {"id": "6a42b381-8a83-4a72-9bac-f1105d4054b1", "key": "Dataset_-_epistasis.xls", "size": 189952, "checksum": "md5:1ff3a19124855ac6c3b0928d40402498", "links": {"self": "https://zenodo.org/api/records/10794563/files/Dataset_-_epistasis.xls/content"}}, {"id": "de14cab0-372c-4225-bfa1-876d0c4fa37d", "key": "README.md", "size": 6497, "checksum": "md5:d3c74e630aae2642ba3c3f817e8c1d58", "links": {"self": "https://zenodo.org/api/records/10794563/files/README.md/content"}}, {"id": "c2f77dc4-0c9c-4841-b173-46bf29963217", "key": "Reciprocal_line-cross_explanation.pdf", "size": 21569, "checksum": "md5:c5e2465c0536db2eac8b1a73c753cacf", "links": {"self": "https://zenodo.org/api/records/10794563/files/Reciprocal_line-cross_explanation.pdf/content"}}, {"id": "65ace00c-9177-4048-b61e-0fa11938c48f", "key": "AICs_-_epistasis.xls", "size": 67584, "checksum": "md5:8a2493cb17629b63e29150f04623b46f", "links": {"self": "https://zenodo.org/api/records/10794563/files/AICs_-_epistasis.xls/content"}}], "owners": [{"id": 90070}], "status": "published", "stats": {"downloads": 77, "unique_downloads": 51, "views": 32, "unique_views": 31, "version_downloads": 77, "version_unique_downloads": 51, "version_unique_views": 31, "version_views": 32}, "state": "done", "submitted": true}
\ No newline at end of file
+{"created": "2024-03-07T16:52:48.717997+00:00", "modified": "2024-07-06T23:12:57.867604+00:00", "id": 10794563, "conceptrecid": "10794562", "doi": "10.5061/dryad.66t1g1k7m", "doi_url": "https://doi.org/10.5061/dryad.66t1g1k7m", "metadata": {"title": "Directional epistasis is common in morphological divergence", "doi": "10.5061/dryad.66t1g1k7m", "publication_date": "2024-03-07", "description": "<p>Epistasis is often portrayed as unimportant in evolution. While random patterns of epistasis may have limited effects on the response to selection, systematic directional epistasis can have substantial effects on evolutionary dynamics. Directional epistasis occurs when allelic substitutions that change a trait also modify the effects of allelic substitutions at other loci in a systematic direction. In this case, trait evolution may induce correlated changes in allelic effects and effective genetic variance (evolvability) that modify further evolution. Although theory thus suggests a potentially important role for directional epistasis in evolution, we still lack empirical evidence about its prevalence and magnitude. Using a new framework to estimate systematic patterns of epistasis from line-crosses experiments, we quantify its effects on 197 size-related traits from diverging natural populations in 24 animal and 17 plant species. We show that directional epistasis is common and tends to become stronger with increasing morphological divergence. In animals, most traits displayed negative directionality toward larger size, suggesting that epistasis constraints reducing evolvability toward larger size may be common. Dominance was also common and did not systematically alter the effects of epistasis.</p>", "access_right": "open", "creators": [{"name": "Bourg, Salom\u00e9", "affiliation": "Norwegian University of Science and Technology", "orcid": "0000-0003-1215-0308"}, {"name": "Bolstad, Geir H.", "affiliation": "Norwegian Institute for Nature Research"}, {"name": "Griffin, Donald V.", "affiliation": "Florida State University"}, {"name": "P\u00e9labon, Christophe", "affiliation": "Norwegian University of Science and Technology"}, {"name": "Hansen, Thomas F.", "affiliation": "University of Oslo"}], "keywords": ["linecross", "Epistasis", "Genetic dominance", "evolvability", "Evolutionary biology"], "related_identifiers": [{"identifier": "https://github.com/salomebo/linecross", "relation": "isDerivedFrom", "scheme": "url"}, {"identifier": "https://github.com/GHBolstad/lmmHeterosced", "relation": "isDerivedFrom", "scheme": "url"}], "resource_type": {"title": "Dataset", "type": "dataset"}, "license": {"id": "cc-zero"}, "communities": [{"id": "dryad"}], "relations": {"version": [{"index": 0, "is_last": true, "parent": {"pid_type": "recid", "pid_value": "10794562"}}]}, "notes": "<p>Funding provided by: Norwegian research council (RCN)*<br>Crossref Funder Registry ID: <br>Award Number: 287214</p><p>Funding provided by: Centre for Advanced Study<br>Crossref Funder Registry ID: https://ror.org/05rbhea42<br>Award Number: 223257</p>", "method": "<p>The data were collected through a literature search.</p>"}, "title": "Directional epistasis is common in morphological divergence", "links": {"self": "https://zenodo.org/api/records/10794563", "self_html": "https://zenodo.org/records/10794563", "doi": "https://doi.org/10.5061/dryad.66t1g1k7m", "self_doi": "https://doi.org/10.5061/dryad.66t1g1k7m", "self_doi_html": "https://zenodo.org/doi/10.5061/dryad.66t1g1k7m", "parent": "https://zenodo.org/api/records/10794562", "parent_html": "https://zenodo.org/records/10794562", "self_iiif_manifest": "https://zenodo.org/api/iiif/record:10794563/manifest", "self_iiif_sequence": "https://zenodo.org/api/iiif/record:10794563/sequence/default", "files": "https://zenodo.org/api/records/10794563/files", "media_files": "https://zenodo.org/api/records/10794563/media-files", "thumbnails": {"10": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^10,/0/default.jpg", "50": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^50,/0/default.jpg", "100": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^100,/0/default.jpg", "250": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^250,/0/default.jpg", "750": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^750,/0/default.jpg", "1200": "https://zenodo.org/api/iiif/record:10794563:Reciprocal_line-cross_explanation.pdf/full/^1200,/0/default.jpg"}, "archive": "https://zenodo.org/api/records/10794563/files-archive", "archive_media": "https://zenodo.org/api/records/10794563/media-files-archive", "latest": "https://zenodo.org/api/records/10794563/versions/latest", "latest_html": "https://zenodo.org/records/10794563/latest", "versions": "https://zenodo.org/api/records/10794563/versions", "draft": "https://zenodo.org/api/records/10794563/draft", "reserve_doi": "https://zenodo.org/api/records/10794563/draft/pids/doi", "access_links": "https://zenodo.org/api/records/10794563/access/links", "access_grants": "https://zenodo.org/api/records/10794563/access/grants", "access_users": "https://zenodo.org/api/records/10794563/access/users", "access_request": "https://zenodo.org/api/records/10794563/access/request", "access": "https://zenodo.org/api/records/10794563/access", "communities": "https://zenodo.org/api/records/10794563/communities", "communities-suggestions": "https://zenodo.org/api/records/10794563/communities-suggestions", "requests": "https://zenodo.org/api/records/10794563/requests"}, "updated": "2024-07-06T23:12:57.867604+00:00", "recid": "10794563", "revision": 7, "files": [{"id": "c0d9f0f6-9f89-45f1-a1ce-6970cf8c167c", "key": "Paperlist_-_epistasis.xls", "size": 63488, "checksum": "md5:7c7d75c3d74c076b6a7aa13da9724eb0", "links": {"self": "https://zenodo.org/api/records/10794563/files/Paperlist_-_epistasis.xls/content"}}, {"id": "41eb3479-7886-405c-aa48-e431fd8dd3a6", "key": "Full_list_of_studies_-_epistasis.xls", "size": 252416, "checksum": "md5:b4dbcbe8680d8e949c740beeb7980df7", "links": {"self": "https://zenodo.org/api/records/10794563/files/Full_list_of_studies_-_epistasis.xls/content"}}, {"id": "6a42b381-8a83-4a72-9bac-f1105d4054b1", "key": "Dataset_-_epistasis.xls", "size": 189952, "checksum": "md5:1ff3a19124855ac6c3b0928d40402498", "links": {"self": "https://zenodo.org/api/records/10794563/files/Dataset_-_epistasis.xls/content"}}, {"id": "de14cab0-372c-4225-bfa1-876d0c4fa37d", "key": "README.md", "size": 6497, "checksum": "md5:d3c74e630aae2642ba3c3f817e8c1d58", "links": {"self": "https://zenodo.org/api/records/10794563/files/README.md/content"}}, {"id": "c2f77dc4-0c9c-4841-b173-46bf29963217", "key": "Reciprocal_line-cross_explanation.pdf", "size": 21569, "checksum": "md5:c5e2465c0536db2eac8b1a73c753cacf", "links": {"self": "https://zenodo.org/api/records/10794563/files/Reciprocal_line-cross_explanation.pdf/content"}}, {"id": "65ace00c-9177-4048-b61e-0fa11938c48f", "key": "AICs_-_epistasis.xls", "size": 67584, "checksum": "md5:8a2493cb17629b63e29150f04623b46f", "links": {"self": "https://zenodo.org/api/records/10794563/files/AICs_-_epistasis.xls/content"}}], "owners": [{"id": "90070"}], "status": "published", "stats": {"downloads": 95, "unique_downloads": 69, "views": 87, "unique_views": 85, "version_downloads": 95, "version_unique_downloads": 69, "version_unique_views": 85, "version_views": 87}, "state": "done", "submitted": true}
\ No newline at end of file
diff --git a/tests/resources/zenodo/10794563_html_out.json b/tests/resources/zenodo/10794563_html_out.json
new file mode 100644
index 00000000..617802cd
--- /dev/null
+++ b/tests/resources/zenodo/10794563_html_out.json
@@ -0,0 +1,51 @@
+{
+  "abstract": "<p>Epistasis is often portrayed as unimportant in evolution. While random patterns of epistasis may have limited effects on the response to selection, systematic directional epistasis can have substantial effects on evolutionary dynamics. Directional epistasis occurs when allelic substitutions that change a trait also modify the effects of allelic substitutions at other loci in a systematic direction. In this case, trait evolution may induce correlated changes in allelic effects and effective genetic variance (evolvability) that modify further evolution. Although theory thus suggests a potentially important role for directional epistasis in evolution, we still lack empirical evidence about its prevalence and magnitude. Using a new framework to estimate systematic patterns of epistasis from line-crosses experiments, we quantify its effects on 197 size-related traits from diverging natural populations in 24 animal and 17 plant species. We show that directional epistasis is common and tends to become stronger with increasing morphological divergence. In animals, most traits displayed negative directionality toward larger size, suggesting that epistasis constraints reducing evolvability toward larger size may be common. Dominance was also common and did not systematically alter the effects of epistasis.</p>",
+  "author": "Bourg, Salomé and Bolstad, Geir H. and Griffin, Donald V. and Pélabon, Christophe and Hansen, Thomas F.",
+  "author_list": [
+    {
+      "affiliation": "Norwegian University of Science and Technology",
+      "family": "Bourg",
+      "given": "Salomé"
+    },
+    {
+      "affiliation": "Norwegian Institute for Nature Research",
+      "family": "Bolstad",
+      "given": "Geir H."
+    },
+    {
+      "affiliation": "Florida State University",
+      "family": "Griffin",
+      "given": "Donald V."
+    },
+    {
+      "affiliation": "Norwegian University of Science and Technology",
+      "family": "Pélabon",
+      "given": "Christophe"
+    },
+    {
+      "affiliation": "University of Oslo",
+      "family": "Hansen",
+      "given": "Thomas F."
+    }
+  ],
+  "day": 7,
+  "doi": "10.5061/dryad.66t1g1k7m",
+  "eprint": 10794563,
+  "license": "cc-zero",
+  "method": "<p>The data were collected through a literature search.</p>",
+  "month": 3,
+  "note": "<p>Funding provided by: Norwegian research council (RCN)*<br>Crossref Funder Registry ID: <br>Award Number: 287214</p><p>Funding provided by: Centre for Advanced Study<br>Crossref Funder Registry ID: https://ror.org/05rbhea42<br>Award Number: 223257</p>",
+  "pubstate": "published",
+  "revision": 7,
+  "tags": [
+    "linecross",
+    "Epistasis",
+    "Genetic dominance",
+    "evolvability",
+    "Evolutionary biology"
+  ],
+  "title": "Directional epistasis is common in morphological divergence",
+  "type": "dataset",
+  "url": "https://zenodo.org/api/records/10794563",
+  "year": 2024
+}
\ No newline at end of file
diff --git a/tests/resources/zenodo/10794563_out.json b/tests/resources/zenodo/10794563_out.json
index 8fe963a4..5d4fb7d9 100644
--- a/tests/resources/zenodo/10794563_out.json
+++ b/tests/resources/zenodo/10794563_out.json
@@ -36,7 +36,7 @@
   "month": 3,
   "note": "Funding provided by: Norwegian research council (RCN)\\*  \nCrossref Funder Registry ID:   \nAward Number: 287214\n\nFunding provided by: Centre for Advanced Study  \nCrossref Funder Registry ID: https://ror.org/05rbhea42  \nAward Number: 223257",
   "pubstate": "published",
-  "revision": 5,
+  "revision": 7,
   "tags": [
     "linecross",
     "Epistasis",
diff --git a/tests/resources/zenodo/10794563_pre_0_14_out.json b/tests/resources/zenodo/10794563_pre_0_14_out.json
new file mode 100644
index 00000000..5ba0650c
--- /dev/null
+++ b/tests/resources/zenodo/10794563_pre_0_14_out.json
@@ -0,0 +1,51 @@
+{
+  "abstract": "Epistasis is often portrayed as unimportant in evolution. While random patterns of epistasis may have limited effects on the response to selection, systematic directional epistasis can have substantial effects on evolutionary dynamics. Directional epistasis occurs when allelic substitutions that change a trait also modify the effects of allelic substitutions at other loci in a systematic direction. In this case, trait evolution may induce correlated changes in allelic effects and effective genetic variance (evolvability) that modify further evolution. Although theory thus suggests a potentially important role for directional epistasis in evolution, we still lack empirical evidence about its prevalence and magnitude. Using a new framework to estimate systematic patterns of epistasis from line-crosses experiments, we quantify its effects on 197 size-related traits from diverging natural populations in 24 animal and 17 plant species. We show that directional epistasis is common and tends to become stronger with increasing morphological divergence. In animals, most traits displayed negative directionality toward larger size, suggesting that epistasis constraints reducing evolvability toward larger size may be common. Dominance was also common and did not systematically alter the effects of epistasis.",
+  "author": "Bourg, Salomé and Bolstad, Geir H. and Griffin, Donald V. and Pélabon, Christophe and Hansen, Thomas F.",
+  "author_list": [
+    {
+      "affiliation": "Norwegian University of Science and Technology",
+      "family": "Bourg",
+      "given": "Salomé"
+    },
+    {
+      "affiliation": "Norwegian Institute for Nature Research",
+      "family": "Bolstad",
+      "given": "Geir H."
+    },
+    {
+      "affiliation": "Florida State University",
+      "family": "Griffin",
+      "given": "Donald V."
+    },
+    {
+      "affiliation": "Norwegian University of Science and Technology",
+      "family": "Pélabon",
+      "given": "Christophe"
+    },
+    {
+      "affiliation": "University of Oslo",
+      "family": "Hansen",
+      "given": "Thomas F."
+    }
+  ],
+  "day": 7,
+  "doi": "10.5061/dryad.66t1g1k7m",
+  "eprint": 10794563,
+  "license": "cc-zero",
+  "method": "The data were collected through a literature search.",
+  "month": 3,
+  "note": "Funding provided by: Norwegian research council (RCN)\\*  \nCrossref Funder Registry ID:   \nAward Number: 287214\n\nFunding provided by: Centre for Advanced Study  \nCrossref Funder Registry ID: https://ror.org/05rbhea42  \nAward Number: 223257",
+  "pubstate": "published",
+  "revision": 7,
+  "tags": [
+    "linecross",
+    "Epistasis",
+    "Genetic dominance",
+    "evolvability",
+    "Evolutionary biology"
+  ],
+  "title": "Directional epistasis is common in morphological divergence",
+  "type": "dataset",
+  "url": "https://zenodo.org/api/records/10794563",
+  "year": 2024
+}
diff --git a/tests/resources/zenodo/7391177.json b/tests/resources/zenodo/7391177.json
index 1e34a549..64f5c6c8 100644
--- a/tests/resources/zenodo/7391177.json
+++ b/tests/resources/zenodo/7391177.json
@@ -1 +1 @@
-{"created": "2022-12-02T16:03:38.189470+00:00", "modified": "2022-12-03T02:26:43.995097+00:00", "id": 7391177, "conceptrecid": "3385997", "doi": "10.5281/zenodo.7391177", "conceptdoi": "10.5281/zenodo.3385997", "doi_url": "https://doi.org/10.5281/zenodo.7391177", "metadata": {"title": "Transformers: State-of-the-Art Natural Language Processing", "doi": "10.5281/zenodo.7391177", "publication_date": "2020-10-01", "description": "PyTorch 2.0 stack support\n<p>We are very excited by the newly announced PyTorch 2.0 stack. You can enable <code>torch.compile</code> on any of our models, and get support with the <code>Trainer</code> (and in all our PyTorch examples) by using the <code>torchdynamo</code> training argument. For instance, just add <code>--torchdynamo inductor</code> when launching those examples from the command line.</p>\n<p>This API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.</p>\n<p>Note that to get the best performance, we recommend:</p>\n<ul>\n<li>using an Ampere GPU (or more recent)</li>\n<li><p>sticking to fixed shaped for now (so use <code>--pad_to_max_length</code> in our examples)</p>\n</li>\n<li><p>Repurpose torchdynamo training args towards torch._dynamo  by @sgugger in #20498</p>\n</li>\n</ul>\nAudio Spectrogram Transformer\n<p>The Audio Spectrogram Transformer model was proposed in <a href=\"https://arxiv.org/abs/2104.01778\">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/vit\">Vision Transformer</a> to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.</p>\n<ul>\n<li>Add Audio Spectogram Transformer  by @NielsRogge in #19981</li>\n</ul>\nJukebox\n<p>The Jukebox model was proposed in <a href=\"https://arxiv.org/pdf/2005.00341.pdf\">Jukebox: A generative model for music</a> by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.</p>\n<ul>\n<li>Add Jukebox model (replaces #16875)  by @ArthurZucker in #17826</li>\n</ul>\nSwitch Transformers\n<p>The SwitchTransformers model was proposed in <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> by William Fedus, Barret Zoph, Noam Shazeer.</p>\n<p>It is the first MoE model supported in <code>transformers</code>, with the largest checkpoint currently available currently containing 1T parameters.</p>\n<ul>\n<li>Add Switch transformers  by @younesbelkada and @ArthurZucker  in #19323</li>\n</ul>\nRocBert\n<p>The RoCBert model was proposed in <a href=\"https://aclanthology.org/2022.acl-long.65.pdf\">RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</a> by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.</p>\n<ul>\n<li>Add RocBert  by @sww9370 in #20013</li>\n</ul>\nCLIPSeg\n<p>The CLIPSeg model was proposed in <a href=\"https://arxiv.org/abs/2112.10003\">Image Segmentation Using Text and Image Prompts</a> by Timo L\u00fcddecke and Alexander Ecker. <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/clip\">CLIP</a>Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.</p>\n<ul>\n<li>Add CLIPSeg  by @NielsRogge in #20066</li>\n</ul>\nNAT and DiNAT\nNAT\n<p>NAT was proposed in <a href=\"https://arxiv.org/abs/2204.07143\">Neighborhood Attention Transformer</a> by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.</p>\n<p>It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.</p>\nDiNAT\n<p>DiNAT was proposed in <a href=\"https://arxiv.org/abs/2209.15001\">Dilated Neighborhood Attention Transformer</a> by Ali Hassani and Humphrey Shi.</p>\n<p>It extends <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/nat\">NAT</a> by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.</p>\n<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models  by @alihassanijr in #20219</li>\n</ul>\nMobileNetV2\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1801.04381\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.</p>\n<ul>\n<li>add MobileNetV2 model  by @hollance in #17845</li>\n</ul>\nMobileNetV1\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1704.04861\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.</p>\n<ul>\n<li>add MobileNetV1 model  by @hollance in #17799</li>\n</ul>\nImage processors\n<p>Image processors replace feature extractors as the processing class for computer vision models.</p>\n<p>Important changes:</p>\n<ul>\n<li><code>size</code> parameter is now a dictionary of <code>{\"height\": h, \"width\": w}</code>, <code>{\"shortest_edge\": s}</code>, <code>{\"shortest_egde\": s, \"longest_edge\": l}</code> instead of int or tuple. </li>\n<li>Addition of <code>data_format</code> flag. You can now specify if you want your images to be returned in <code>\"channels_first\"</code> - NCHW - or <code>\"channels_last\"</code> - NHWC - format. </li>\n<li>Processing flags e.g. <code>do_resize</code> can be passed directly to the <code>preprocess</code> method instead of modifying the class attribute: <code>image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")</code></li>\n<li>Leaving <code>return_tensors</code> unset will return a list of numpy arrays.</li>\n</ul>\n<p>The classes are backwards compatible and can be created using existing feature extractor configurations - with the <code>size</code> parameter converted.</p>\n<ul>\n<li>Add Image Processors  by @amyeroberts in #19796</li>\n<li>Add Donut image processor by @amyeroberts #20425 </li>\n<li>Add segmentation + object detection image processors by @amyeroberts in #20160 </li>\n<li>AutoImageProcessor  by @amyeroberts in #20111</li>\n</ul>\nBackbone for computer vision models\n<p>We're adding support for a general <code>AutoBackbone</code> class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.</p>\n<ul>\n<li>Add AutoBackbone + ResNetBackbone  by @NielsRogge in #20229</li>\n<li>Improve backbone  by @NielsRogge in #20380</li>\n<li>[AutoBackbone] Improve API  by @NielsRogge in #20407</li>\n</ul>\nSupport for <code>safetensors</code> offloading\n<p>If the model you are using has a <code>safetensors</code> checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.</p>\n<ul>\n<li>Safetensors offload  by @sgugger in #20321</li>\n</ul>\nContrastive search in the <code>generate</code> method\n<ul>\n<li>Generate: TF contrastive search with XLA support  by @gante in #20050</li>\n<li>Generate: contrastive search with full optional outputs  by @gante in #19963</li>\n</ul>\nBreaking changes\n<ul>\n<li>\ud83d\udea8 \ud83d\udea8 \ud83d\udea8 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in <code>convert_tokens_to_string</code>  by @beneyal in #15775</li>\n</ul>\nBugfixes and improvements\n<ul>\n<li>add dataset  by @stevhliu in #20005</li>\n<li>Add BERT resources  by @stevhliu in #19852</li>\n<li>Add LayoutLMv3 resource  by @stevhliu in #19932</li>\n<li>fix typo  by @stevhliu in #20006</li>\n<li>Update object detection pipeline to use post_process_object_detection methods by @alaradirik in #20004</li>\n<li>clean up vision/text config dict arguments  by @ydshieh in #19954</li>\n<li>make sentencepiece import conditional in bertjapanesetokenizer  by @ripose-jp in #20012</li>\n<li>Fix gradient checkpoint test in encoder-decoder  by @ydshieh in #20017</li>\n<li>Quality  by @sgugger in #20002</li>\n<li>Update auto processor to check image processor created  by @amyeroberts in #20021</li>\n<li>[Doctest] Add configuration_deberta_v2.py  by @Saad135 in #19995</li>\n<li>Improve model tester  by @ydshieh in #19984</li>\n<li>Fix doctest  by @ydshieh in #20023</li>\n<li>Show installed libraries and their versions in CI jobs  by @ydshieh in #20026</li>\n<li>reorganize glossary  by @stevhliu in #20010</li>\n<li>Now supporting pathlike in pipelines too.  by @Narsil in #20030</li>\n<li>Add **kwargs  by @amyeroberts in #20037</li>\n<li>Fix some doctests after PR 15775  by @ydshieh in #20036</li>\n<li>[Doctest] Add configuration_camembert.py  by @Saad135 in #20039</li>\n<li>[Whisper Tokenizer] Make more user-friendly  by @sanchit-gandhi in #19921</li>\n<li>[FuturWarning] Add futur warning for LEDForSequenceClassification  by @ArthurZucker in #19066</li>\n<li>fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc  by @sywangyi in #19891</li>\n<li>Update esmfold conversion script  by @Rocketknight1 in #20028</li>\n<li>Fixed torch.finfo issue with torch.fx  by @michaelbenayoun in #20040</li>\n<li>Only resize embeddings when necessary  by @sgugger in #20043</li>\n<li>Speed up TF token classification postprocessing by converting complete tensors to numpy  by @deutschmn in #19976</li>\n<li>Fix ESM LM head test  by @Rocketknight1 in #20045</li>\n<li>Update README.md  by @bofenghuang in #20063</li>\n<li>fix <code>tokenizer_type</code> to avoid error when loading checkpoint back  by @pacman100 in #20062</li>\n<li>[Trainer] Fix model name in push_to_hub  by @sanchit-gandhi in #20064</li>\n<li>PoolformerImageProcessor defaults to match previous FE  by @amyeroberts in #20048</li>\n<li>change constant torch.tensor to torch.full  by @MerHS in #20061</li>\n<li>Update READMEs for ESMFold and add notebooks  by @Rocketknight1 in #20067</li>\n<li>Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2  by @jordiclive in #20068</li>\n<li>Allow passing arguments to model testers for CLIP-like models  by @ydshieh in #20044</li>\n<li>Show installed libraries and their versions in GA jobs  by @ydshieh in #20069</li>\n<li>Update defaults and logic to match old FE  by @amyeroberts in #20065</li>\n<li>Update modeling_tf_utils.py  by @cakiki in #20076</li>\n<li>Update hub.py  by @cakiki in #20075</li>\n<li>[Doctest] Add configuration_dpr.py  by @Saad135 in #20080</li>\n<li>Removing RobertaConfig inheritance from CamembertConfig  by @Saad135 in #20059</li>\n<li>Skip 2 tests in <code>VisionTextDualEncoderProcessorTest</code>  by @ydshieh in #20098</li>\n<li>Replace unsupported facebookresearch/bitsandbytes  by @tomaarsen in #20093</li>\n<li>docs: Resolve many typos in the English docs  by @tomaarsen in #20088</li>\n<li>use huggingface_hub.model_inifo() to get pipline_tag  by @y-tag in #20077</li>\n<li>Fix <code>generate_dummy_inputs</code> for <code>ImageGPTOnnxConfig</code>  by @ydshieh in #20103</li>\n<li>docs: Fixed variables in f-strings  by @tomaarsen in #20087</li>\n<li>Add new terms to the glossary  by @stevhliu in #20051</li>\n<li>Replace awkward timm link with the expected one  by @tomaarsen in #20109</li>\n<li>Fix AutoTokenizer with subfolder passed  by @sgugger in #20110</li>\n<li>[Audio Processor] Only pass sr to feat extractor  by @sanchit-gandhi in #20022</li>\n<li>Update github pr docs actions  by @mishig25 in #20125</li>\n<li>Adapt has_labels test when no labels were found  by @sgugger in #20113</li>\n<li>Improve tiny model creation script  by @ydshieh in #20119</li>\n<li>Remove BertConfig inheritance from RobertaConfig  by @Saad135 in #20124</li>\n<li>[Swin] Add Swin SimMIM checkpoints  by @NielsRogge in #20034</li>\n<li>Update <code>CLIPSegModelTester</code>  by @ydshieh in #20134</li>\n<li>Update SwinForMaskedImageModeling doctest values  by @amyeroberts in #20139</li>\n<li>Attempting to test automatically the <code>_keys_to_ignore</code>.  by @Narsil in #20042</li>\n<li>Generate: move generation_<em>.py src files into generation/</em>.py  by @gante in #20096</li>\n<li>add cv + audio labels  by @stevhliu in #20114</li>\n<li>Update VisionEncoderDecoder to use an image processor  by @amyeroberts in #20137</li>\n<li>[CLIPSeg] Add resources  by @NielsRogge in #20118</li>\n<li>Make DummyObject more robust  by @mariosasko in #20146</li>\n<li>Add <code>RoCBertTokenizer</code> to <code>TOKENIZER_MAPPING_NAMES</code>  by @ydshieh in #20141</li>\n<li>Adding support for LayoutLMvX variants for <code>object-detection</code>.  by @Narsil in #20143</li>\n<li>Add doc tests  by @NielsRogge in #20158</li>\n<li>doc comment fix: Args was in wrong place  by @hollance in #20164</li>\n<li>Update <code>OnnxConfig.generate_dummy_inputs</code> to check <code>ImageProcessingMixin</code>  by @ydshieh in #20157</li>\n<li>Generate: fix TF doctests  by @gante in #20159</li>\n<li>Fix arg names for our models  by @Rocketknight1 in #20166</li>\n<li>[processor] Add 'model input names' property  by @sanchit-gandhi in #20117</li>\n<li>Fix object-detection bug (height, width inversion).  by @Narsil in #20167</li>\n<li>[OWL-ViT] Make model consistent with CLIP  by @NielsRogge in #20144</li>\n<li>Fix type - update any PIL.Image.Resampling  by @amyeroberts in #20172</li>\n<li>Fix tapas scatter  by @Bearnardd in #20149</li>\n<li>Update README.md  by @code-with-rajeev in #19530</li>\n<li>Proposal Remove the weird <code>inspect</code> in ASR pipeline and make WhisperEncoder just nice to use.  by @Narsil in #19571</li>\n<li>Pytorch type hints  by @IMvision12 in #20112</li>\n<li>Generate: TF sample doctest result update  by @gante in #20208</li>\n<li>[ROC_BERT] Make CI happy  by @younesbelkada in #20175</li>\n<li>add _keys_to_ignore_on_load_unexpected = [r\"pooler\"]  by @ArthurZucker in #20210</li>\n<li>docs: translated index page to korean  by @wonhyeongseo in #20180</li>\n<li>feat: add i18n issue template  by @wonhyeongseo in #20199</li>\n<li>[Examples] Generalise Seq2Seq ASR to handle Whisper  by @sanchit-gandhi in #19519</li>\n<li>mark <code>test_save_load_fast_init_from_base</code> as <code>is_flaky</code>  by @ydshieh in #20200</li>\n<li>Update README.md  by @Nietism in #20188</li>\n<li>Downgrade log warning -&gt; info  by @amyeroberts in #20202</li>\n<li>Generate: add Bloom fixes for contrastive search  by @gante in #20213</li>\n<li>Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm.  by @Narsil in #20104</li>\n<li>[docs] set overflowing image width to auto-scale  by @wonhyeongseo in #20197</li>\n<li>Update tokenizer_summary.mdx  by @bofenghuang in #20135</li>\n<li>Make <code>ImageSegmentationPipelineTests</code> less flaky  by @ydshieh in #20147</li>\n<li>update relative positional embedding  by @ArthurZucker in #20203</li>\n<li>[WHISPER] Update modeling tests  by @ArthurZucker in #20162</li>\n<li>Add <code>accelerate</code> support for <code>ViT</code> family  by @younesbelkada in #20174</li>\n<li>Add param_name to size_dict logs &amp; tidy  by @amyeroberts in #20205</li>\n<li>Add object detection + segmentation transforms  by @amyeroberts in #20003</li>\n<li>Typo on doctring in ElectraTokenizer  by @FacerAin in #20192</li>\n<li>Remove <code>authorized_missing_keys</code>in favor of _keys_to_ignore_on_load_missing  by @ArthurZucker in #20228</li>\n<li>Add missing ESM autoclass  by @Rocketknight1 in #20177</li>\n<li>fix device issue  by @ydshieh in #20227</li>\n<li>fixed spelling error in testing.mdx  by @kasmith11 in #20220</li>\n<li>Fix <code>run_clip.py</code>  by @ydshieh in #20234</li>\n<li>Fix docstring of CLIPTokenizer(Fast)  by @TilmannR in #20233</li>\n<li>Fix MaskformerFeatureExtractor  by @NielsRogge in #20100</li>\n<li>New logging support to \"Trainer\" Class (ClearML Logger)  by @skinan in #20184</li>\n<li>Enable PyTorch 1.13  by @sgugger in #20168</li>\n<li>[CLIP] allow loading projection layer in vision and text model  by @patil-suraj in #18962</li>\n<li>Slightly alter Keras dummy loss  by @Rocketknight1 in #20232</li>\n<li>Add to DeBERTa resources  by @Saad135 in #20155</li>\n<li>Add clip resources to the transformers documentation  by @ambujpawar in #20190</li>\n<li>Update reqs to include min gather_for_metrics Accelerate version  by @muellerzr in #20242</li>\n<li>Allow trainer to return eval. loss for CLIP-like models  by @ydshieh in #20214</li>\n<li>Adds image-guided object detection support to OWL-ViT  by @alaradirik in #20136</li>\n<li>Adding <code>audio-classification</code> example in the doc.  by @Narsil in #20235</li>\n<li>Updating the doctest for conversational.  by @Narsil in #20236</li>\n<li>Adding doctest for <code>fill-mask</code> pipeline.  by @Narsil in #20241</li>\n<li>Adding doctest for <code>feature-extraction</code>.  by @Narsil in #20240</li>\n<li>Adding ASR pipeline example.  by @Narsil in #20226</li>\n<li>Adding doctest for document-question-answering  by @Narsil in #20239</li>\n<li>Adding an example for <code>depth-estimation</code> pipeline.  by @Narsil in #20237</li>\n<li>Complete doc migration  by @mishig25 in #20267</li>\n<li>Fix result saving errors of pytorch examples  by @li-plus in #20276</li>\n<li>Adding a doctest for <code>table-question-answering</code> pipeline.  by @Narsil in #20260</li>\n<li>Adding doctest for <code>image-segmentation</code> pipeline.  by @Narsil in #20256</li>\n<li>Adding doctest for <code>text2text-generation</code> pipeline.  by @Narsil in #20261</li>\n<li>Adding doctest for <code>text-generation</code> pipeline.  by @Narsil in #20264</li>\n<li>Add TF protein notebook to notebooks doc  by @Rocketknight1 in #20271</li>\n<li>Rephrasing the link.  by @Narsil in #20253</li>\n<li>Add Chinese-CLIP implementation  by @yangapku in #20368</li>\n<li>Adding doctest example for <code>image-classification</code> pipeline.  by @Narsil in #20254</li>\n<li>Adding doctest for <code>zero-shot-image-classification</code> pipeline.  by @Narsil in #20272</li>\n<li>Adding doctest for <code>zero-shot-classification</code> pipeline.  by @Narsil in #20268</li>\n<li>Adding doctest for <code>visual-question-answering</code> pipeline.  by @Narsil in #20266</li>\n<li>Adding doctest for <code>text-classification</code> pipeline.  by @Narsil in #20262</li>\n<li>Adding doctest for <code>question-answering</code> pipeline.  by @Narsil in #20259</li>\n<li>[Docs] Add resources of OpenAI GPT  by @shogohida in #20084</li>\n<li>Adding doctest for <code>image-to-text</code> pipeline.  by @Narsil in #20257</li>\n<li>Adding doctest for <code>token-classification</code> pipeline.  by @Narsil in #20265</li>\n<li>remaining pytorch type hints  by @IMvision12 in #20217</li>\n<li>Data collator for token classification pads labels column when receives pytorch tensors  by @markovalexander in #20244</li>\n<li>[Doctest] Add configuration_deformable_detr.py  by @Saad135 in #20273</li>\n<li>Fix summarization script  by @muellerzr in #20286</li>\n<li>[DOCTEST] Fix the documentation of RoCBert  by @ArthurZucker in #20142</li>\n<li>[bnb] Let's warn users when saving 8-bit models  by @younesbelkada in #20282</li>\n<li>Adding <code>zero-shot-object-detection</code> pipeline doctest.  by @Narsil in #20274</li>\n<li>Adding doctest for <code>object-detection</code> pipeline.  by @Narsil in #20258</li>\n<li>Image transforms functionality used instead  by @amyeroberts in #20278</li>\n<li>TF: add test for <code>PushToHubCallback</code>  by @gante in #20231</li>\n<li>Generate: general TF XLA constrastive search are now slow tests  by @gante in #20277</li>\n<li>Fixing the doctests failures.  by @Narsil in #20294</li>\n<li>set the default cache_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast  by @sywangyi in #20289</li>\n<li>Add docstrings for canine model  by @raghavanone in #19457</li>\n<li>Add missing report button for Example test  by @ydshieh in #20293</li>\n<li>refactor test  by @younesbelkada in #20300</li>\n<li>[Tiny model creation] deal with <code>ImageProcessor</code>  by @ydshieh in #20298</li>\n<li>Fix blender bot missleading doc  by @ArthurZucker in #20301</li>\n<li>remove two tokens that should not be suppressed  by @ArthurZucker in #20302</li>\n<li>[ASR Examples] Update README for Whisper  by @sanchit-gandhi in #20230</li>\n<li>Add padding image transformation  by @amyeroberts in #19838</li>\n<li>Pin TensorFlow  by @sgugger in #20313</li>\n<li>Add AnyPrecisionAdamW optimizer  by @atturaioe in #18961</li>\n<li>[Proposal] Breaking change <code>zero-shot-object-detection</code> for improved     consistency.  by @Narsil in #20280</li>\n<li>Fix flakey test with seed  by @muellerzr in #20318</li>\n<li>Pin TF 2.10.1 for Push CI  by @ydshieh in #20319</li>\n<li>Remove double brackets  by @stevhliu in #20307</li>\n<li>TF: future proof our keras imports  by @gante in #20317</li>\n<li>organize pipelines by modality  by @stevhliu in #20306</li>\n<li>Fix torch device issues  by @ydshieh in #20304</li>\n<li>Generate: add generation config class  by @gante in #20218</li>\n<li>translate zh quicktour by @bfss in #20095) </li>\n<li>Add Spanish translation of serialization.mdx  by @donelianc in #20245</li>\n<li>Add LayerScale to NAT/DiNAT  by @alihassanijr in #20325</li>\n<li>[Switch Transformers] Fix failing slow test  by @younesbelkada in #20346</li>\n<li>fix: \"BigSicence\" typo in docs  by @rajrajhans in #20331</li>\n<li>Generate: <code>model_kwargs</code> can also be an input to <code>prepare_inputs_for_generation</code>  by @gante in #20353</li>\n<li>Update Special Language Tokens for PLBART  by @jordiclive in #19980</li>\n<li>Add resources  by @NielsRogge in #20296</li>\n<li>Enhance HfArgumentParser functionality and ease of use  by @konstantinjdobler in #20323</li>\n<li>Add inference section to task guides  by @stevhliu in #18781</li>\n<li>Fix toctree for Section 3 in Spanish Documentation  by @donelianc in #20360</li>\n<li>Generate: shorter XLA contrastive search tests  by @gante in #20354</li>\n<li>revert <code>keys_to_ignore</code> for M2M100  by @younesbelkada in #20381</li>\n<li>add <code>accelerate</code> support for <code>ESM</code>  by @younesbelkada in #20379</li>\n<li>Fix nightly runs  by @sgugger in #20352</li>\n<li>Optimizes DonutProcessor token2json method for speed  by @michaelnation26 in #20283</li>\n<li>Indicate better minimal version of PyTorch in big model inference  by @sgugger in #20385</li>\n<li>Fix longformer onnx broken export  by @fxmarty in #20292</li>\n<li>Use tiny models for ONNX tests - text modality  by @lewtun in #20333</li>\n<li>[ESM] fix <code>accelerate</code> tests for esmfold  by @younesbelkada in #20387</li>\n<li>Generate: fix plbart generation tests  by @gante in #20391</li>\n<li>[bloom] convert script tweaks  by @stas00 in #18593</li>\n<li>Fix doctest file path  by @ydshieh in #20400</li>\n<li>[Image Transformers] to_pil fix float edge cases  by @patrickvonplaten in #20406</li>\n<li>make daily CI happy  by @younesbelkada in #20410</li>\n<li>fix nasty <code>bnb</code> bug  by @younesbelkada in #20408</li>\n<li>change the way sentinel tokens can retrived  by @raghavanone in #20373</li>\n<li>[BNB] Throw <code>ValueError</code> when trying to cast or assign  by @younesbelkada in #20409</li>\n<li>Use updated <code>model_max_length</code> when saving tokenizers  by @ydshieh in #20401</li>\n<li>Add Spanish translation of pr_checks.mdx  by @donelianc in #20339</li>\n<li>fix device in longformer onnx path  by @fxmarty in #20419</li>\n<li>Fix ModelOutput instantiation when there is only one tuple  by @sgugger in #20416</li>\n<li><code>accelerate</code> support for <code>OwlViT</code>  by @younesbelkada in #20411</li>\n<li>[AnyPrecisionAdamW] test fix  by @stas00 in #20454</li>\n<li>fix <code>word_to_tokens</code> docstring format  by @SaulLu in #20450</li>\n<li>Fix typo in FSMT Tokenizer  by @kamalkraj in #20456</li>\n<li>Fix device issues in <code>CLIPSegModelIntegrationTest</code>  by @ydshieh in #20467</li>\n<li>Fix links for <code>contrastive_loss</code>  by @ydshieh in #20455</li>\n<li>Fix doctests for audio models  by @ydshieh in #20468</li>\n<li>Fix ESM checkpoints for tests  by @Rocketknight1 in #20436</li>\n<li>More TF int dtype fixes  by @Rocketknight1 in #20384</li>\n<li>make tensors in function build_relative_position created on proper device instead of always on cpu  by @qq775294390 in #20434</li>\n<li>update cpu related doc  by @sywangyi in #20444</li>\n<li>with pytorch cpu only version. without --no_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch&gt;=1.10, using Ampere GPU with cuda&gt;=11.0\"  by @sywangyi in #20445</li>\n<li>[CLIPTokenizer] Improve warning  by @patrickvonplaten in #20458</li>\n<li>Replace assertions with value errors on distilbert model  by @JuheonChu in #20463</li>\n<li>[Doctest] Add configuration_fsmt.py  by @sha016 in #19936</li>\n<li>Replace assertion with ValueError exceptions in run_image_captioning_flax.py  by @katiele47 in #20365</li>\n<li>[FLAX] Add dtype to embedding for bert/bart/opt/t5  by @merrymercy in #20340</li>\n<li>fix both failing RoCBert tests  by @ArthurZucker in #20469</li>\n<li>Include image processor in add-new-model-like  by @amyeroberts in #20439</li>\n<li>chore: add link to the video cls notebook.  by @sayakpaul in #20386</li>\n<li>add timeout option for deepspeed engine  by @henghuiz in #20443</li>\n<li>[Maskformer] Add MaskFormerSwin backbone  by @NielsRogge in #20344</li>\n<li>Extract warnings from CI artifacts  by @ydshieh in #20474</li>\n<li>Add Donut image processor  by @amyeroberts in #20425</li>\n<li>Fix torch meshgrid warnings  by @fxmarty in #20475</li>\n<li>Fix init import_structure sorting  by @sgugger in #20477</li>\n<li>extract warnings in GH workflows  by @ydshieh in #20487</li>\n<li>add in layer gpt2 tokenizer  by @piEsposito in #20421</li>\n<li>Replace assert statements with raise exceptions  by @miyu386 in #20478</li>\n<li>fixed small typo  by @sandeepgadhwal in #20490</li>\n<li>Fix documentation code to import facebook/detr-resnet-50 model  by @JuanFKurucz in #20491</li>\n<li>Fix disk offload for full safetensors checkpoints  by @sgugger in #20497</li>\n<li>[modelcard] Check for IterableDataset  by @sanchit-gandhi in #20495</li>\n<li>[modelcard] Set model name if empty  by @sanchit-gandhi in #20496</li>\n<li>Add segmentation + object detection image processors  by @amyeroberts in #20160</li>\n<li>remove <code>attention_mask</code> truncation in whisper  by @ydshieh in #20488</li>\n<li>Make <code>add_special_tokens</code> more clear  by @ydshieh in #20424</li>\n<li>[OPT/Galactica] Load large <code>galactica</code> models  by @younesbelkada in #20390</li>\n<li>Support extraction of both train and eval XLA graphs  by @jeffhataws in #20492</li>\n<li>fix ipex+fp32 jit trace error in ipex 1.13  by @sywangyi in #20504</li>\n<li>Expected output for the test changed  by @ArthurZucker in #20493</li>\n<li>Fix TF nightly tests  by @Rocketknight1 in #20507</li>\n<li>Update doc examples feature extractor -&gt; image processor  by @amyeroberts in #20501</li>\n<li>Fix Typo in Docs for GPU  by @julianpollmann in #20509</li>\n<li>Fix minimum version for device_map  by @sgugger in #20489</li>\n<li>Update <code>AutomaticSpeechRecognitionPipeline</code> doc example  by @ydshieh in #20512</li>\n<li>Add <code>natten</code> for CI  by @ydshieh in #20511</li>\n<li>Fix Data2VecTextForCasualLM example code documentation  by @JuanFKurucz in #20510</li>\n<li>Add some warning for Dynamo and enable TF32 when it's set  by @sgugger in #20515</li>\n<li>[modelcard] Update dataset tags  by @sanchit-gandhi in #20506</li>\n<li>Change Doctests CI launch time  by @ydshieh in #20523</li>\n<li>Fix <code>PLBart</code> doctest  by @ydshieh in #20527</li>\n<li>Fix <code>ConditionalDetrForSegmentation</code> doc example  by @ydshieh in #20531</li>\n<li>add doc for  by @younesbelkada in #20525</li>\n<li>Update <code>ZeroShotObjectDetectionPipeline</code> doc example  by @ydshieh in #20528</li>\n<li>update post_process_image_guided_detection  by @fcakyon in #20521</li>\n<li>QnA example: add speed metric  by @sywangyi in #20522</li>\n<li>Fix doctest  by @NielsRogge in #20534</li>\n<li>Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code  by @JuanFKurucz in #20516</li>\n<li>Fix link in pipeline device map  by @stevhliu in #20517</li>\n</ul>\nSignificant community contributions\n<p>The following contributors have made significant changes to the library over the last release:</p>\n<ul>\n<li>@sww9370<ul>\n<li>Add RocBert (#20013)</li>\n</ul>\n</li>\n<li>@IMvision12<ul>\n<li>Pytorch type hints (#20112)</li>\n<li>remaining pytorch type hints (#20217)</li>\n</ul>\n</li>\n<li>@alihassanijr<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)</li>\n<li>Add LayerScale to NAT/DiNAT (#20325)</li>\n</ul>\n</li>\n<li>@bfss<ul>\n<li>translate zh quicktour(#20095) (#20181)</li>\n</ul>\n</li>\n<li>@donelianc<ul>\n<li>Add Spanish translation of serialization.mdx (#20245)</li>\n<li>Fix toctree for Section 3 in Spanish Documentation (#20360)</li>\n<li>Add Spanish translation of pr_checks.mdx (#20339)</li>\n</ul>\n</li>\n<li>@yangapku<ul>\n<li>Add Chinese-CLIP implementation (#20368)</li>\n</ul>\n</li>\n</ul>", "access_right": "open", "creators": [{"name": "Wolf, Thomas", "affiliation": null}, {"name": "Debut, Lysandre", "affiliation": null}, {"name": "Sanh, Victor", "affiliation": null}, {"name": "Chaumond, Julien", "affiliation": null}, {"name": "Delangue, Clement", "affiliation": null}, {"name": "Moi, Anthony", "affiliation": null}, {"name": "Cistac, Perric", "affiliation": null}, {"name": "Ma, Clara", "affiliation": null}, {"name": "Jernite, Yacine", "affiliation": null}, {"name": "Plu, Julien", "affiliation": null}, {"name": "Xu, Canwen", "affiliation": null}, {"name": "Le Scao, Teven", "affiliation": null}, {"name": "Gugger, Sylvain", "affiliation": null}, {"name": "Drame, Mariama", "affiliation": null}, {"name": "Lhoest, Quentin", "affiliation": null}, {"name": "Rush, Alexander M.", "affiliation": null}], "related_identifiers": [{"identifier": "https://github.com/huggingface/transformers/tree/v4.25.1", "relation": "isSupplementTo", "scheme": "url"}], "version": "v4.25.1", "resource_type": {"title": "Software", "type": "software"}, "license": {"id": "other-open"}, "relations": {"version": [{"index": 94, "is_last": true, "parent": {"pid_type": "recid", "pid_value": "3385997"}}]}, "notes": "If you use this software, please cite it using these metadata."}, "title": "Transformers: State-of-the-Art Natural Language Processing", "links": {"self": "https://zenodo.org/api/records/7391177", "self_html": "https://zenodo.org/records/7391177", "self_doi": "https://zenodo.org/doi/10.5281/zenodo.7391177", "doi": "https://doi.org/10.5281/zenodo.7391177", "parent": "https://zenodo.org/api/records/3385997", "parent_html": "https://zenodo.org/records/3385997", "parent_doi": "https://zenodo.org/doi/10.5281/zenodo.3385997", "self_iiif_manifest": "https://zenodo.org/api/iiif/record:7391177/manifest", "self_iiif_sequence": "https://zenodo.org/api/iiif/record:7391177/sequence/default", "files": "https://zenodo.org/api/records/7391177/files", "media_files": "https://zenodo.org/api/records/7391177/media-files", "archive": "https://zenodo.org/api/records/7391177/files-archive", "archive_media": "https://zenodo.org/api/records/7391177/media-files-archive", "latest": "https://zenodo.org/api/records/7391177/versions/latest", "latest_html": "https://zenodo.org/records/7391177/latest", "draft": "https://zenodo.org/api/records/7391177/draft", "versions": "https://zenodo.org/api/records/7391177/versions", "access_links": "https://zenodo.org/api/records/7391177/access/links", "access_users": "https://zenodo.org/api/records/7391177/access/users", "access_request": "https://zenodo.org/api/records/7391177/access/request", "access": "https://zenodo.org/api/records/7391177/access", "reserve_doi": "https://zenodo.org/api/records/7391177/draft/pids/doi", "communities": "https://zenodo.org/api/records/7391177/communities", "communities-suggestions": "https://zenodo.org/api/records/7391177/communities-suggestions", "requests": "https://zenodo.org/api/records/7391177/requests"}, "updated": "2022-12-03T02:26:43.995097+00:00", "recid": "7391177", "revision": 3, "files": [{"id": "56fcedb0-8d46-4121-8a2c-f2edf8ff120e", "key": "huggingface/transformers-v4.25.1.zip", "size": 14365317, "checksum": "md5:c8369dbbc1c61822d6069d2d7aa8d263", "links": {"self": "https://zenodo.org/api/records/7391177/files/huggingface/transformers-v4.25.1.zip/content"}}], "owners": [{"id": 75471}], "status": "published", "stats": {"downloads": 2179, "unique_downloads": 1998, "views": 75329, "unique_views": 68179, "version_downloads": 63, "version_unique_downloads": 55, "version_unique_views": 3605, "version_views": 4015}, "state": "done", "submitted": true}
\ No newline at end of file
+{"created": "2022-12-02T16:03:38.189470+00:00", "modified": "2022-12-03T02:26:43.995097+00:00", "id": 7391177, "conceptrecid": "3385997", "doi": "10.5281/zenodo.7391177", "conceptdoi": "10.5281/zenodo.3385997", "doi_url": "https://doi.org/10.5281/zenodo.7391177", "metadata": {"title": "Transformers: State-of-the-Art Natural Language Processing", "doi": "10.5281/zenodo.7391177", "publication_date": "2020-10-01", "description": "PyTorch 2.0 stack support\n<p>We are very excited by the newly announced PyTorch 2.0 stack. You can enable <code>torch.compile</code> on any of our models, and get support with the <code>Trainer</code> (and in all our PyTorch examples) by using the <code>torchdynamo</code> training argument. For instance, just add <code>--torchdynamo inductor</code> when launching those examples from the command line.</p>\n<p>This API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.</p>\n<p>Note that to get the best performance, we recommend:</p>\n<ul>\n<li>using an Ampere GPU (or more recent)</li>\n<li><p>sticking to fixed shaped for now (so use <code>--pad_to_max_length</code> in our examples)</p>\n</li>\n<li><p>Repurpose torchdynamo training args towards torch._dynamo  by @sgugger in #20498</p>\n</li>\n</ul>\nAudio Spectrogram Transformer\n<p>The Audio Spectrogram Transformer model was proposed in <a href=\"https://arxiv.org/abs/2104.01778\">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/vit\">Vision Transformer</a> to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.</p>\n<ul>\n<li>Add Audio Spectogram Transformer  by @NielsRogge in #19981</li>\n</ul>\nJukebox\n<p>The Jukebox model was proposed in <a href=\"https://arxiv.org/pdf/2005.00341.pdf\">Jukebox: A generative model for music</a> by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.</p>\n<ul>\n<li>Add Jukebox model (replaces #16875)  by @ArthurZucker in #17826</li>\n</ul>\nSwitch Transformers\n<p>The SwitchTransformers model was proposed in <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> by William Fedus, Barret Zoph, Noam Shazeer.</p>\n<p>It is the first MoE model supported in <code>transformers</code>, with the largest checkpoint currently available currently containing 1T parameters.</p>\n<ul>\n<li>Add Switch transformers  by @younesbelkada and @ArthurZucker  in #19323</li>\n</ul>\nRocBert\n<p>The RoCBert model was proposed in <a href=\"https://aclanthology.org/2022.acl-long.65.pdf\">RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</a> by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.</p>\n<ul>\n<li>Add RocBert  by @sww9370 in #20013</li>\n</ul>\nCLIPSeg\n<p>The CLIPSeg model was proposed in <a href=\"https://arxiv.org/abs/2112.10003\">Image Segmentation Using Text and Image Prompts</a> by Timo L\u00fcddecke and Alexander Ecker. <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/clip\">CLIP</a>Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.</p>\n<ul>\n<li>Add CLIPSeg  by @NielsRogge in #20066</li>\n</ul>\nNAT and DiNAT\nNAT\n<p>NAT was proposed in <a href=\"https://arxiv.org/abs/2204.07143\">Neighborhood Attention Transformer</a> by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.</p>\n<p>It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.</p>\nDiNAT\n<p>DiNAT was proposed in <a href=\"https://arxiv.org/abs/2209.15001\">Dilated Neighborhood Attention Transformer</a> by Ali Hassani and Humphrey Shi.</p>\n<p>It extends <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/nat\">NAT</a> by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.</p>\n<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models  by @alihassanijr in #20219</li>\n</ul>\nMobileNetV2\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1801.04381\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.</p>\n<ul>\n<li>add MobileNetV2 model  by @hollance in #17845</li>\n</ul>\nMobileNetV1\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1704.04861\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.</p>\n<ul>\n<li>add MobileNetV1 model  by @hollance in #17799</li>\n</ul>\nImage processors\n<p>Image processors replace feature extractors as the processing class for computer vision models.</p>\n<p>Important changes:</p>\n<ul>\n<li><code>size</code> parameter is now a dictionary of <code>{\"height\": h, \"width\": w}</code>, <code>{\"shortest_edge\": s}</code>, <code>{\"shortest_egde\": s, \"longest_edge\": l}</code> instead of int or tuple. </li>\n<li>Addition of <code>data_format</code> flag. You can now specify if you want your images to be returned in <code>\"channels_first\"</code> - NCHW - or <code>\"channels_last\"</code> - NHWC - format. </li>\n<li>Processing flags e.g. <code>do_resize</code> can be passed directly to the <code>preprocess</code> method instead of modifying the class attribute: <code>image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")</code></li>\n<li>Leaving <code>return_tensors</code> unset will return a list of numpy arrays.</li>\n</ul>\n<p>The classes are backwards compatible and can be created using existing feature extractor configurations - with the <code>size</code> parameter converted.</p>\n<ul>\n<li>Add Image Processors  by @amyeroberts in #19796</li>\n<li>Add Donut image processor by @amyeroberts #20425 </li>\n<li>Add segmentation + object detection image processors by @amyeroberts in #20160 </li>\n<li>AutoImageProcessor  by @amyeroberts in #20111</li>\n</ul>\nBackbone for computer vision models\n<p>We're adding support for a general <code>AutoBackbone</code> class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.</p>\n<ul>\n<li>Add AutoBackbone + ResNetBackbone  by @NielsRogge in #20229</li>\n<li>Improve backbone  by @NielsRogge in #20380</li>\n<li>[AutoBackbone] Improve API  by @NielsRogge in #20407</li>\n</ul>\nSupport for <code>safetensors</code> offloading\n<p>If the model you are using has a <code>safetensors</code> checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.</p>\n<ul>\n<li>Safetensors offload  by @sgugger in #20321</li>\n</ul>\nContrastive search in the <code>generate</code> method\n<ul>\n<li>Generate: TF contrastive search with XLA support  by @gante in #20050</li>\n<li>Generate: contrastive search with full optional outputs  by @gante in #19963</li>\n</ul>\nBreaking changes\n<ul>\n<li>\ud83d\udea8 \ud83d\udea8 \ud83d\udea8 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in <code>convert_tokens_to_string</code>  by @beneyal in #15775</li>\n</ul>\nBugfixes and improvements\n<ul>\n<li>add dataset  by @stevhliu in #20005</li>\n<li>Add BERT resources  by @stevhliu in #19852</li>\n<li>Add LayoutLMv3 resource  by @stevhliu in #19932</li>\n<li>fix typo  by @stevhliu in #20006</li>\n<li>Update object detection pipeline to use post_process_object_detection methods by @alaradirik in #20004</li>\n<li>clean up vision/text config dict arguments  by @ydshieh in #19954</li>\n<li>make sentencepiece import conditional in bertjapanesetokenizer  by @ripose-jp in #20012</li>\n<li>Fix gradient checkpoint test in encoder-decoder  by @ydshieh in #20017</li>\n<li>Quality  by @sgugger in #20002</li>\n<li>Update auto processor to check image processor created  by @amyeroberts in #20021</li>\n<li>[Doctest] Add configuration_deberta_v2.py  by @Saad135 in #19995</li>\n<li>Improve model tester  by @ydshieh in #19984</li>\n<li>Fix doctest  by @ydshieh in #20023</li>\n<li>Show installed libraries and their versions in CI jobs  by @ydshieh in #20026</li>\n<li>reorganize glossary  by @stevhliu in #20010</li>\n<li>Now supporting pathlike in pipelines too.  by @Narsil in #20030</li>\n<li>Add **kwargs  by @amyeroberts in #20037</li>\n<li>Fix some doctests after PR 15775  by @ydshieh in #20036</li>\n<li>[Doctest] Add configuration_camembert.py  by @Saad135 in #20039</li>\n<li>[Whisper Tokenizer] Make more user-friendly  by @sanchit-gandhi in #19921</li>\n<li>[FuturWarning] Add futur warning for LEDForSequenceClassification  by @ArthurZucker in #19066</li>\n<li>fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc  by @sywangyi in #19891</li>\n<li>Update esmfold conversion script  by @Rocketknight1 in #20028</li>\n<li>Fixed torch.finfo issue with torch.fx  by @michaelbenayoun in #20040</li>\n<li>Only resize embeddings when necessary  by @sgugger in #20043</li>\n<li>Speed up TF token classification postprocessing by converting complete tensors to numpy  by @deutschmn in #19976</li>\n<li>Fix ESM LM head test  by @Rocketknight1 in #20045</li>\n<li>Update README.md  by @bofenghuang in #20063</li>\n<li>fix <code>tokenizer_type</code> to avoid error when loading checkpoint back  by @pacman100 in #20062</li>\n<li>[Trainer] Fix model name in push_to_hub  by @sanchit-gandhi in #20064</li>\n<li>PoolformerImageProcessor defaults to match previous FE  by @amyeroberts in #20048</li>\n<li>change constant torch.tensor to torch.full  by @MerHS in #20061</li>\n<li>Update READMEs for ESMFold and add notebooks  by @Rocketknight1 in #20067</li>\n<li>Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2  by @jordiclive in #20068</li>\n<li>Allow passing arguments to model testers for CLIP-like models  by @ydshieh in #20044</li>\n<li>Show installed libraries and their versions in GA jobs  by @ydshieh in #20069</li>\n<li>Update defaults and logic to match old FE  by @amyeroberts in #20065</li>\n<li>Update modeling_tf_utils.py  by @cakiki in #20076</li>\n<li>Update hub.py  by @cakiki in #20075</li>\n<li>[Doctest] Add configuration_dpr.py  by @Saad135 in #20080</li>\n<li>Removing RobertaConfig inheritance from CamembertConfig  by @Saad135 in #20059</li>\n<li>Skip 2 tests in <code>VisionTextDualEncoderProcessorTest</code>  by @ydshieh in #20098</li>\n<li>Replace unsupported facebookresearch/bitsandbytes  by @tomaarsen in #20093</li>\n<li>docs: Resolve many typos in the English docs  by @tomaarsen in #20088</li>\n<li>use huggingface_hub.model_inifo() to get pipline_tag  by @y-tag in #20077</li>\n<li>Fix <code>generate_dummy_inputs</code> for <code>ImageGPTOnnxConfig</code>  by @ydshieh in #20103</li>\n<li>docs: Fixed variables in f-strings  by @tomaarsen in #20087</li>\n<li>Add new terms to the glossary  by @stevhliu in #20051</li>\n<li>Replace awkward timm link with the expected one  by @tomaarsen in #20109</li>\n<li>Fix AutoTokenizer with subfolder passed  by @sgugger in #20110</li>\n<li>[Audio Processor] Only pass sr to feat extractor  by @sanchit-gandhi in #20022</li>\n<li>Update github pr docs actions  by @mishig25 in #20125</li>\n<li>Adapt has_labels test when no labels were found  by @sgugger in #20113</li>\n<li>Improve tiny model creation script  by @ydshieh in #20119</li>\n<li>Remove BertConfig inheritance from RobertaConfig  by @Saad135 in #20124</li>\n<li>[Swin] Add Swin SimMIM checkpoints  by @NielsRogge in #20034</li>\n<li>Update <code>CLIPSegModelTester</code>  by @ydshieh in #20134</li>\n<li>Update SwinForMaskedImageModeling doctest values  by @amyeroberts in #20139</li>\n<li>Attempting to test automatically the <code>_keys_to_ignore</code>.  by @Narsil in #20042</li>\n<li>Generate: move generation_<em>.py src files into generation/</em>.py  by @gante in #20096</li>\n<li>add cv + audio labels  by @stevhliu in #20114</li>\n<li>Update VisionEncoderDecoder to use an image processor  by @amyeroberts in #20137</li>\n<li>[CLIPSeg] Add resources  by @NielsRogge in #20118</li>\n<li>Make DummyObject more robust  by @mariosasko in #20146</li>\n<li>Add <code>RoCBertTokenizer</code> to <code>TOKENIZER_MAPPING_NAMES</code>  by @ydshieh in #20141</li>\n<li>Adding support for LayoutLMvX variants for <code>object-detection</code>.  by @Narsil in #20143</li>\n<li>Add doc tests  by @NielsRogge in #20158</li>\n<li>doc comment fix: Args was in wrong place  by @hollance in #20164</li>\n<li>Update <code>OnnxConfig.generate_dummy_inputs</code> to check <code>ImageProcessingMixin</code>  by @ydshieh in #20157</li>\n<li>Generate: fix TF doctests  by @gante in #20159</li>\n<li>Fix arg names for our models  by @Rocketknight1 in #20166</li>\n<li>[processor] Add 'model input names' property  by @sanchit-gandhi in #20117</li>\n<li>Fix object-detection bug (height, width inversion).  by @Narsil in #20167</li>\n<li>[OWL-ViT] Make model consistent with CLIP  by @NielsRogge in #20144</li>\n<li>Fix type - update any PIL.Image.Resampling  by @amyeroberts in #20172</li>\n<li>Fix tapas scatter  by @Bearnardd in #20149</li>\n<li>Update README.md  by @code-with-rajeev in #19530</li>\n<li>Proposal Remove the weird <code>inspect</code> in ASR pipeline and make WhisperEncoder just nice to use.  by @Narsil in #19571</li>\n<li>Pytorch type hints  by @IMvision12 in #20112</li>\n<li>Generate: TF sample doctest result update  by @gante in #20208</li>\n<li>[ROC_BERT] Make CI happy  by @younesbelkada in #20175</li>\n<li>add _keys_to_ignore_on_load_unexpected = [r\"pooler\"]  by @ArthurZucker in #20210</li>\n<li>docs: translated index page to korean  by @wonhyeongseo in #20180</li>\n<li>feat: add i18n issue template  by @wonhyeongseo in #20199</li>\n<li>[Examples] Generalise Seq2Seq ASR to handle Whisper  by @sanchit-gandhi in #19519</li>\n<li>mark <code>test_save_load_fast_init_from_base</code> as <code>is_flaky</code>  by @ydshieh in #20200</li>\n<li>Update README.md  by @Nietism in #20188</li>\n<li>Downgrade log warning -&gt; info  by @amyeroberts in #20202</li>\n<li>Generate: add Bloom fixes for contrastive search  by @gante in #20213</li>\n<li>Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm.  by @Narsil in #20104</li>\n<li>[docs] set overflowing image width to auto-scale  by @wonhyeongseo in #20197</li>\n<li>Update tokenizer_summary.mdx  by @bofenghuang in #20135</li>\n<li>Make <code>ImageSegmentationPipelineTests</code> less flaky  by @ydshieh in #20147</li>\n<li>update relative positional embedding  by @ArthurZucker in #20203</li>\n<li>[WHISPER] Update modeling tests  by @ArthurZucker in #20162</li>\n<li>Add <code>accelerate</code> support for <code>ViT</code> family  by @younesbelkada in #20174</li>\n<li>Add param_name to size_dict logs &amp; tidy  by @amyeroberts in #20205</li>\n<li>Add object detection + segmentation transforms  by @amyeroberts in #20003</li>\n<li>Typo on doctring in ElectraTokenizer  by @FacerAin in #20192</li>\n<li>Remove <code>authorized_missing_keys</code>in favor of _keys_to_ignore_on_load_missing  by @ArthurZucker in #20228</li>\n<li>Add missing ESM autoclass  by @Rocketknight1 in #20177</li>\n<li>fix device issue  by @ydshieh in #20227</li>\n<li>fixed spelling error in testing.mdx  by @kasmith11 in #20220</li>\n<li>Fix <code>run_clip.py</code>  by @ydshieh in #20234</li>\n<li>Fix docstring of CLIPTokenizer(Fast)  by @TilmannR in #20233</li>\n<li>Fix MaskformerFeatureExtractor  by @NielsRogge in #20100</li>\n<li>New logging support to \"Trainer\" Class (ClearML Logger)  by @skinan in #20184</li>\n<li>Enable PyTorch 1.13  by @sgugger in #20168</li>\n<li>[CLIP] allow loading projection layer in vision and text model  by @patil-suraj in #18962</li>\n<li>Slightly alter Keras dummy loss  by @Rocketknight1 in #20232</li>\n<li>Add to DeBERTa resources  by @Saad135 in #20155</li>\n<li>Add clip resources to the transformers documentation  by @ambujpawar in #20190</li>\n<li>Update reqs to include min gather_for_metrics Accelerate version  by @muellerzr in #20242</li>\n<li>Allow trainer to return eval. loss for CLIP-like models  by @ydshieh in #20214</li>\n<li>Adds image-guided object detection support to OWL-ViT  by @alaradirik in #20136</li>\n<li>Adding <code>audio-classification</code> example in the doc.  by @Narsil in #20235</li>\n<li>Updating the doctest for conversational.  by @Narsil in #20236</li>\n<li>Adding doctest for <code>fill-mask</code> pipeline.  by @Narsil in #20241</li>\n<li>Adding doctest for <code>feature-extraction</code>.  by @Narsil in #20240</li>\n<li>Adding ASR pipeline example.  by @Narsil in #20226</li>\n<li>Adding doctest for document-question-answering  by @Narsil in #20239</li>\n<li>Adding an example for <code>depth-estimation</code> pipeline.  by @Narsil in #20237</li>\n<li>Complete doc migration  by @mishig25 in #20267</li>\n<li>Fix result saving errors of pytorch examples  by @li-plus in #20276</li>\n<li>Adding a doctest for <code>table-question-answering</code> pipeline.  by @Narsil in #20260</li>\n<li>Adding doctest for <code>image-segmentation</code> pipeline.  by @Narsil in #20256</li>\n<li>Adding doctest for <code>text2text-generation</code> pipeline.  by @Narsil in #20261</li>\n<li>Adding doctest for <code>text-generation</code> pipeline.  by @Narsil in #20264</li>\n<li>Add TF protein notebook to notebooks doc  by @Rocketknight1 in #20271</li>\n<li>Rephrasing the link.  by @Narsil in #20253</li>\n<li>Add Chinese-CLIP implementation  by @yangapku in #20368</li>\n<li>Adding doctest example for <code>image-classification</code> pipeline.  by @Narsil in #20254</li>\n<li>Adding doctest for <code>zero-shot-image-classification</code> pipeline.  by @Narsil in #20272</li>\n<li>Adding doctest for <code>zero-shot-classification</code> pipeline.  by @Narsil in #20268</li>\n<li>Adding doctest for <code>visual-question-answering</code> pipeline.  by @Narsil in #20266</li>\n<li>Adding doctest for <code>text-classification</code> pipeline.  by @Narsil in #20262</li>\n<li>Adding doctest for <code>question-answering</code> pipeline.  by @Narsil in #20259</li>\n<li>[Docs] Add resources of OpenAI GPT  by @shogohida in #20084</li>\n<li>Adding doctest for <code>image-to-text</code> pipeline.  by @Narsil in #20257</li>\n<li>Adding doctest for <code>token-classification</code> pipeline.  by @Narsil in #20265</li>\n<li>remaining pytorch type hints  by @IMvision12 in #20217</li>\n<li>Data collator for token classification pads labels column when receives pytorch tensors  by @markovalexander in #20244</li>\n<li>[Doctest] Add configuration_deformable_detr.py  by @Saad135 in #20273</li>\n<li>Fix summarization script  by @muellerzr in #20286</li>\n<li>[DOCTEST] Fix the documentation of RoCBert  by @ArthurZucker in #20142</li>\n<li>[bnb] Let's warn users when saving 8-bit models  by @younesbelkada in #20282</li>\n<li>Adding <code>zero-shot-object-detection</code> pipeline doctest.  by @Narsil in #20274</li>\n<li>Adding doctest for <code>object-detection</code> pipeline.  by @Narsil in #20258</li>\n<li>Image transforms functionality used instead  by @amyeroberts in #20278</li>\n<li>TF: add test for <code>PushToHubCallback</code>  by @gante in #20231</li>\n<li>Generate: general TF XLA constrastive search are now slow tests  by @gante in #20277</li>\n<li>Fixing the doctests failures.  by @Narsil in #20294</li>\n<li>set the default cache_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast  by @sywangyi in #20289</li>\n<li>Add docstrings for canine model  by @raghavanone in #19457</li>\n<li>Add missing report button for Example test  by @ydshieh in #20293</li>\n<li>refactor test  by @younesbelkada in #20300</li>\n<li>[Tiny model creation] deal with <code>ImageProcessor</code>  by @ydshieh in #20298</li>\n<li>Fix blender bot missleading doc  by @ArthurZucker in #20301</li>\n<li>remove two tokens that should not be suppressed  by @ArthurZucker in #20302</li>\n<li>[ASR Examples] Update README for Whisper  by @sanchit-gandhi in #20230</li>\n<li>Add padding image transformation  by @amyeroberts in #19838</li>\n<li>Pin TensorFlow  by @sgugger in #20313</li>\n<li>Add AnyPrecisionAdamW optimizer  by @atturaioe in #18961</li>\n<li>[Proposal] Breaking change <code>zero-shot-object-detection</code> for improved     consistency.  by @Narsil in #20280</li>\n<li>Fix flakey test with seed  by @muellerzr in #20318</li>\n<li>Pin TF 2.10.1 for Push CI  by @ydshieh in #20319</li>\n<li>Remove double brackets  by @stevhliu in #20307</li>\n<li>TF: future proof our keras imports  by @gante in #20317</li>\n<li>organize pipelines by modality  by @stevhliu in #20306</li>\n<li>Fix torch device issues  by @ydshieh in #20304</li>\n<li>Generate: add generation config class  by @gante in #20218</li>\n<li>translate zh quicktour by @bfss in #20095) </li>\n<li>Add Spanish translation of serialization.mdx  by @donelianc in #20245</li>\n<li>Add LayerScale to NAT/DiNAT  by @alihassanijr in #20325</li>\n<li>[Switch Transformers] Fix failing slow test  by @younesbelkada in #20346</li>\n<li>fix: \"BigSicence\" typo in docs  by @rajrajhans in #20331</li>\n<li>Generate: <code>model_kwargs</code> can also be an input to <code>prepare_inputs_for_generation</code>  by @gante in #20353</li>\n<li>Update Special Language Tokens for PLBART  by @jordiclive in #19980</li>\n<li>Add resources  by @NielsRogge in #20296</li>\n<li>Enhance HfArgumentParser functionality and ease of use  by @konstantinjdobler in #20323</li>\n<li>Add inference section to task guides  by @stevhliu in #18781</li>\n<li>Fix toctree for Section 3 in Spanish Documentation  by @donelianc in #20360</li>\n<li>Generate: shorter XLA contrastive search tests  by @gante in #20354</li>\n<li>revert <code>keys_to_ignore</code> for M2M100  by @younesbelkada in #20381</li>\n<li>add <code>accelerate</code> support for <code>ESM</code>  by @younesbelkada in #20379</li>\n<li>Fix nightly runs  by @sgugger in #20352</li>\n<li>Optimizes DonutProcessor token2json method for speed  by @michaelnation26 in #20283</li>\n<li>Indicate better minimal version of PyTorch in big model inference  by @sgugger in #20385</li>\n<li>Fix longformer onnx broken export  by @fxmarty in #20292</li>\n<li>Use tiny models for ONNX tests - text modality  by @lewtun in #20333</li>\n<li>[ESM] fix <code>accelerate</code> tests for esmfold  by @younesbelkada in #20387</li>\n<li>Generate: fix plbart generation tests  by @gante in #20391</li>\n<li>[bloom] convert script tweaks  by @stas00 in #18593</li>\n<li>Fix doctest file path  by @ydshieh in #20400</li>\n<li>[Image Transformers] to_pil fix float edge cases  by @patrickvonplaten in #20406</li>\n<li>make daily CI happy  by @younesbelkada in #20410</li>\n<li>fix nasty <code>bnb</code> bug  by @younesbelkada in #20408</li>\n<li>change the way sentinel tokens can retrived  by @raghavanone in #20373</li>\n<li>[BNB] Throw <code>ValueError</code> when trying to cast or assign  by @younesbelkada in #20409</li>\n<li>Use updated <code>model_max_length</code> when saving tokenizers  by @ydshieh in #20401</li>\n<li>Add Spanish translation of pr_checks.mdx  by @donelianc in #20339</li>\n<li>fix device in longformer onnx path  by @fxmarty in #20419</li>\n<li>Fix ModelOutput instantiation when there is only one tuple  by @sgugger in #20416</li>\n<li><code>accelerate</code> support for <code>OwlViT</code>  by @younesbelkada in #20411</li>\n<li>[AnyPrecisionAdamW] test fix  by @stas00 in #20454</li>\n<li>fix <code>word_to_tokens</code> docstring format  by @SaulLu in #20450</li>\n<li>Fix typo in FSMT Tokenizer  by @kamalkraj in #20456</li>\n<li>Fix device issues in <code>CLIPSegModelIntegrationTest</code>  by @ydshieh in #20467</li>\n<li>Fix links for <code>contrastive_loss</code>  by @ydshieh in #20455</li>\n<li>Fix doctests for audio models  by @ydshieh in #20468</li>\n<li>Fix ESM checkpoints for tests  by @Rocketknight1 in #20436</li>\n<li>More TF int dtype fixes  by @Rocketknight1 in #20384</li>\n<li>make tensors in function build_relative_position created on proper device instead of always on cpu  by @qq775294390 in #20434</li>\n<li>update cpu related doc  by @sywangyi in #20444</li>\n<li>with pytorch cpu only version. without --no_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch&gt;=1.10, using Ampere GPU with cuda&gt;=11.0\"  by @sywangyi in #20445</li>\n<li>[CLIPTokenizer] Improve warning  by @patrickvonplaten in #20458</li>\n<li>Replace assertions with value errors on distilbert model  by @JuheonChu in #20463</li>\n<li>[Doctest] Add configuration_fsmt.py  by @sha016 in #19936</li>\n<li>Replace assertion with ValueError exceptions in run_image_captioning_flax.py  by @katiele47 in #20365</li>\n<li>[FLAX] Add dtype to embedding for bert/bart/opt/t5  by @merrymercy in #20340</li>\n<li>fix both failing RoCBert tests  by @ArthurZucker in #20469</li>\n<li>Include image processor in add-new-model-like  by @amyeroberts in #20439</li>\n<li>chore: add link to the video cls notebook.  by @sayakpaul in #20386</li>\n<li>add timeout option for deepspeed engine  by @henghuiz in #20443</li>\n<li>[Maskformer] Add MaskFormerSwin backbone  by @NielsRogge in #20344</li>\n<li>Extract warnings from CI artifacts  by @ydshieh in #20474</li>\n<li>Add Donut image processor  by @amyeroberts in #20425</li>\n<li>Fix torch meshgrid warnings  by @fxmarty in #20475</li>\n<li>Fix init import_structure sorting  by @sgugger in #20477</li>\n<li>extract warnings in GH workflows  by @ydshieh in #20487</li>\n<li>add in layer gpt2 tokenizer  by @piEsposito in #20421</li>\n<li>Replace assert statements with raise exceptions  by @miyu386 in #20478</li>\n<li>fixed small typo  by @sandeepgadhwal in #20490</li>\n<li>Fix documentation code to import facebook/detr-resnet-50 model  by @JuanFKurucz in #20491</li>\n<li>Fix disk offload for full safetensors checkpoints  by @sgugger in #20497</li>\n<li>[modelcard] Check for IterableDataset  by @sanchit-gandhi in #20495</li>\n<li>[modelcard] Set model name if empty  by @sanchit-gandhi in #20496</li>\n<li>Add segmentation + object detection image processors  by @amyeroberts in #20160</li>\n<li>remove <code>attention_mask</code> truncation in whisper  by @ydshieh in #20488</li>\n<li>Make <code>add_special_tokens</code> more clear  by @ydshieh in #20424</li>\n<li>[OPT/Galactica] Load large <code>galactica</code> models  by @younesbelkada in #20390</li>\n<li>Support extraction of both train and eval XLA graphs  by @jeffhataws in #20492</li>\n<li>fix ipex+fp32 jit trace error in ipex 1.13  by @sywangyi in #20504</li>\n<li>Expected output for the test changed  by @ArthurZucker in #20493</li>\n<li>Fix TF nightly tests  by @Rocketknight1 in #20507</li>\n<li>Update doc examples feature extractor -&gt; image processor  by @amyeroberts in #20501</li>\n<li>Fix Typo in Docs for GPU  by @julianpollmann in #20509</li>\n<li>Fix minimum version for device_map  by @sgugger in #20489</li>\n<li>Update <code>AutomaticSpeechRecognitionPipeline</code> doc example  by @ydshieh in #20512</li>\n<li>Add <code>natten</code> for CI  by @ydshieh in #20511</li>\n<li>Fix Data2VecTextForCasualLM example code documentation  by @JuanFKurucz in #20510</li>\n<li>Add some warning for Dynamo and enable TF32 when it's set  by @sgugger in #20515</li>\n<li>[modelcard] Update dataset tags  by @sanchit-gandhi in #20506</li>\n<li>Change Doctests CI launch time  by @ydshieh in #20523</li>\n<li>Fix <code>PLBart</code> doctest  by @ydshieh in #20527</li>\n<li>Fix <code>ConditionalDetrForSegmentation</code> doc example  by @ydshieh in #20531</li>\n<li>add doc for  by @younesbelkada in #20525</li>\n<li>Update <code>ZeroShotObjectDetectionPipeline</code> doc example  by @ydshieh in #20528</li>\n<li>update post_process_image_guided_detection  by @fcakyon in #20521</li>\n<li>QnA example: add speed metric  by @sywangyi in #20522</li>\n<li>Fix doctest  by @NielsRogge in #20534</li>\n<li>Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code  by @JuanFKurucz in #20516</li>\n<li>Fix link in pipeline device map  by @stevhliu in #20517</li>\n</ul>\nSignificant community contributions\n<p>The following contributors have made significant changes to the library over the last release:</p>\n<ul>\n<li>@sww9370<ul>\n<li>Add RocBert (#20013)</li>\n</ul>\n</li>\n<li>@IMvision12<ul>\n<li>Pytorch type hints (#20112)</li>\n<li>remaining pytorch type hints (#20217)</li>\n</ul>\n</li>\n<li>@alihassanijr<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)</li>\n<li>Add LayerScale to NAT/DiNAT (#20325)</li>\n</ul>\n</li>\n<li>@bfss<ul>\n<li>translate zh quicktour(#20095) (#20181)</li>\n</ul>\n</li>\n<li>@donelianc<ul>\n<li>Add Spanish translation of serialization.mdx (#20245)</li>\n<li>Fix toctree for Section 3 in Spanish Documentation (#20360)</li>\n<li>Add Spanish translation of pr_checks.mdx (#20339)</li>\n</ul>\n</li>\n<li>@yangapku<ul>\n<li>Add Chinese-CLIP implementation (#20368)</li>\n</ul>\n</li>\n</ul>", "access_right": "open", "creators": [{"name": "Wolf, Thomas", "affiliation": null}, {"name": "Debut, Lysandre", "affiliation": null}, {"name": "Sanh, Victor", "affiliation": null}, {"name": "Chaumond, Julien", "affiliation": null}, {"name": "Delangue, Clement", "affiliation": null}, {"name": "Moi, Anthony", "affiliation": null}, {"name": "Cistac, Perric", "affiliation": null}, {"name": "Ma, Clara", "affiliation": null}, {"name": "Jernite, Yacine", "affiliation": null}, {"name": "Plu, Julien", "affiliation": null}, {"name": "Xu, Canwen", "affiliation": null}, {"name": "Le Scao, Teven", "affiliation": null}, {"name": "Gugger, Sylvain", "affiliation": null}, {"name": "Drame, Mariama", "affiliation": null}, {"name": "Lhoest, Quentin", "affiliation": null}, {"name": "Rush, Alexander M.", "affiliation": null}], "related_identifiers": [{"identifier": "https://github.com/huggingface/transformers/tree/v4.25.1", "relation": "isSupplementTo", "scheme": "url"}], "version": "v4.25.1", "resource_type": {"title": "Software", "type": "software"}, "license": {"id": "other-open"}, "relations": {"version": [{"index": 94, "is_last": true, "parent": {"pid_type": "recid", "pid_value": "3385997"}}]}, "notes": "If you use this software, please cite it using these metadata."}, "title": "Transformers: State-of-the-Art Natural Language Processing", "links": {"self": "https://zenodo.org/api/records/7391177", "self_html": "https://zenodo.org/records/7391177", "doi": "https://doi.org/10.5281/zenodo.7391177", "self_doi": "https://doi.org/10.5281/zenodo.7391177", "self_doi_html": "https://zenodo.org/doi/10.5281/zenodo.7391177", "parent": "https://zenodo.org/api/records/3385997", "parent_html": "https://zenodo.org/records/3385997", "parent_doi": "https://doi.org/10.5281/zenodo.3385997", "parent_doi_html": "https://zenodo.org/doi/10.5281/zenodo.3385997", "self_iiif_manifest": "https://zenodo.org/api/iiif/record:7391177/manifest", "self_iiif_sequence": "https://zenodo.org/api/iiif/record:7391177/sequence/default", "files": "https://zenodo.org/api/records/7391177/files", "media_files": "https://zenodo.org/api/records/7391177/media-files", "archive": "https://zenodo.org/api/records/7391177/files-archive", "archive_media": "https://zenodo.org/api/records/7391177/media-files-archive", "latest": "https://zenodo.org/api/records/7391177/versions/latest", "latest_html": "https://zenodo.org/records/7391177/latest", "versions": "https://zenodo.org/api/records/7391177/versions", "draft": "https://zenodo.org/api/records/7391177/draft", "reserve_doi": "https://zenodo.org/api/records/7391177/draft/pids/doi", "access_links": "https://zenodo.org/api/records/7391177/access/links", "access_grants": "https://zenodo.org/api/records/7391177/access/grants", "access_users": "https://zenodo.org/api/records/7391177/access/users", "access_request": "https://zenodo.org/api/records/7391177/access/request", "access": "https://zenodo.org/api/records/7391177/access", "communities": "https://zenodo.org/api/records/7391177/communities", "communities-suggestions": "https://zenodo.org/api/records/7391177/communities-suggestions", "requests": "https://zenodo.org/api/records/7391177/requests"}, "updated": "2022-12-03T02:26:43.995097+00:00", "recid": "7391177", "revision": 3, "files": [{"id": "56fcedb0-8d46-4121-8a2c-f2edf8ff120e", "key": "huggingface/transformers-v4.25.1.zip", "size": 14365317, "checksum": "md5:c8369dbbc1c61822d6069d2d7aa8d263", "links": {"self": "https://zenodo.org/api/records/7391177/files/huggingface/transformers-v4.25.1.zip/content"}}], "owners": [{"id": "75471"}], "status": "published", "stats": {"downloads": 2385, "unique_downloads": 2202, "views": 80686, "unique_views": 73437, "version_downloads": 85, "version_unique_downloads": 77, "version_unique_views": 4955, "version_views": 5403}, "state": "done", "submitted": true}
\ No newline at end of file
diff --git a/tests/resources/zenodo/7391177_html_out.json b/tests/resources/zenodo/7391177_html_out.json
new file mode 100644
index 00000000..a2609264
--- /dev/null
+++ b/tests/resources/zenodo/7391177_html_out.json
@@ -0,0 +1,83 @@
+{
+  "abstract": "PyTorch 2.0 stack support\n<p>We are very excited by the newly announced PyTorch 2.0 stack. You can enable <code>torch.compile</code> on any of our models, and get support with the <code>Trainer</code> (and in all our PyTorch examples) by using the <code>torchdynamo</code> training argument. For instance, just add <code>--torchdynamo inductor</code> when launching those examples from the command line.</p>\n<p>This API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.</p>\n<p>Note that to get the best performance, we recommend:</p>\n<ul>\n<li>using an Ampere GPU (or more recent)</li>\n<li><p>sticking to fixed shaped for now (so use <code>--pad_to_max_length</code> in our examples)</p>\n</li>\n<li><p>Repurpose torchdynamo training args towards torch._dynamo  by @sgugger in #20498</p>\n</li>\n</ul>\nAudio Spectrogram Transformer\n<p>The Audio Spectrogram Transformer model was proposed in <a href=\"https://arxiv.org/abs/2104.01778\">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/vit\">Vision Transformer</a> to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.</p>\n<ul>\n<li>Add Audio Spectogram Transformer  by @NielsRogge in #19981</li>\n</ul>\nJukebox\n<p>The Jukebox model was proposed in <a href=\"https://arxiv.org/pdf/2005.00341.pdf\">Jukebox: A generative model for music</a> by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.</p>\n<ul>\n<li>Add Jukebox model (replaces #16875)  by @ArthurZucker in #17826</li>\n</ul>\nSwitch Transformers\n<p>The SwitchTransformers model was proposed in <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> by William Fedus, Barret Zoph, Noam Shazeer.</p>\n<p>It is the first MoE model supported in <code>transformers</code>, with the largest checkpoint currently available currently containing 1T parameters.</p>\n<ul>\n<li>Add Switch transformers  by @younesbelkada and @ArthurZucker  in #19323</li>\n</ul>\nRocBert\n<p>The RoCBert model was proposed in <a href=\"https://aclanthology.org/2022.acl-long.65.pdf\">RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</a> by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.</p>\n<ul>\n<li>Add RocBert  by @sww9370 in #20013</li>\n</ul>\nCLIPSeg\n<p>The CLIPSeg model was proposed in <a href=\"https://arxiv.org/abs/2112.10003\">Image Segmentation Using Text and Image Prompts</a> by Timo Lüddecke and Alexander Ecker. <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/clip\">CLIP</a>Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.</p>\n<ul>\n<li>Add CLIPSeg  by @NielsRogge in #20066</li>\n</ul>\nNAT and DiNAT\nNAT\n<p>NAT was proposed in <a href=\"https://arxiv.org/abs/2204.07143\">Neighborhood Attention Transformer</a> by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.</p>\n<p>It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.</p>\nDiNAT\n<p>DiNAT was proposed in <a href=\"https://arxiv.org/abs/2209.15001\">Dilated Neighborhood Attention Transformer</a> by Ali Hassani and Humphrey Shi.</p>\n<p>It extends <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/nat\">NAT</a> by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.</p>\n<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models  by @alihassanijr in #20219</li>\n</ul>\nMobileNetV2\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1801.04381\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.</p>\n<ul>\n<li>add MobileNetV2 model  by @hollance in #17845</li>\n</ul>\nMobileNetV1\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1704.04861\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.</p>\n<ul>\n<li>add MobileNetV1 model  by @hollance in #17799</li>\n</ul>\nImage processors\n<p>Image processors replace feature extractors as the processing class for computer vision models.</p>\n<p>Important changes:</p>\n<ul>\n<li><code>size</code> parameter is now a dictionary of <code>{\"height\": h, \"width\": w}</code>, <code>{\"shortest_edge\": s}</code>, <code>{\"shortest_egde\": s, \"longest_edge\": l}</code> instead of int or tuple. </li>\n<li>Addition of <code>data_format</code> flag. You can now specify if you want your images to be returned in <code>\"channels_first\"</code> - NCHW - or <code>\"channels_last\"</code> - NHWC - format. </li>\n<li>Processing flags e.g. <code>do_resize</code> can be passed directly to the <code>preprocess</code> method instead of modifying the class attribute: <code>image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")</code></li>\n<li>Leaving <code>return_tensors</code> unset will return a list of numpy arrays.</li>\n</ul>\n<p>The classes are backwards compatible and can be created using existing feature extractor configurations - with the <code>size</code> parameter converted.</p>\n<ul>\n<li>Add Image Processors  by @amyeroberts in #19796</li>\n<li>Add Donut image processor by @amyeroberts #20425 </li>\n<li>Add segmentation + object detection image processors by @amyeroberts in #20160 </li>\n<li>AutoImageProcessor  by @amyeroberts in #20111</li>\n</ul>\nBackbone for computer vision models\n<p>We're adding support for a general <code>AutoBackbone</code> class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.</p>\n<ul>\n<li>Add AutoBackbone + ResNetBackbone  by @NielsRogge in #20229</li>\n<li>Improve backbone  by @NielsRogge in #20380</li>\n<li>[AutoBackbone] Improve API  by @NielsRogge in #20407</li>\n</ul>\nSupport for <code>safetensors</code> offloading\n<p>If the model you are using has a <code>safetensors</code> checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.</p>\n<ul>\n<li>Safetensors offload  by @sgugger in #20321</li>\n</ul>\nContrastive search in the <code>generate</code> method\n<ul>\n<li>Generate: TF contrastive search with XLA support  by @gante in #20050</li>\n<li>Generate: contrastive search with full optional outputs  by @gante in #19963</li>\n</ul>\nBreaking changes\n<ul>\n<li>🚨 🚨 🚨 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in <code>convert_tokens_to_string</code>  by @beneyal in #15775</li>\n</ul>\nBugfixes and improvements\n<ul>\n<li>add dataset  by @stevhliu in #20005</li>\n<li>Add BERT resources  by @stevhliu in #19852</li>\n<li>Add LayoutLMv3 resource  by @stevhliu in #19932</li>\n<li>fix typo  by @stevhliu in #20006</li>\n<li>Update object detection pipeline to use post_process_object_detection methods by @alaradirik in #20004</li>\n<li>clean up vision/text config dict arguments  by @ydshieh in #19954</li>\n<li>make sentencepiece import conditional in bertjapanesetokenizer  by @ripose-jp in #20012</li>\n<li>Fix gradient checkpoint test in encoder-decoder  by @ydshieh in #20017</li>\n<li>Quality  by @sgugger in #20002</li>\n<li>Update auto processor to check image processor created  by @amyeroberts in #20021</li>\n<li>[Doctest] Add configuration_deberta_v2.py  by @Saad135 in #19995</li>\n<li>Improve model tester  by @ydshieh in #19984</li>\n<li>Fix doctest  by @ydshieh in #20023</li>\n<li>Show installed libraries and their versions in CI jobs  by @ydshieh in #20026</li>\n<li>reorganize glossary  by @stevhliu in #20010</li>\n<li>Now supporting pathlike in pipelines too.  by @Narsil in #20030</li>\n<li>Add **kwargs  by @amyeroberts in #20037</li>\n<li>Fix some doctests after PR 15775  by @ydshieh in #20036</li>\n<li>[Doctest] Add configuration_camembert.py  by @Saad135 in #20039</li>\n<li>[Whisper Tokenizer] Make more user-friendly  by @sanchit-gandhi in #19921</li>\n<li>[FuturWarning] Add futur warning for LEDForSequenceClassification  by @ArthurZucker in #19066</li>\n<li>fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc  by @sywangyi in #19891</li>\n<li>Update esmfold conversion script  by @Rocketknight1 in #20028</li>\n<li>Fixed torch.finfo issue with torch.fx  by @michaelbenayoun in #20040</li>\n<li>Only resize embeddings when necessary  by @sgugger in #20043</li>\n<li>Speed up TF token classification postprocessing by converting complete tensors to numpy  by @deutschmn in #19976</li>\n<li>Fix ESM LM head test  by @Rocketknight1 in #20045</li>\n<li>Update README.md  by @bofenghuang in #20063</li>\n<li>fix <code>tokenizer_type</code> to avoid error when loading checkpoint back  by @pacman100 in #20062</li>\n<li>[Trainer] Fix model name in push_to_hub  by @sanchit-gandhi in #20064</li>\n<li>PoolformerImageProcessor defaults to match previous FE  by @amyeroberts in #20048</li>\n<li>change constant torch.tensor to torch.full  by @MerHS in #20061</li>\n<li>Update READMEs for ESMFold and add notebooks  by @Rocketknight1 in #20067</li>\n<li>Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2  by @jordiclive in #20068</li>\n<li>Allow passing arguments to model testers for CLIP-like models  by @ydshieh in #20044</li>\n<li>Show installed libraries and their versions in GA jobs  by @ydshieh in #20069</li>\n<li>Update defaults and logic to match old FE  by @amyeroberts in #20065</li>\n<li>Update modeling_tf_utils.py  by @cakiki in #20076</li>\n<li>Update hub.py  by @cakiki in #20075</li>\n<li>[Doctest] Add configuration_dpr.py  by @Saad135 in #20080</li>\n<li>Removing RobertaConfig inheritance from CamembertConfig  by @Saad135 in #20059</li>\n<li>Skip 2 tests in <code>VisionTextDualEncoderProcessorTest</code>  by @ydshieh in #20098</li>\n<li>Replace unsupported facebookresearch/bitsandbytes  by @tomaarsen in #20093</li>\n<li>docs: Resolve many typos in the English docs  by @tomaarsen in #20088</li>\n<li>use huggingface_hub.model_inifo() to get pipline_tag  by @y-tag in #20077</li>\n<li>Fix <code>generate_dummy_inputs</code> for <code>ImageGPTOnnxConfig</code>  by @ydshieh in #20103</li>\n<li>docs: Fixed variables in f-strings  by @tomaarsen in #20087</li>\n<li>Add new terms to the glossary  by @stevhliu in #20051</li>\n<li>Replace awkward timm link with the expected one  by @tomaarsen in #20109</li>\n<li>Fix AutoTokenizer with subfolder passed  by @sgugger in #20110</li>\n<li>[Audio Processor] Only pass sr to feat extractor  by @sanchit-gandhi in #20022</li>\n<li>Update github pr docs actions  by @mishig25 in #20125</li>\n<li>Adapt has_labels test when no labels were found  by @sgugger in #20113</li>\n<li>Improve tiny model creation script  by @ydshieh in #20119</li>\n<li>Remove BertConfig inheritance from RobertaConfig  by @Saad135 in #20124</li>\n<li>[Swin] Add Swin SimMIM checkpoints  by @NielsRogge in #20034</li>\n<li>Update <code>CLIPSegModelTester</code>  by @ydshieh in #20134</li>\n<li>Update SwinForMaskedImageModeling doctest values  by @amyeroberts in #20139</li>\n<li>Attempting to test automatically the <code>_keys_to_ignore</code>.  by @Narsil in #20042</li>\n<li>Generate: move generation_<em>.py src files into generation/</em>.py  by @gante in #20096</li>\n<li>add cv + audio labels  by @stevhliu in #20114</li>\n<li>Update VisionEncoderDecoder to use an image processor  by @amyeroberts in #20137</li>\n<li>[CLIPSeg] Add resources  by @NielsRogge in #20118</li>\n<li>Make DummyObject more robust  by @mariosasko in #20146</li>\n<li>Add <code>RoCBertTokenizer</code> to <code>TOKENIZER_MAPPING_NAMES</code>  by @ydshieh in #20141</li>\n<li>Adding support for LayoutLMvX variants for <code>object-detection</code>.  by @Narsil in #20143</li>\n<li>Add doc tests  by @NielsRogge in #20158</li>\n<li>doc comment fix: Args was in wrong place  by @hollance in #20164</li>\n<li>Update <code>OnnxConfig.generate_dummy_inputs</code> to check <code>ImageProcessingMixin</code>  by @ydshieh in #20157</li>\n<li>Generate: fix TF doctests  by @gante in #20159</li>\n<li>Fix arg names for our models  by @Rocketknight1 in #20166</li>\n<li>[processor] Add 'model input names' property  by @sanchit-gandhi in #20117</li>\n<li>Fix object-detection bug (height, width inversion).  by @Narsil in #20167</li>\n<li>[OWL-ViT] Make model consistent with CLIP  by @NielsRogge in #20144</li>\n<li>Fix type - update any PIL.Image.Resampling  by @amyeroberts in #20172</li>\n<li>Fix tapas scatter  by @Bearnardd in #20149</li>\n<li>Update README.md  by @code-with-rajeev in #19530</li>\n<li>Proposal Remove the weird <code>inspect</code> in ASR pipeline and make WhisperEncoder just nice to use.  by @Narsil in #19571</li>\n<li>Pytorch type hints  by @IMvision12 in #20112</li>\n<li>Generate: TF sample doctest result update  by @gante in #20208</li>\n<li>[ROC_BERT] Make CI happy  by @younesbelkada in #20175</li>\n<li>add _keys_to_ignore_on_load_unexpected = [r\"pooler\"]  by @ArthurZucker in #20210</li>\n<li>docs: translated index page to korean  by @wonhyeongseo in #20180</li>\n<li>feat: add i18n issue template  by @wonhyeongseo in #20199</li>\n<li>[Examples] Generalise Seq2Seq ASR to handle Whisper  by @sanchit-gandhi in #19519</li>\n<li>mark <code>test_save_load_fast_init_from_base</code> as <code>is_flaky</code>  by @ydshieh in #20200</li>\n<li>Update README.md  by @Nietism in #20188</li>\n<li>Downgrade log warning -&gt; info  by @amyeroberts in #20202</li>\n<li>Generate: add Bloom fixes for contrastive search  by @gante in #20213</li>\n<li>Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm.  by @Narsil in #20104</li>\n<li>[docs] set overflowing image width to auto-scale  by @wonhyeongseo in #20197</li>\n<li>Update tokenizer_summary.mdx  by @bofenghuang in #20135</li>\n<li>Make <code>ImageSegmentationPipelineTests</code> less flaky  by @ydshieh in #20147</li>\n<li>update relative positional embedding  by @ArthurZucker in #20203</li>\n<li>[WHISPER] Update modeling tests  by @ArthurZucker in #20162</li>\n<li>Add <code>accelerate</code> support for <code>ViT</code> family  by @younesbelkada in #20174</li>\n<li>Add param_name to size_dict logs &amp; tidy  by @amyeroberts in #20205</li>\n<li>Add object detection + segmentation transforms  by @amyeroberts in #20003</li>\n<li>Typo on doctring in ElectraTokenizer  by @FacerAin in #20192</li>\n<li>Remove <code>authorized_missing_keys</code>in favor of _keys_to_ignore_on_load_missing  by @ArthurZucker in #20228</li>\n<li>Add missing ESM autoclass  by @Rocketknight1 in #20177</li>\n<li>fix device issue  by @ydshieh in #20227</li>\n<li>fixed spelling error in testing.mdx  by @kasmith11 in #20220</li>\n<li>Fix <code>run_clip.py</code>  by @ydshieh in #20234</li>\n<li>Fix docstring of CLIPTokenizer(Fast)  by @TilmannR in #20233</li>\n<li>Fix MaskformerFeatureExtractor  by @NielsRogge in #20100</li>\n<li>New logging support to \"Trainer\" Class (ClearML Logger)  by @skinan in #20184</li>\n<li>Enable PyTorch 1.13  by @sgugger in #20168</li>\n<li>[CLIP] allow loading projection layer in vision and text model  by @patil-suraj in #18962</li>\n<li>Slightly alter Keras dummy loss  by @Rocketknight1 in #20232</li>\n<li>Add to DeBERTa resources  by @Saad135 in #20155</li>\n<li>Add clip resources to the transformers documentation  by @ambujpawar in #20190</li>\n<li>Update reqs to include min gather_for_metrics Accelerate version  by @muellerzr in #20242</li>\n<li>Allow trainer to return eval. loss for CLIP-like models  by @ydshieh in #20214</li>\n<li>Adds image-guided object detection support to OWL-ViT  by @alaradirik in #20136</li>\n<li>Adding <code>audio-classification</code> example in the doc.  by @Narsil in #20235</li>\n<li>Updating the doctest for conversational.  by @Narsil in #20236</li>\n<li>Adding doctest for <code>fill-mask</code> pipeline.  by @Narsil in #20241</li>\n<li>Adding doctest for <code>feature-extraction</code>.  by @Narsil in #20240</li>\n<li>Adding ASR pipeline example.  by @Narsil in #20226</li>\n<li>Adding doctest for document-question-answering  by @Narsil in #20239</li>\n<li>Adding an example for <code>depth-estimation</code> pipeline.  by @Narsil in #20237</li>\n<li>Complete doc migration  by @mishig25 in #20267</li>\n<li>Fix result saving errors of pytorch examples  by @li-plus in #20276</li>\n<li>Adding a doctest for <code>table-question-answering</code> pipeline.  by @Narsil in #20260</li>\n<li>Adding doctest for <code>image-segmentation</code> pipeline.  by @Narsil in #20256</li>\n<li>Adding doctest for <code>text2text-generation</code> pipeline.  by @Narsil in #20261</li>\n<li>Adding doctest for <code>text-generation</code> pipeline.  by @Narsil in #20264</li>\n<li>Add TF protein notebook to notebooks doc  by @Rocketknight1 in #20271</li>\n<li>Rephrasing the link.  by @Narsil in #20253</li>\n<li>Add Chinese-CLIP implementation  by @yangapku in #20368</li>\n<li>Adding doctest example for <code>image-classification</code> pipeline.  by @Narsil in #20254</li>\n<li>Adding doctest for <code>zero-shot-image-classification</code> pipeline.  by @Narsil in #20272</li>\n<li>Adding doctest for <code>zero-shot-classification</code> pipeline.  by @Narsil in #20268</li>\n<li>Adding doctest for <code>visual-question-answering</code> pipeline.  by @Narsil in #20266</li>\n<li>Adding doctest for <code>text-classification</code> pipeline.  by @Narsil in #20262</li>\n<li>Adding doctest for <code>question-answering</code> pipeline.  by @Narsil in #20259</li>\n<li>[Docs] Add resources of OpenAI GPT  by @shogohida in #20084</li>\n<li>Adding doctest for <code>image-to-text</code> pipeline.  by @Narsil in #20257</li>\n<li>Adding doctest for <code>token-classification</code> pipeline.  by @Narsil in #20265</li>\n<li>remaining pytorch type hints  by @IMvision12 in #20217</li>\n<li>Data collator for token classification pads labels column when receives pytorch tensors  by @markovalexander in #20244</li>\n<li>[Doctest] Add configuration_deformable_detr.py  by @Saad135 in #20273</li>\n<li>Fix summarization script  by @muellerzr in #20286</li>\n<li>[DOCTEST] Fix the documentation of RoCBert  by @ArthurZucker in #20142</li>\n<li>[bnb] Let's warn users when saving 8-bit models  by @younesbelkada in #20282</li>\n<li>Adding <code>zero-shot-object-detection</code> pipeline doctest.  by @Narsil in #20274</li>\n<li>Adding doctest for <code>object-detection</code> pipeline.  by @Narsil in #20258</li>\n<li>Image transforms functionality used instead  by @amyeroberts in #20278</li>\n<li>TF: add test for <code>PushToHubCallback</code>  by @gante in #20231</li>\n<li>Generate: general TF XLA constrastive search are now slow tests  by @gante in #20277</li>\n<li>Fixing the doctests failures.  by @Narsil in #20294</li>\n<li>set the default cache_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast  by @sywangyi in #20289</li>\n<li>Add docstrings for canine model  by @raghavanone in #19457</li>\n<li>Add missing report button for Example test  by @ydshieh in #20293</li>\n<li>refactor test  by @younesbelkada in #20300</li>\n<li>[Tiny model creation] deal with <code>ImageProcessor</code>  by @ydshieh in #20298</li>\n<li>Fix blender bot missleading doc  by @ArthurZucker in #20301</li>\n<li>remove two tokens that should not be suppressed  by @ArthurZucker in #20302</li>\n<li>[ASR Examples] Update README for Whisper  by @sanchit-gandhi in #20230</li>\n<li>Add padding image transformation  by @amyeroberts in #19838</li>\n<li>Pin TensorFlow  by @sgugger in #20313</li>\n<li>Add AnyPrecisionAdamW optimizer  by @atturaioe in #18961</li>\n<li>[Proposal] Breaking change <code>zero-shot-object-detection</code> for improved     consistency.  by @Narsil in #20280</li>\n<li>Fix flakey test with seed  by @muellerzr in #20318</li>\n<li>Pin TF 2.10.1 for Push CI  by @ydshieh in #20319</li>\n<li>Remove double brackets  by @stevhliu in #20307</li>\n<li>TF: future proof our keras imports  by @gante in #20317</li>\n<li>organize pipelines by modality  by @stevhliu in #20306</li>\n<li>Fix torch device issues  by @ydshieh in #20304</li>\n<li>Generate: add generation config class  by @gante in #20218</li>\n<li>translate zh quicktour by @bfss in #20095) </li>\n<li>Add Spanish translation of serialization.mdx  by @donelianc in #20245</li>\n<li>Add LayerScale to NAT/DiNAT  by @alihassanijr in #20325</li>\n<li>[Switch Transformers] Fix failing slow test  by @younesbelkada in #20346</li>\n<li>fix: \"BigSicence\" typo in docs  by @rajrajhans in #20331</li>\n<li>Generate: <code>model_kwargs</code> can also be an input to <code>prepare_inputs_for_generation</code>  by @gante in #20353</li>\n<li>Update Special Language Tokens for PLBART  by @jordiclive in #19980</li>\n<li>Add resources  by @NielsRogge in #20296</li>\n<li>Enhance HfArgumentParser functionality and ease of use  by @konstantinjdobler in #20323</li>\n<li>Add inference section to task guides  by @stevhliu in #18781</li>\n<li>Fix toctree for Section 3 in Spanish Documentation  by @donelianc in #20360</li>\n<li>Generate: shorter XLA contrastive search tests  by @gante in #20354</li>\n<li>revert <code>keys_to_ignore</code> for M2M100  by @younesbelkada in #20381</li>\n<li>add <code>accelerate</code> support for <code>ESM</code>  by @younesbelkada in #20379</li>\n<li>Fix nightly runs  by @sgugger in #20352</li>\n<li>Optimizes DonutProcessor token2json method for speed  by @michaelnation26 in #20283</li>\n<li>Indicate better minimal version of PyTorch in big model inference  by @sgugger in #20385</li>\n<li>Fix longformer onnx broken export  by @fxmarty in #20292</li>\n<li>Use tiny models for ONNX tests - text modality  by @lewtun in #20333</li>\n<li>[ESM] fix <code>accelerate</code> tests for esmfold  by @younesbelkada in #20387</li>\n<li>Generate: fix plbart generation tests  by @gante in #20391</li>\n<li>[bloom] convert script tweaks  by @stas00 in #18593</li>\n<li>Fix doctest file path  by @ydshieh in #20400</li>\n<li>[Image Transformers] to_pil fix float edge cases  by @patrickvonplaten in #20406</li>\n<li>make daily CI happy  by @younesbelkada in #20410</li>\n<li>fix nasty <code>bnb</code> bug  by @younesbelkada in #20408</li>\n<li>change the way sentinel tokens can retrived  by @raghavanone in #20373</li>\n<li>[BNB] Throw <code>ValueError</code> when trying to cast or assign  by @younesbelkada in #20409</li>\n<li>Use updated <code>model_max_length</code> when saving tokenizers  by @ydshieh in #20401</li>\n<li>Add Spanish translation of pr_checks.mdx  by @donelianc in #20339</li>\n<li>fix device in longformer onnx path  by @fxmarty in #20419</li>\n<li>Fix ModelOutput instantiation when there is only one tuple  by @sgugger in #20416</li>\n<li><code>accelerate</code> support for <code>OwlViT</code>  by @younesbelkada in #20411</li>\n<li>[AnyPrecisionAdamW] test fix  by @stas00 in #20454</li>\n<li>fix <code>word_to_tokens</code> docstring format  by @SaulLu in #20450</li>\n<li>Fix typo in FSMT Tokenizer  by @kamalkraj in #20456</li>\n<li>Fix device issues in <code>CLIPSegModelIntegrationTest</code>  by @ydshieh in #20467</li>\n<li>Fix links for <code>contrastive_loss</code>  by @ydshieh in #20455</li>\n<li>Fix doctests for audio models  by @ydshieh in #20468</li>\n<li>Fix ESM checkpoints for tests  by @Rocketknight1 in #20436</li>\n<li>More TF int dtype fixes  by @Rocketknight1 in #20384</li>\n<li>make tensors in function build_relative_position created on proper device instead of always on cpu  by @qq775294390 in #20434</li>\n<li>update cpu related doc  by @sywangyi in #20444</li>\n<li>with pytorch cpu only version. without --no_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch&gt;=1.10, using Ampere GPU with cuda&gt;=11.0\"  by @sywangyi in #20445</li>\n<li>[CLIPTokenizer] Improve warning  by @patrickvonplaten in #20458</li>\n<li>Replace assertions with value errors on distilbert model  by @JuheonChu in #20463</li>\n<li>[Doctest] Add configuration_fsmt.py  by @sha016 in #19936</li>\n<li>Replace assertion with ValueError exceptions in run_image_captioning_flax.py  by @katiele47 in #20365</li>\n<li>[FLAX] Add dtype to embedding for bert/bart/opt/t5  by @merrymercy in #20340</li>\n<li>fix both failing RoCBert tests  by @ArthurZucker in #20469</li>\n<li>Include image processor in add-new-model-like  by @amyeroberts in #20439</li>\n<li>chore: add link to the video cls notebook.  by @sayakpaul in #20386</li>\n<li>add timeout option for deepspeed engine  by @henghuiz in #20443</li>\n<li>[Maskformer] Add MaskFormerSwin backbone  by @NielsRogge in #20344</li>\n<li>Extract warnings from CI artifacts  by @ydshieh in #20474</li>\n<li>Add Donut image processor  by @amyeroberts in #20425</li>\n<li>Fix torch meshgrid warnings  by @fxmarty in #20475</li>\n<li>Fix init import_structure sorting  by @sgugger in #20477</li>\n<li>extract warnings in GH workflows  by @ydshieh in #20487</li>\n<li>add in layer gpt2 tokenizer  by @piEsposito in #20421</li>\n<li>Replace assert statements with raise exceptions  by @miyu386 in #20478</li>\n<li>fixed small typo  by @sandeepgadhwal in #20490</li>\n<li>Fix documentation code to import facebook/detr-resnet-50 model  by @JuanFKurucz in #20491</li>\n<li>Fix disk offload for full safetensors checkpoints  by @sgugger in #20497</li>\n<li>[modelcard] Check for IterableDataset  by @sanchit-gandhi in #20495</li>\n<li>[modelcard] Set model name if empty  by @sanchit-gandhi in #20496</li>\n<li>Add segmentation + object detection image processors  by @amyeroberts in #20160</li>\n<li>remove <code>attention_mask</code> truncation in whisper  by @ydshieh in #20488</li>\n<li>Make <code>add_special_tokens</code> more clear  by @ydshieh in #20424</li>\n<li>[OPT/Galactica] Load large <code>galactica</code> models  by @younesbelkada in #20390</li>\n<li>Support extraction of both train and eval XLA graphs  by @jeffhataws in #20492</li>\n<li>fix ipex+fp32 jit trace error in ipex 1.13  by @sywangyi in #20504</li>\n<li>Expected output for the test changed  by @ArthurZucker in #20493</li>\n<li>Fix TF nightly tests  by @Rocketknight1 in #20507</li>\n<li>Update doc examples feature extractor -&gt; image processor  by @amyeroberts in #20501</li>\n<li>Fix Typo in Docs for GPU  by @julianpollmann in #20509</li>\n<li>Fix minimum version for device_map  by @sgugger in #20489</li>\n<li>Update <code>AutomaticSpeechRecognitionPipeline</code> doc example  by @ydshieh in #20512</li>\n<li>Add <code>natten</code> for CI  by @ydshieh in #20511</li>\n<li>Fix Data2VecTextForCasualLM example code documentation  by @JuanFKurucz in #20510</li>\n<li>Add some warning for Dynamo and enable TF32 when it's set  by @sgugger in #20515</li>\n<li>[modelcard] Update dataset tags  by @sanchit-gandhi in #20506</li>\n<li>Change Doctests CI launch time  by @ydshieh in #20523</li>\n<li>Fix <code>PLBart</code> doctest  by @ydshieh in #20527</li>\n<li>Fix <code>ConditionalDetrForSegmentation</code> doc example  by @ydshieh in #20531</li>\n<li>add doc for  by @younesbelkada in #20525</li>\n<li>Update <code>ZeroShotObjectDetectionPipeline</code> doc example  by @ydshieh in #20528</li>\n<li>update post_process_image_guided_detection  by @fcakyon in #20521</li>\n<li>QnA example: add speed metric  by @sywangyi in #20522</li>\n<li>Fix doctest  by @NielsRogge in #20534</li>\n<li>Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code  by @JuanFKurucz in #20516</li>\n<li>Fix link in pipeline device map  by @stevhliu in #20517</li>\n</ul>\nSignificant community contributions\n<p>The following contributors have made significant changes to the library over the last release:</p>\n<ul>\n<li>@sww9370<ul>\n<li>Add RocBert (#20013)</li>\n</ul>\n</li>\n<li>@IMvision12<ul>\n<li>Pytorch type hints (#20112)</li>\n<li>remaining pytorch type hints (#20217)</li>\n</ul>\n</li>\n<li>@alihassanijr<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)</li>\n<li>Add LayerScale to NAT/DiNAT (#20325)</li>\n</ul>\n</li>\n<li>@bfss<ul>\n<li>translate zh quicktour(#20095) (#20181)</li>\n</ul>\n</li>\n<li>@donelianc<ul>\n<li>Add Spanish translation of serialization.mdx (#20245)</li>\n<li>Fix toctree for Section 3 in Spanish Documentation (#20360)</li>\n<li>Add Spanish translation of pr_checks.mdx (#20339)</li>\n</ul>\n</li>\n<li>@yangapku<ul>\n<li>Add Chinese-CLIP implementation (#20368)</li>\n</ul>\n</li>\n</ul>",
+  "author": "Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.",
+  "author_list": [
+    {
+      "family": "Wolf",
+      "given": "Thomas"
+    },
+    {
+      "family": "Debut",
+      "given": "Lysandre"
+    },
+    {
+      "family": "Sanh",
+      "given": "Victor"
+    },
+    {
+      "family": "Chaumond",
+      "given": "Julien"
+    },
+    {
+      "family": "Delangue",
+      "given": "Clement"
+    },
+    {
+      "family": "Moi",
+      "given": "Anthony"
+    },
+    {
+      "family": "Cistac",
+      "given": "Perric"
+    },
+    {
+      "family": "Ma",
+      "given": "Clara"
+    },
+    {
+      "family": "Jernite",
+      "given": "Yacine"
+    },
+    {
+      "family": "Plu",
+      "given": "Julien"
+    },
+    {
+      "family": "Xu",
+      "given": "Canwen"
+    },
+    {
+      "family": "Le Scao",
+      "given": "Teven"
+    },
+    {
+      "family": "Gugger",
+      "given": "Sylvain"
+    },
+    {
+      "family": "Drame",
+      "given": "Mariama"
+    },
+    {
+      "family": "Lhoest",
+      "given": "Quentin"
+    },
+    {
+      "family": "Rush",
+      "given": "Alexander M."
+    }
+  ],
+  "day": 1,
+  "doi": "10.5281/zenodo.7391177",
+  "eprint": 7391177,
+  "license": "other-open",
+  "month": 10,
+  "note": "If you use this software, please cite it using these metadata.",
+  "pubstate": "published",
+  "revision": 3,
+  "title": "Transformers: State-of-the-Art Natural Language Processing",
+  "type": "software",
+  "url": "https://zenodo.org/api/records/7391177",
+  "version": "v4.25.1",
+  "year": 2020
+}
\ No newline at end of file
diff --git a/tests/resources/zenodo/7391177_out.json b/tests/resources/zenodo/7391177_out.json
index 2b0a4bf4..b3ddad56 100644
--- a/tests/resources/zenodo/7391177_out.json
+++ b/tests/resources/zenodo/7391177_out.json
@@ -1,5 +1,5 @@
 {
-  "abstract": "PyTorch 2.0 stack support\nWe are very excited by the newly announced PyTorch 2.0 stack. You can enable `torch.compile` on any of our models, and get support with the `Trainer` (and in all our PyTorch examples) by using the `torchdynamo` training argument. For instance, just add `--torchdynamo inductor` when launching those examples from the command line.\n\n\nThis API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.\n\n\nNote that to get the best performance, we recommend:\n\n\n* using an Ampere GPU (or more recent)\n* sticking to fixed shaped for now (so use `--pad_to_max_length` in our examples)\n* Repurpose torchdynamo training args towards torch.\\_dynamo by @sgugger in #20498\n\n\nAudio Spectrogram Transformer\nThe Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.\n\n\n* Add Audio Spectogram Transformer by @NielsRogge in #19981\n\n\nJukebox\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.\n\n\n* Add Jukebox model (replaces #16875) by @ArthurZucker in #17826\n\n\nSwitch Transformers\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n\n\nIt is the first MoE model supported in `transformers`, with the largest checkpoint currently available currently containing 1T parameters.\n\n\n* Add Switch transformers by @younesbelkada and @ArthurZucker in #19323\n\n\nRocBert\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\n\n* Add RocBert by @sww9370 in #20013\n\n\nCLIPSeg\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker. [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.\n\n\n* Add CLIPSeg by @NielsRogge in #20066\n\n\nNAT and DiNAT\nNAT\nNAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\n\nDiNAT\nDiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n\n\nIt extends [NAT](https://huggingface.co/docs/transformers/main/en/model_doc/nat) by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.\n\n\n* Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models by @alihassanijr in #20219\n\n\nMobileNetV2\nThe MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n\n\n* add MobileNetV2 model by @hollance in #17845\n\n\nMobileNetV1\nThe MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n\n\n* add MobileNetV1 model by @hollance in #17799\n\n\nImage processors\nImage processors replace feature extractors as the processing class for computer vision models.\n\n\nImportant changes:\n\n\n* `size` parameter is now a dictionary of `{\"height\": h, \"width\": w}`, `{\"shortest_edge\": s}`, `{\"shortest_egde\": s, \"longest_edge\": l}` instead of int or tuple.\n* Addition of `data_format` flag. You can now specify if you want your images to be returned in `\"channels_first\"` - NCHW - or `\"channels_last\"` - NHWC - format.\n* Processing flags e.g. `do_resize` can be passed directly to the `preprocess` method instead of modifying the class attribute: `image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")`\n* Leaving `return_tensors` unset will return a list of numpy arrays.\n\n\nThe classes are backwards compatible and can be created using existing feature extractor configurations - with the `size` parameter converted.\n\n\n* Add Image Processors by @amyeroberts in #19796\n* Add Donut image processor by @amyeroberts #20425\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* AutoImageProcessor by @amyeroberts in #20111\n\n\nBackbone for computer vision models\nWe're adding support for a general `AutoBackbone` class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.\n\n\n* Add AutoBackbone + ResNetBackbone by @NielsRogge in #20229\n* Improve backbone by @NielsRogge in #20380\n* [AutoBackbone] Improve API by @NielsRogge in #20407\n\n\nSupport for `safetensors` offloading\nIf the model you are using has a `safetensors` checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.\n\n\n* Safetensors offload by @sgugger in #20321\n\n\nContrastive search in the `generate` method\n* Generate: TF contrastive search with XLA support by @gante in #20050\n* Generate: contrastive search with full optional outputs by @gante in #19963\n\n\nBreaking changes\n* 🚨 🚨 🚨 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in `convert_tokens_to_string` by @beneyal in #15775\n\n\nBugfixes and improvements\n* add dataset by @stevhliu in #20005\n* Add BERT resources by @stevhliu in #19852\n* Add LayoutLMv3 resource by @stevhliu in #19932\n* fix typo by @stevhliu in #20006\n* Update object detection pipeline to use post\\_process\\_object\\_detection methods by @alaradirik in #20004\n* clean up vision/text config dict arguments by @ydshieh in #19954\n* make sentencepiece import conditional in bertjapanesetokenizer by @ripose-jp in #20012\n* Fix gradient checkpoint test in encoder-decoder by @ydshieh in #20017\n* Quality by @sgugger in #20002\n* Update auto processor to check image processor created by @amyeroberts in #20021\n* [Doctest] Add configuration\\_deberta\\_v2.py by @Saad135 in #19995\n* Improve model tester by @ydshieh in #19984\n* Fix doctest by @ydshieh in #20023\n* Show installed libraries and their versions in CI jobs by @ydshieh in #20026\n* reorganize glossary by @stevhliu in #20010\n* Now supporting pathlike in pipelines too. by @Narsil in #20030\n* Add \\*\\*kwargs by @amyeroberts in #20037\n* Fix some doctests after PR 15775 by @ydshieh in #20036\n* [Doctest] Add configuration\\_camembert.py by @Saad135 in #20039\n* [Whisper Tokenizer] Make more user-friendly by @sanchit-gandhi in #19921\n* [FuturWarning] Add futur warning for LEDForSequenceClassification by @ArthurZucker in #19066\n* fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc by @sywangyi in #19891\n* Update esmfold conversion script by @Rocketknight1 in #20028\n* Fixed torch.finfo issue with torch.fx by @michaelbenayoun in #20040\n* Only resize embeddings when necessary by @sgugger in #20043\n* Speed up TF token classification postprocessing by converting complete tensors to numpy by @deutschmn in #19976\n* Fix ESM LM head test by @Rocketknight1 in #20045\n* Update README.md by @bofenghuang in #20063\n* fix `tokenizer_type` to avoid error when loading checkpoint back by @pacman100 in #20062\n* [Trainer] Fix model name in push\\_to\\_hub by @sanchit-gandhi in #20064\n* PoolformerImageProcessor defaults to match previous FE by @amyeroberts in #20048\n* change constant torch.tensor to torch.full by @MerHS in #20061\n* Update READMEs for ESMFold and add notebooks by @Rocketknight1 in #20067\n* Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2 by @jordiclive in #20068\n* Allow passing arguments to model testers for CLIP-like models by @ydshieh in #20044\n* Show installed libraries and their versions in GA jobs by @ydshieh in #20069\n* Update defaults and logic to match old FE by @amyeroberts in #20065\n* Update modeling\\_tf\\_utils.py by @cakiki in #20076\n* Update hub.py by @cakiki in #20075\n* [Doctest] Add configuration\\_dpr.py by @Saad135 in #20080\n* Removing RobertaConfig inheritance from CamembertConfig by @Saad135 in #20059\n* Skip 2 tests in `VisionTextDualEncoderProcessorTest` by @ydshieh in #20098\n* Replace unsupported facebookresearch/bitsandbytes by @tomaarsen in #20093\n* docs: Resolve many typos in the English docs by @tomaarsen in #20088\n* use huggingface\\_hub.model\\_inifo() to get pipline\\_tag by @y-tag in #20077\n* Fix `generate_dummy_inputs` for `ImageGPTOnnxConfig` by @ydshieh in #20103\n* docs: Fixed variables in f-strings by @tomaarsen in #20087\n* Add new terms to the glossary by @stevhliu in #20051\n* Replace awkward timm link with the expected one by @tomaarsen in #20109\n* Fix AutoTokenizer with subfolder passed by @sgugger in #20110\n* [Audio Processor] Only pass sr to feat extractor by @sanchit-gandhi in #20022\n* Update github pr docs actions by @mishig25 in #20125\n* Adapt has\\_labels test when no labels were found by @sgugger in #20113\n* Improve tiny model creation script by @ydshieh in #20119\n* Remove BertConfig inheritance from RobertaConfig by @Saad135 in #20124\n* [Swin] Add Swin SimMIM checkpoints by @NielsRogge in #20034\n* Update `CLIPSegModelTester` by @ydshieh in #20134\n* Update SwinForMaskedImageModeling doctest values by @amyeroberts in #20139\n* Attempting to test automatically the `_keys_to_ignore`. by @Narsil in #20042\n* Generate: move generation\\_*.py src files into generation/*.py by @gante in #20096\n* add cv + audio labels by @stevhliu in #20114\n* Update VisionEncoderDecoder to use an image processor by @amyeroberts in #20137\n* [CLIPSeg] Add resources by @NielsRogge in #20118\n* Make DummyObject more robust by @mariosasko in #20146\n* Add `RoCBertTokenizer` to `TOKENIZER_MAPPING_NAMES` by @ydshieh in #20141\n* Adding support for LayoutLMvX variants for `object-detection`. by @Narsil in #20143\n* Add doc tests by @NielsRogge in #20158\n* doc comment fix: Args was in wrong place by @hollance in #20164\n* Update `OnnxConfig.generate_dummy_inputs` to check `ImageProcessingMixin` by @ydshieh in #20157\n* Generate: fix TF doctests by @gante in #20159\n* Fix arg names for our models by @Rocketknight1 in #20166\n* [processor] Add 'model input names' property by @sanchit-gandhi in #20117\n* Fix object-detection bug (height, width inversion). by @Narsil in #20167\n* [OWL-ViT] Make model consistent with CLIP by @NielsRogge in #20144\n* Fix type - update any PIL.Image.Resampling by @amyeroberts in #20172\n* Fix tapas scatter by @Bearnardd in #20149\n* Update README.md by @code-with-rajeev in #19530\n* Proposal Remove the weird `inspect` in ASR pipeline and make WhisperEncoder just nice to use. by @Narsil in #19571\n* Pytorch type hints by @IMvision12 in #20112\n* Generate: TF sample doctest result update by @gante in #20208\n* [ROC\\_BERT] Make CI happy by @younesbelkada in #20175\n* add \\_keys\\_to\\_ignore\\_on\\_load\\_unexpected = [r\"pooler\"] by @ArthurZucker in #20210\n* docs: translated index page to korean by @wonhyeongseo in #20180\n* feat: add i18n issue template by @wonhyeongseo in #20199\n* [Examples] Generalise Seq2Seq ASR to handle Whisper by @sanchit-gandhi in #19519\n* mark `test_save_load_fast_init_from_base` as `is_flaky` by @ydshieh in #20200\n* Update README.md by @Nietism in #20188\n* Downgrade log warning -> info by @amyeroberts in #20202\n* Generate: add Bloom fixes for contrastive search by @gante in #20213\n* Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm. by @Narsil in #20104\n* [docs] set overflowing image width to auto-scale by @wonhyeongseo in #20197\n* Update tokenizer\\_summary.mdx by @bofenghuang in #20135\n* Make `ImageSegmentationPipelineTests` less flaky by @ydshieh in #20147\n* update relative positional embedding by @ArthurZucker in #20203\n* [WHISPER] Update modeling tests by @ArthurZucker in #20162\n* Add `accelerate` support for `ViT` family by @younesbelkada in #20174\n* Add param\\_name to size\\_dict logs & tidy by @amyeroberts in #20205\n* Add object detection + segmentation transforms by @amyeroberts in #20003\n* Typo on doctring in ElectraTokenizer by @FacerAin in #20192\n* Remove `authorized_missing_keys`in favor of \\_keys\\_to\\_ignore\\_on\\_load\\_missing by @ArthurZucker in #20228\n* Add missing ESM autoclass by @Rocketknight1 in #20177\n* fix device issue by @ydshieh in #20227\n* fixed spelling error in testing.mdx by @kasmith11 in #20220\n* Fix `run_clip.py` by @ydshieh in #20234\n* Fix docstring of CLIPTokenizer(Fast) by @TilmannR in #20233\n* Fix MaskformerFeatureExtractor by @NielsRogge in #20100\n* New logging support to \"Trainer\" Class (ClearML Logger) by @skinan in #20184\n* Enable PyTorch 1.13 by @sgugger in #20168\n* [CLIP] allow loading projection layer in vision and text model by @patil-suraj in #18962\n* Slightly alter Keras dummy loss by @Rocketknight1 in #20232\n* Add to DeBERTa resources by @Saad135 in #20155\n* Add clip resources to the transformers documentation by @ambujpawar in #20190\n* Update reqs to include min gather\\_for\\_metrics Accelerate version by @muellerzr in #20242\n* Allow trainer to return eval. loss for CLIP-like models by @ydshieh in #20214\n* Adds image-guided object detection support to OWL-ViT by @alaradirik in #20136\n* Adding `audio-classification` example in the doc. by @Narsil in #20235\n* Updating the doctest for conversational. by @Narsil in #20236\n* Adding doctest for `fill-mask` pipeline. by @Narsil in #20241\n* Adding doctest for `feature-extraction`. by @Narsil in #20240\n* Adding ASR pipeline example. by @Narsil in #20226\n* Adding doctest for document-question-answering by @Narsil in #20239\n* Adding an example for `depth-estimation` pipeline. by @Narsil in #20237\n* Complete doc migration by @mishig25 in #20267\n* Fix result saving errors of pytorch examples by @li-plus in #20276\n* Adding a doctest for `table-question-answering` pipeline. by @Narsil in #20260\n* Adding doctest for `image-segmentation` pipeline. by @Narsil in #20256\n* Adding doctest for `text2text-generation` pipeline. by @Narsil in #20261\n* Adding doctest for `text-generation` pipeline. by @Narsil in #20264\n* Add TF protein notebook to notebooks doc by @Rocketknight1 in #20271\n* Rephrasing the link. by @Narsil in #20253\n* Add Chinese-CLIP implementation by @yangapku in #20368\n* Adding doctest example for `image-classification` pipeline. by @Narsil in #20254\n* Adding doctest for `zero-shot-image-classification` pipeline. by @Narsil in #20272\n* Adding doctest for `zero-shot-classification` pipeline. by @Narsil in #20268\n* Adding doctest for `visual-question-answering` pipeline. by @Narsil in #20266\n* Adding doctest for `text-classification` pipeline. by @Narsil in #20262\n* Adding doctest for `question-answering` pipeline. by @Narsil in #20259\n* [Docs] Add resources of OpenAI GPT by @shogohida in #20084\n* Adding doctest for `image-to-text` pipeline. by @Narsil in #20257\n* Adding doctest for `token-classification` pipeline. by @Narsil in #20265\n* remaining pytorch type hints by @IMvision12 in #20217\n* Data collator for token classification pads labels column when receives pytorch tensors by @markovalexander in #20244\n* [Doctest] Add configuration\\_deformable\\_detr.py by @Saad135 in #20273\n* Fix summarization script by @muellerzr in #20286\n* [DOCTEST] Fix the documentation of RoCBert by @ArthurZucker in #20142\n* [bnb] Let's warn users when saving 8-bit models by @younesbelkada in #20282\n* Adding `zero-shot-object-detection` pipeline doctest. by @Narsil in #20274\n* Adding doctest for `object-detection` pipeline. by @Narsil in #20258\n* Image transforms functionality used instead by @amyeroberts in #20278\n* TF: add test for `PushToHubCallback` by @gante in #20231\n* Generate: general TF XLA constrastive search are now slow tests by @gante in #20277\n* Fixing the doctests failures. by @Narsil in #20294\n* set the default cache\\_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast by @sywangyi in #20289\n* Add docstrings for canine model by @raghavanone in #19457\n* Add missing report button for Example test by @ydshieh in #20293\n* refactor test by @younesbelkada in #20300\n* [Tiny model creation] deal with `ImageProcessor` by @ydshieh in #20298\n* Fix blender bot missleading doc by @ArthurZucker in #20301\n* remove two tokens that should not be suppressed by @ArthurZucker in #20302\n* [ASR Examples] Update README for Whisper by @sanchit-gandhi in #20230\n* Add padding image transformation by @amyeroberts in #19838\n* Pin TensorFlow by @sgugger in #20313\n* Add AnyPrecisionAdamW optimizer by @atturaioe in #18961\n* [Proposal] Breaking change `zero-shot-object-detection` for improved consistency. by @Narsil in #20280\n* Fix flakey test with seed by @muellerzr in #20318\n* Pin TF 2.10.1 for Push CI by @ydshieh in #20319\n* Remove double brackets by @stevhliu in #20307\n* TF: future proof our keras imports by @gante in #20317\n* organize pipelines by modality by @stevhliu in #20306\n* Fix torch device issues by @ydshieh in #20304\n* Generate: add generation config class by @gante in #20218\n* translate zh quicktour by @bfss in #20095)\n* Add Spanish translation of serialization.mdx by @donelianc in #20245\n* Add LayerScale to NAT/DiNAT by @alihassanijr in #20325\n* [Switch Transformers] Fix failing slow test by @younesbelkada in #20346\n* fix: \"BigSicence\" typo in docs by @rajrajhans in #20331\n* Generate: `model_kwargs` can also be an input to `prepare_inputs_for_generation` by @gante in #20353\n* Update Special Language Tokens for PLBART by @jordiclive in #19980\n* Add resources by @NielsRogge in #20296\n* Enhance HfArgumentParser functionality and ease of use by @konstantinjdobler in #20323\n* Add inference section to task guides by @stevhliu in #18781\n* Fix toctree for Section 3 in Spanish Documentation by @donelianc in #20360\n* Generate: shorter XLA contrastive search tests by @gante in #20354\n* revert `keys_to_ignore` for M2M100 by @younesbelkada in #20381\n* add `accelerate` support for `ESM` by @younesbelkada in #20379\n* Fix nightly runs by @sgugger in #20352\n* Optimizes DonutProcessor token2json method for speed by @michaelnation26 in #20283\n* Indicate better minimal version of PyTorch in big model inference by @sgugger in #20385\n* Fix longformer onnx broken export by @fxmarty in #20292\n* Use tiny models for ONNX tests - text modality by @lewtun in #20333\n* [ESM] fix `accelerate` tests for esmfold by @younesbelkada in #20387\n* Generate: fix plbart generation tests by @gante in #20391\n* [bloom] convert script tweaks by @stas00 in #18593\n* Fix doctest file path by @ydshieh in #20400\n* [Image Transformers] to\\_pil fix float edge cases by @patrickvonplaten in #20406\n* make daily CI happy by @younesbelkada in #20410\n* fix nasty `bnb` bug by @younesbelkada in #20408\n* change the way sentinel tokens can retrived by @raghavanone in #20373\n* [BNB] Throw `ValueError` when trying to cast or assign by @younesbelkada in #20409\n* Use updated `model_max_length` when saving tokenizers by @ydshieh in #20401\n* Add Spanish translation of pr\\_checks.mdx by @donelianc in #20339\n* fix device in longformer onnx path by @fxmarty in #20419\n* Fix ModelOutput instantiation when there is only one tuple by @sgugger in #20416\n* `accelerate` support for `OwlViT` by @younesbelkada in #20411\n* [AnyPrecisionAdamW] test fix by @stas00 in #20454\n* fix `word_to_tokens` docstring format by @SaulLu in #20450\n* Fix typo in FSMT Tokenizer by @kamalkraj in #20456\n* Fix device issues in `CLIPSegModelIntegrationTest` by @ydshieh in #20467\n* Fix links for `contrastive_loss` by @ydshieh in #20455\n* Fix doctests for audio models by @ydshieh in #20468\n* Fix ESM checkpoints for tests by @Rocketknight1 in #20436\n* More TF int dtype fixes by @Rocketknight1 in #20384\n* make tensors in function build\\_relative\\_position created on proper device instead of always on cpu by @qq775294390 in #20434\n* update cpu related doc by @sywangyi in #20444\n* with pytorch cpu only version. without --no\\_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\" by @sywangyi in #20445\n* [CLIPTokenizer] Improve warning by @patrickvonplaten in #20458\n* Replace assertions with value errors on distilbert model by @JuheonChu in #20463\n* [Doctest] Add configuration\\_fsmt.py by @sha016 in #19936\n* Replace assertion with ValueError exceptions in run\\_image\\_captioning\\_flax.py by @katiele47 in #20365\n* [FLAX] Add dtype to embedding for bert/bart/opt/t5 by @merrymercy in #20340\n* fix both failing RoCBert tests by @ArthurZucker in #20469\n* Include image processor in add-new-model-like by @amyeroberts in #20439\n* chore: add link to the video cls notebook. by @sayakpaul in #20386\n* add timeout option for deepspeed engine by @henghuiz in #20443\n* [Maskformer] Add MaskFormerSwin backbone by @NielsRogge in #20344\n* Extract warnings from CI artifacts by @ydshieh in #20474\n* Add Donut image processor by @amyeroberts in #20425\n* Fix torch meshgrid warnings by @fxmarty in #20475\n* Fix init import\\_structure sorting by @sgugger in #20477\n* extract warnings in GH workflows by @ydshieh in #20487\n* add in layer gpt2 tokenizer by @piEsposito in #20421\n* Replace assert statements with raise exceptions by @miyu386 in #20478\n* fixed small typo by @sandeepgadhwal in #20490\n* Fix documentation code to import facebook/detr-resnet-50 model by @JuanFKurucz in #20491\n* Fix disk offload for full safetensors checkpoints by @sgugger in #20497\n* [modelcard] Check for IterableDataset by @sanchit-gandhi in #20495\n* [modelcard] Set model name if empty by @sanchit-gandhi in #20496\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* remove `attention_mask` truncation in whisper by @ydshieh in #20488\n* Make `add_special_tokens` more clear by @ydshieh in #20424\n* [OPT/Galactica] Load large `galactica` models by @younesbelkada in #20390\n* Support extraction of both train and eval XLA graphs by @jeffhataws in #20492\n* fix ipex+fp32 jit trace error in ipex 1.13 by @sywangyi in #20504\n* Expected output for the test changed by @ArthurZucker in #20493\n* Fix TF nightly tests by @Rocketknight1 in #20507\n* Update doc examples feature extractor -> image processor by @amyeroberts in #20501\n* Fix Typo in Docs for GPU by @julianpollmann in #20509\n* Fix minimum version for device\\_map by @sgugger in #20489\n* Update `AutomaticSpeechRecognitionPipeline` doc example by @ydshieh in #20512\n* Add `natten` for CI by @ydshieh in #20511\n* Fix Data2VecTextForCasualLM example code documentation by @JuanFKurucz in #20510\n* Add some warning for Dynamo and enable TF32 when it's set by @sgugger in #20515\n* [modelcard] Update dataset tags by @sanchit-gandhi in #20506\n* Change Doctests CI launch time by @ydshieh in #20523\n* Fix `PLBart` doctest by @ydshieh in #20527\n* Fix `ConditionalDetrForSegmentation` doc example by @ydshieh in #20531\n* add doc for by @younesbelkada in #20525\n* Update `ZeroShotObjectDetectionPipeline` doc example by @ydshieh in #20528\n* update post\\_process\\_image\\_guided\\_detection by @fcakyon in #20521\n* QnA example: add speed metric by @sywangyi in #20522\n* Fix doctest by @NielsRogge in #20534\n* Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code by @JuanFKurucz in #20516\n* Fix link in pipeline device map by @stevhliu in #20517\n\n\nSignificant community contributions\nThe following contributors have made significant changes to the library over the last release:\n\n\n* @sww9370\n\t+ Add RocBert (#20013)\n* @IMvision12\n\t+ Pytorch type hints (#20112)\n\t+ remaining pytorch type hints (#20217)\n* @alihassanijr\n\t+ Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)\n\t+ Add LayerScale to NAT/DiNAT (#20325)\n* @bfss\n\t+ translate zh quicktour(#20095) (#20181)\n* @donelianc\n\t+ Add Spanish translation of serialization.mdx (#20245)\n\t+ Fix toctree for Section 3 in Spanish Documentation (#20360)\n\t+ Add Spanish translation of pr\\_checks.mdx (#20339)\n* @yangapku\n\t+ Add Chinese-CLIP implementation (#20368)",
+  "abstract": "PyTorch 2.0 stack support\n\nWe are very excited by the newly announced PyTorch 2.0 stack. You can enable `torch.compile` on any of our models, and get support with the `Trainer` (and in all our PyTorch examples) by using the `torchdynamo` training argument. For instance, just add `--torchdynamo inductor` when launching those examples from the command line.\n\nThis API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.\n\nNote that to get the best performance, we recommend:\n\n* using an Ampere GPU (or more recent)\n* sticking to fixed shaped for now (so use `--pad_to_max_length` in our examples)\n* Repurpose torchdynamo training args towards torch.\\_dynamo by @sgugger in #20498\n\nAudio Spectrogram Transformer\n\nThe Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.\n\n* Add Audio Spectogram Transformer by @NielsRogge in #19981\n\nJukebox\n\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.\n\n* Add Jukebox model (replaces #16875) by @ArthurZucker in #17826\n\nSwitch Transformers\n\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n\nIt is the first MoE model supported in `transformers`, with the largest checkpoint currently available currently containing 1T parameters.\n\n* Add Switch transformers by @younesbelkada and @ArthurZucker in #19323\n\nRocBert\n\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\n* Add RocBert by @sww9370 in #20013\n\nCLIPSeg\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker. [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.\n\n* Add CLIPSeg by @NielsRogge in #20066\n\nNAT and DiNAT\nNAT\n\nNAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\nDiNAT\n\nDiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n\nIt extends [NAT](https://huggingface.co/docs/transformers/main/en/model_doc/nat) by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.\n\n* Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models by @alihassanijr in #20219\n\nMobileNetV2\n\nThe MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n\n* add MobileNetV2 model by @hollance in #17845\n\nMobileNetV1\n\nThe MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n\n* add MobileNetV1 model by @hollance in #17799\n\nImage processors\n\nImage processors replace feature extractors as the processing class for computer vision models.\n\nImportant changes:\n\n* `size` parameter is now a dictionary of `{\"height\": h, \"width\": w}`, `{\"shortest_edge\": s}`, `{\"shortest_egde\": s, \"longest_edge\": l}` instead of int or tuple.\n* Addition of `data_format` flag. You can now specify if you want your images to be returned in `\"channels_first\"` - NCHW - or `\"channels_last\"` - NHWC - format.\n* Processing flags e.g. `do_resize` can be passed directly to the `preprocess` method instead of modifying the class attribute: `image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")`\n* Leaving `return_tensors` unset will return a list of numpy arrays.\n\nThe classes are backwards compatible and can be created using existing feature extractor configurations - with the `size` parameter converted.\n\n* Add Image Processors by @amyeroberts in #19796\n* Add Donut image processor by @amyeroberts #20425\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* AutoImageProcessor by @amyeroberts in #20111\n\nBackbone for computer vision models\n\nWe're adding support for a general `AutoBackbone` class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.\n\n* Add AutoBackbone + ResNetBackbone by @NielsRogge in #20229\n* Improve backbone by @NielsRogge in #20380\n* [AutoBackbone] Improve API by @NielsRogge in #20407\n\nSupport for `safetensors` offloading\n\nIf the model you are using has a `safetensors` checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.\n\n* Safetensors offload by @sgugger in #20321\n\nContrastive search in the `generate` method\n\n* Generate: TF contrastive search with XLA support by @gante in #20050\n* Generate: contrastive search with full optional outputs by @gante in #19963\n\nBreaking changes\n\n* 🚨 🚨 🚨 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in `convert_tokens_to_string` by @beneyal in #15775\n\nBugfixes and improvements\n\n* add dataset by @stevhliu in #20005\n* Add BERT resources by @stevhliu in #19852\n* Add LayoutLMv3 resource by @stevhliu in #19932\n* fix typo by @stevhliu in #20006\n* Update object detection pipeline to use post\\_process\\_object\\_detection methods by @alaradirik in #20004\n* clean up vision/text config dict arguments by @ydshieh in #19954\n* make sentencepiece import conditional in bertjapanesetokenizer by @ripose-jp in #20012\n* Fix gradient checkpoint test in encoder-decoder by @ydshieh in #20017\n* Quality by @sgugger in #20002\n* Update auto processor to check image processor created by @amyeroberts in #20021\n* [Doctest] Add configuration\\_deberta\\_v2.py by @Saad135 in #19995\n* Improve model tester by @ydshieh in #19984\n* Fix doctest by @ydshieh in #20023\n* Show installed libraries and their versions in CI jobs by @ydshieh in #20026\n* reorganize glossary by @stevhliu in #20010\n* Now supporting pathlike in pipelines too. by @Narsil in #20030\n* Add \\*\\*kwargs by @amyeroberts in #20037\n* Fix some doctests after PR 15775 by @ydshieh in #20036\n* [Doctest] Add configuration\\_camembert.py by @Saad135 in #20039\n* [Whisper Tokenizer] Make more user-friendly by @sanchit-gandhi in #19921\n* [FuturWarning] Add futur warning for LEDForSequenceClassification by @ArthurZucker in #19066\n* fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc by @sywangyi in #19891\n* Update esmfold conversion script by @Rocketknight1 in #20028\n* Fixed torch.finfo issue with torch.fx by @michaelbenayoun in #20040\n* Only resize embeddings when necessary by @sgugger in #20043\n* Speed up TF token classification postprocessing by converting complete tensors to numpy by @deutschmn in #19976\n* Fix ESM LM head test by @Rocketknight1 in #20045\n* Update README.md by @bofenghuang in #20063\n* fix `tokenizer_type` to avoid error when loading checkpoint back by @pacman100 in #20062\n* [Trainer] Fix model name in push\\_to\\_hub by @sanchit-gandhi in #20064\n* PoolformerImageProcessor defaults to match previous FE by @amyeroberts in #20048\n* change constant torch.tensor to torch.full by @MerHS in #20061\n* Update READMEs for ESMFold and add notebooks by @Rocketknight1 in #20067\n* Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2 by @jordiclive in #20068\n* Allow passing arguments to model testers for CLIP-like models by @ydshieh in #20044\n* Show installed libraries and their versions in GA jobs by @ydshieh in #20069\n* Update defaults and logic to match old FE by @amyeroberts in #20065\n* Update modeling\\_tf\\_utils.py by @cakiki in #20076\n* Update hub.py by @cakiki in #20075\n* [Doctest] Add configuration\\_dpr.py by @Saad135 in #20080\n* Removing RobertaConfig inheritance from CamembertConfig by @Saad135 in #20059\n* Skip 2 tests in `VisionTextDualEncoderProcessorTest` by @ydshieh in #20098\n* Replace unsupported facebookresearch/bitsandbytes by @tomaarsen in #20093\n* docs: Resolve many typos in the English docs by @tomaarsen in #20088\n* use huggingface\\_hub.model\\_inifo() to get pipline\\_tag by @y-tag in #20077\n* Fix `generate_dummy_inputs` for `ImageGPTOnnxConfig` by @ydshieh in #20103\n* docs: Fixed variables in f-strings by @tomaarsen in #20087\n* Add new terms to the glossary by @stevhliu in #20051\n* Replace awkward timm link with the expected one by @tomaarsen in #20109\n* Fix AutoTokenizer with subfolder passed by @sgugger in #20110\n* [Audio Processor] Only pass sr to feat extractor by @sanchit-gandhi in #20022\n* Update github pr docs actions by @mishig25 in #20125\n* Adapt has\\_labels test when no labels were found by @sgugger in #20113\n* Improve tiny model creation script by @ydshieh in #20119\n* Remove BertConfig inheritance from RobertaConfig by @Saad135 in #20124\n* [Swin] Add Swin SimMIM checkpoints by @NielsRogge in #20034\n* Update `CLIPSegModelTester` by @ydshieh in #20134\n* Update SwinForMaskedImageModeling doctest values by @amyeroberts in #20139\n* Attempting to test automatically the `_keys_to_ignore`. by @Narsil in #20042\n* Generate: move generation\\_*.py src files into generation/*.py by @gante in #20096\n* add cv + audio labels by @stevhliu in #20114\n* Update VisionEncoderDecoder to use an image processor by @amyeroberts in #20137\n* [CLIPSeg] Add resources by @NielsRogge in #20118\n* Make DummyObject more robust by @mariosasko in #20146\n* Add `RoCBertTokenizer` to `TOKENIZER_MAPPING_NAMES` by @ydshieh in #20141\n* Adding support for LayoutLMvX variants for `object-detection`. by @Narsil in #20143\n* Add doc tests by @NielsRogge in #20158\n* doc comment fix: Args was in wrong place by @hollance in #20164\n* Update `OnnxConfig.generate_dummy_inputs` to check `ImageProcessingMixin` by @ydshieh in #20157\n* Generate: fix TF doctests by @gante in #20159\n* Fix arg names for our models by @Rocketknight1 in #20166\n* [processor] Add 'model input names' property by @sanchit-gandhi in #20117\n* Fix object-detection bug (height, width inversion). by @Narsil in #20167\n* [OWL-ViT] Make model consistent with CLIP by @NielsRogge in #20144\n* Fix type - update any PIL.Image.Resampling by @amyeroberts in #20172\n* Fix tapas scatter by @Bearnardd in #20149\n* Update README.md by @code-with-rajeev in #19530\n* Proposal Remove the weird `inspect` in ASR pipeline and make WhisperEncoder just nice to use. by @Narsil in #19571\n* Pytorch type hints by @IMvision12 in #20112\n* Generate: TF sample doctest result update by @gante in #20208\n* [ROC\\_BERT] Make CI happy by @younesbelkada in #20175\n* add \\_keys\\_to\\_ignore\\_on\\_load\\_unexpected = [r\"pooler\"] by @ArthurZucker in #20210\n* docs: translated index page to korean by @wonhyeongseo in #20180\n* feat: add i18n issue template by @wonhyeongseo in #20199\n* [Examples] Generalise Seq2Seq ASR to handle Whisper by @sanchit-gandhi in #19519\n* mark `test_save_load_fast_init_from_base` as `is_flaky` by @ydshieh in #20200\n* Update README.md by @Nietism in #20188\n* Downgrade log warning -> info by @amyeroberts in #20202\n* Generate: add Bloom fixes for contrastive search by @gante in #20213\n* Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm. by @Narsil in #20104\n* [docs] set overflowing image width to auto-scale by @wonhyeongseo in #20197\n* Update tokenizer\\_summary.mdx by @bofenghuang in #20135\n* Make `ImageSegmentationPipelineTests` less flaky by @ydshieh in #20147\n* update relative positional embedding by @ArthurZucker in #20203\n* [WHISPER] Update modeling tests by @ArthurZucker in #20162\n* Add `accelerate` support for `ViT` family by @younesbelkada in #20174\n* Add param\\_name to size\\_dict logs & tidy by @amyeroberts in #20205\n* Add object detection + segmentation transforms by @amyeroberts in #20003\n* Typo on doctring in ElectraTokenizer by @FacerAin in #20192\n* Remove `authorized_missing_keys`in favor of \\_keys\\_to\\_ignore\\_on\\_load\\_missing by @ArthurZucker in #20228\n* Add missing ESM autoclass by @Rocketknight1 in #20177\n* fix device issue by @ydshieh in #20227\n* fixed spelling error in testing.mdx by @kasmith11 in #20220\n* Fix `run_clip.py` by @ydshieh in #20234\n* Fix docstring of CLIPTokenizer(Fast) by @TilmannR in #20233\n* Fix MaskformerFeatureExtractor by @NielsRogge in #20100\n* New logging support to \"Trainer\" Class (ClearML Logger) by @skinan in #20184\n* Enable PyTorch 1.13 by @sgugger in #20168\n* [CLIP] allow loading projection layer in vision and text model by @patil-suraj in #18962\n* Slightly alter Keras dummy loss by @Rocketknight1 in #20232\n* Add to DeBERTa resources by @Saad135 in #20155\n* Add clip resources to the transformers documentation by @ambujpawar in #20190\n* Update reqs to include min gather\\_for\\_metrics Accelerate version by @muellerzr in #20242\n* Allow trainer to return eval. loss for CLIP-like models by @ydshieh in #20214\n* Adds image-guided object detection support to OWL-ViT by @alaradirik in #20136\n* Adding `audio-classification` example in the doc. by @Narsil in #20235\n* Updating the doctest for conversational. by @Narsil in #20236\n* Adding doctest for `fill-mask` pipeline. by @Narsil in #20241\n* Adding doctest for `feature-extraction`. by @Narsil in #20240\n* Adding ASR pipeline example. by @Narsil in #20226\n* Adding doctest for document-question-answering by @Narsil in #20239\n* Adding an example for `depth-estimation` pipeline. by @Narsil in #20237\n* Complete doc migration by @mishig25 in #20267\n* Fix result saving errors of pytorch examples by @li-plus in #20276\n* Adding a doctest for `table-question-answering` pipeline. by @Narsil in #20260\n* Adding doctest for `image-segmentation` pipeline. by @Narsil in #20256\n* Adding doctest for `text2text-generation` pipeline. by @Narsil in #20261\n* Adding doctest for `text-generation` pipeline. by @Narsil in #20264\n* Add TF protein notebook to notebooks doc by @Rocketknight1 in #20271\n* Rephrasing the link. by @Narsil in #20253\n* Add Chinese-CLIP implementation by @yangapku in #20368\n* Adding doctest example for `image-classification` pipeline. by @Narsil in #20254\n* Adding doctest for `zero-shot-image-classification` pipeline. by @Narsil in #20272\n* Adding doctest for `zero-shot-classification` pipeline. by @Narsil in #20268\n* Adding doctest for `visual-question-answering` pipeline. by @Narsil in #20266\n* Adding doctest for `text-classification` pipeline. by @Narsil in #20262\n* Adding doctest for `question-answering` pipeline. by @Narsil in #20259\n* [Docs] Add resources of OpenAI GPT by @shogohida in #20084\n* Adding doctest for `image-to-text` pipeline. by @Narsil in #20257\n* Adding doctest for `token-classification` pipeline. by @Narsil in #20265\n* remaining pytorch type hints by @IMvision12 in #20217\n* Data collator for token classification pads labels column when receives pytorch tensors by @markovalexander in #20244\n* [Doctest] Add configuration\\_deformable\\_detr.py by @Saad135 in #20273\n* Fix summarization script by @muellerzr in #20286\n* [DOCTEST] Fix the documentation of RoCBert by @ArthurZucker in #20142\n* [bnb] Let's warn users when saving 8-bit models by @younesbelkada in #20282\n* Adding `zero-shot-object-detection` pipeline doctest. by @Narsil in #20274\n* Adding doctest for `object-detection` pipeline. by @Narsil in #20258\n* Image transforms functionality used instead by @amyeroberts in #20278\n* TF: add test for `PushToHubCallback` by @gante in #20231\n* Generate: general TF XLA constrastive search are now slow tests by @gante in #20277\n* Fixing the doctests failures. by @Narsil in #20294\n* set the default cache\\_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast by @sywangyi in #20289\n* Add docstrings for canine model by @raghavanone in #19457\n* Add missing report button for Example test by @ydshieh in #20293\n* refactor test by @younesbelkada in #20300\n* [Tiny model creation] deal with `ImageProcessor` by @ydshieh in #20298\n* Fix blender bot missleading doc by @ArthurZucker in #20301\n* remove two tokens that should not be suppressed by @ArthurZucker in #20302\n* [ASR Examples] Update README for Whisper by @sanchit-gandhi in #20230\n* Add padding image transformation by @amyeroberts in #19838\n* Pin TensorFlow by @sgugger in #20313\n* Add AnyPrecisionAdamW optimizer by @atturaioe in #18961\n* [Proposal] Breaking change `zero-shot-object-detection` for improved consistency. by @Narsil in #20280\n* Fix flakey test with seed by @muellerzr in #20318\n* Pin TF 2.10.1 for Push CI by @ydshieh in #20319\n* Remove double brackets by @stevhliu in #20307\n* TF: future proof our keras imports by @gante in #20317\n* organize pipelines by modality by @stevhliu in #20306\n* Fix torch device issues by @ydshieh in #20304\n* Generate: add generation config class by @gante in #20218\n* translate zh quicktour by @bfss in #20095)\n* Add Spanish translation of serialization.mdx by @donelianc in #20245\n* Add LayerScale to NAT/DiNAT by @alihassanijr in #20325\n* [Switch Transformers] Fix failing slow test by @younesbelkada in #20346\n* fix: \"BigSicence\" typo in docs by @rajrajhans in #20331\n* Generate: `model_kwargs` can also be an input to `prepare_inputs_for_generation` by @gante in #20353\n* Update Special Language Tokens for PLBART by @jordiclive in #19980\n* Add resources by @NielsRogge in #20296\n* Enhance HfArgumentParser functionality and ease of use by @konstantinjdobler in #20323\n* Add inference section to task guides by @stevhliu in #18781\n* Fix toctree for Section 3 in Spanish Documentation by @donelianc in #20360\n* Generate: shorter XLA contrastive search tests by @gante in #20354\n* revert `keys_to_ignore` for M2M100 by @younesbelkada in #20381\n* add `accelerate` support for `ESM` by @younesbelkada in #20379\n* Fix nightly runs by @sgugger in #20352\n* Optimizes DonutProcessor token2json method for speed by @michaelnation26 in #20283\n* Indicate better minimal version of PyTorch in big model inference by @sgugger in #20385\n* Fix longformer onnx broken export by @fxmarty in #20292\n* Use tiny models for ONNX tests - text modality by @lewtun in #20333\n* [ESM] fix `accelerate` tests for esmfold by @younesbelkada in #20387\n* Generate: fix plbart generation tests by @gante in #20391\n* [bloom] convert script tweaks by @stas00 in #18593\n* Fix doctest file path by @ydshieh in #20400\n* [Image Transformers] to\\_pil fix float edge cases by @patrickvonplaten in #20406\n* make daily CI happy by @younesbelkada in #20410\n* fix nasty `bnb` bug by @younesbelkada in #20408\n* change the way sentinel tokens can retrived by @raghavanone in #20373\n* [BNB] Throw `ValueError` when trying to cast or assign by @younesbelkada in #20409\n* Use updated `model_max_length` when saving tokenizers by @ydshieh in #20401\n* Add Spanish translation of pr\\_checks.mdx by @donelianc in #20339\n* fix device in longformer onnx path by @fxmarty in #20419\n* Fix ModelOutput instantiation when there is only one tuple by @sgugger in #20416\n* `accelerate` support for `OwlViT` by @younesbelkada in #20411\n* [AnyPrecisionAdamW] test fix by @stas00 in #20454\n* fix `word_to_tokens` docstring format by @SaulLu in #20450\n* Fix typo in FSMT Tokenizer by @kamalkraj in #20456\n* Fix device issues in `CLIPSegModelIntegrationTest` by @ydshieh in #20467\n* Fix links for `contrastive_loss` by @ydshieh in #20455\n* Fix doctests for audio models by @ydshieh in #20468\n* Fix ESM checkpoints for tests by @Rocketknight1 in #20436\n* More TF int dtype fixes by @Rocketknight1 in #20384\n* make tensors in function build\\_relative\\_position created on proper device instead of always on cpu by @qq775294390 in #20434\n* update cpu related doc by @sywangyi in #20444\n* with pytorch cpu only version. without --no\\_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\" by @sywangyi in #20445\n* [CLIPTokenizer] Improve warning by @patrickvonplaten in #20458\n* Replace assertions with value errors on distilbert model by @JuheonChu in #20463\n* [Doctest] Add configuration\\_fsmt.py by @sha016 in #19936\n* Replace assertion with ValueError exceptions in run\\_image\\_captioning\\_flax.py by @katiele47 in #20365\n* [FLAX] Add dtype to embedding for bert/bart/opt/t5 by @merrymercy in #20340\n* fix both failing RoCBert tests by @ArthurZucker in #20469\n* Include image processor in add-new-model-like by @amyeroberts in #20439\n* chore: add link to the video cls notebook. by @sayakpaul in #20386\n* add timeout option for deepspeed engine by @henghuiz in #20443\n* [Maskformer] Add MaskFormerSwin backbone by @NielsRogge in #20344\n* Extract warnings from CI artifacts by @ydshieh in #20474\n* Add Donut image processor by @amyeroberts in #20425\n* Fix torch meshgrid warnings by @fxmarty in #20475\n* Fix init import\\_structure sorting by @sgugger in #20477\n* extract warnings in GH workflows by @ydshieh in #20487\n* add in layer gpt2 tokenizer by @piEsposito in #20421\n* Replace assert statements with raise exceptions by @miyu386 in #20478\n* fixed small typo by @sandeepgadhwal in #20490\n* Fix documentation code to import facebook/detr-resnet-50 model by @JuanFKurucz in #20491\n* Fix disk offload for full safetensors checkpoints by @sgugger in #20497\n* [modelcard] Check for IterableDataset by @sanchit-gandhi in #20495\n* [modelcard] Set model name if empty by @sanchit-gandhi in #20496\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* remove `attention_mask` truncation in whisper by @ydshieh in #20488\n* Make `add_special_tokens` more clear by @ydshieh in #20424\n* [OPT/Galactica] Load large `galactica` models by @younesbelkada in #20390\n* Support extraction of both train and eval XLA graphs by @jeffhataws in #20492\n* fix ipex+fp32 jit trace error in ipex 1.13 by @sywangyi in #20504\n* Expected output for the test changed by @ArthurZucker in #20493\n* Fix TF nightly tests by @Rocketknight1 in #20507\n* Update doc examples feature extractor -> image processor by @amyeroberts in #20501\n* Fix Typo in Docs for GPU by @julianpollmann in #20509\n* Fix minimum version for device\\_map by @sgugger in #20489\n* Update `AutomaticSpeechRecognitionPipeline` doc example by @ydshieh in #20512\n* Add `natten` for CI by @ydshieh in #20511\n* Fix Data2VecTextForCasualLM example code documentation by @JuanFKurucz in #20510\n* Add some warning for Dynamo and enable TF32 when it's set by @sgugger in #20515\n* [modelcard] Update dataset tags by @sanchit-gandhi in #20506\n* Change Doctests CI launch time by @ydshieh in #20523\n* Fix `PLBart` doctest by @ydshieh in #20527\n* Fix `ConditionalDetrForSegmentation` doc example by @ydshieh in #20531\n* add doc for by @younesbelkada in #20525\n* Update `ZeroShotObjectDetectionPipeline` doc example by @ydshieh in #20528\n* update post\\_process\\_image\\_guided\\_detection by @fcakyon in #20521\n* QnA example: add speed metric by @sywangyi in #20522\n* Fix doctest by @NielsRogge in #20534\n* Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code by @JuanFKurucz in #20516\n* Fix link in pipeline device map by @stevhliu in #20517\n\nSignificant community contributions\n\nThe following contributors have made significant changes to the library over the last release:\n\n* @sww9370\n  + Add RocBert (#20013)\n* @IMvision12\n  + Pytorch type hints (#20112)\n  + remaining pytorch type hints (#20217)\n* @alihassanijr\n  + Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)\n  + Add LayerScale to NAT/DiNAT (#20325)\n* @bfss\n  + translate zh quicktour(#20095) (#20181)\n* @donelianc\n  + Add Spanish translation of serialization.mdx (#20245)\n  + Fix toctree for Section 3 in Spanish Documentation (#20360)\n  + Add Spanish translation of pr\\_checks.mdx (#20339)\n* @yangapku\n  + Add Chinese-CLIP implementation (#20368)",
   "author": "Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.",
   "author_list": [
     {
diff --git a/tests/resources/zenodo/7391177_pre_0_14_out.json b/tests/resources/zenodo/7391177_pre_0_14_out.json
new file mode 100644
index 00000000..2b0a4bf4
--- /dev/null
+++ b/tests/resources/zenodo/7391177_pre_0_14_out.json
@@ -0,0 +1,83 @@
+{
+  "abstract": "PyTorch 2.0 stack support\nWe are very excited by the newly announced PyTorch 2.0 stack. You can enable `torch.compile` on any of our models, and get support with the `Trainer` (and in all our PyTorch examples) by using the `torchdynamo` training argument. For instance, just add `--torchdynamo inductor` when launching those examples from the command line.\n\n\nThis API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.\n\n\nNote that to get the best performance, we recommend:\n\n\n* using an Ampere GPU (or more recent)\n* sticking to fixed shaped for now (so use `--pad_to_max_length` in our examples)\n* Repurpose torchdynamo training args towards torch.\\_dynamo by @sgugger in #20498\n\n\nAudio Spectrogram Transformer\nThe Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.\n\n\n* Add Audio Spectogram Transformer by @NielsRogge in #19981\n\n\nJukebox\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.\n\n\n* Add Jukebox model (replaces #16875) by @ArthurZucker in #17826\n\n\nSwitch Transformers\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n\n\nIt is the first MoE model supported in `transformers`, with the largest checkpoint currently available currently containing 1T parameters.\n\n\n* Add Switch transformers by @younesbelkada and @ArthurZucker in #19323\n\n\nRocBert\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\n\n* Add RocBert by @sww9370 in #20013\n\n\nCLIPSeg\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker. [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.\n\n\n* Add CLIPSeg by @NielsRogge in #20066\n\n\nNAT and DiNAT\nNAT\nNAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\n\nDiNAT\nDiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n\n\nIt extends [NAT](https://huggingface.co/docs/transformers/main/en/model_doc/nat) by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.\n\n\n* Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models by @alihassanijr in #20219\n\n\nMobileNetV2\nThe MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n\n\n* add MobileNetV2 model by @hollance in #17845\n\n\nMobileNetV1\nThe MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n\n\n* add MobileNetV1 model by @hollance in #17799\n\n\nImage processors\nImage processors replace feature extractors as the processing class for computer vision models.\n\n\nImportant changes:\n\n\n* `size` parameter is now a dictionary of `{\"height\": h, \"width\": w}`, `{\"shortest_edge\": s}`, `{\"shortest_egde\": s, \"longest_edge\": l}` instead of int or tuple.\n* Addition of `data_format` flag. You can now specify if you want your images to be returned in `\"channels_first\"` - NCHW - or `\"channels_last\"` - NHWC - format.\n* Processing flags e.g. `do_resize` can be passed directly to the `preprocess` method instead of modifying the class attribute: `image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")`\n* Leaving `return_tensors` unset will return a list of numpy arrays.\n\n\nThe classes are backwards compatible and can be created using existing feature extractor configurations - with the `size` parameter converted.\n\n\n* Add Image Processors by @amyeroberts in #19796\n* Add Donut image processor by @amyeroberts #20425\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* AutoImageProcessor by @amyeroberts in #20111\n\n\nBackbone for computer vision models\nWe're adding support for a general `AutoBackbone` class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.\n\n\n* Add AutoBackbone + ResNetBackbone by @NielsRogge in #20229\n* Improve backbone by @NielsRogge in #20380\n* [AutoBackbone] Improve API by @NielsRogge in #20407\n\n\nSupport for `safetensors` offloading\nIf the model you are using has a `safetensors` checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.\n\n\n* Safetensors offload by @sgugger in #20321\n\n\nContrastive search in the `generate` method\n* Generate: TF contrastive search with XLA support by @gante in #20050\n* Generate: contrastive search with full optional outputs by @gante in #19963\n\n\nBreaking changes\n* 🚨 🚨 🚨 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in `convert_tokens_to_string` by @beneyal in #15775\n\n\nBugfixes and improvements\n* add dataset by @stevhliu in #20005\n* Add BERT resources by @stevhliu in #19852\n* Add LayoutLMv3 resource by @stevhliu in #19932\n* fix typo by @stevhliu in #20006\n* Update object detection pipeline to use post\\_process\\_object\\_detection methods by @alaradirik in #20004\n* clean up vision/text config dict arguments by @ydshieh in #19954\n* make sentencepiece import conditional in bertjapanesetokenizer by @ripose-jp in #20012\n* Fix gradient checkpoint test in encoder-decoder by @ydshieh in #20017\n* Quality by @sgugger in #20002\n* Update auto processor to check image processor created by @amyeroberts in #20021\n* [Doctest] Add configuration\\_deberta\\_v2.py by @Saad135 in #19995\n* Improve model tester by @ydshieh in #19984\n* Fix doctest by @ydshieh in #20023\n* Show installed libraries and their versions in CI jobs by @ydshieh in #20026\n* reorganize glossary by @stevhliu in #20010\n* Now supporting pathlike in pipelines too. by @Narsil in #20030\n* Add \\*\\*kwargs by @amyeroberts in #20037\n* Fix some doctests after PR 15775 by @ydshieh in #20036\n* [Doctest] Add configuration\\_camembert.py by @Saad135 in #20039\n* [Whisper Tokenizer] Make more user-friendly by @sanchit-gandhi in #19921\n* [FuturWarning] Add futur warning for LEDForSequenceClassification by @ArthurZucker in #19066\n* fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc by @sywangyi in #19891\n* Update esmfold conversion script by @Rocketknight1 in #20028\n* Fixed torch.finfo issue with torch.fx by @michaelbenayoun in #20040\n* Only resize embeddings when necessary by @sgugger in #20043\n* Speed up TF token classification postprocessing by converting complete tensors to numpy by @deutschmn in #19976\n* Fix ESM LM head test by @Rocketknight1 in #20045\n* Update README.md by @bofenghuang in #20063\n* fix `tokenizer_type` to avoid error when loading checkpoint back by @pacman100 in #20062\n* [Trainer] Fix model name in push\\_to\\_hub by @sanchit-gandhi in #20064\n* PoolformerImageProcessor defaults to match previous FE by @amyeroberts in #20048\n* change constant torch.tensor to torch.full by @MerHS in #20061\n* Update READMEs for ESMFold and add notebooks by @Rocketknight1 in #20067\n* Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2 by @jordiclive in #20068\n* Allow passing arguments to model testers for CLIP-like models by @ydshieh in #20044\n* Show installed libraries and their versions in GA jobs by @ydshieh in #20069\n* Update defaults and logic to match old FE by @amyeroberts in #20065\n* Update modeling\\_tf\\_utils.py by @cakiki in #20076\n* Update hub.py by @cakiki in #20075\n* [Doctest] Add configuration\\_dpr.py by @Saad135 in #20080\n* Removing RobertaConfig inheritance from CamembertConfig by @Saad135 in #20059\n* Skip 2 tests in `VisionTextDualEncoderProcessorTest` by @ydshieh in #20098\n* Replace unsupported facebookresearch/bitsandbytes by @tomaarsen in #20093\n* docs: Resolve many typos in the English docs by @tomaarsen in #20088\n* use huggingface\\_hub.model\\_inifo() to get pipline\\_tag by @y-tag in #20077\n* Fix `generate_dummy_inputs` for `ImageGPTOnnxConfig` by @ydshieh in #20103\n* docs: Fixed variables in f-strings by @tomaarsen in #20087\n* Add new terms to the glossary by @stevhliu in #20051\n* Replace awkward timm link with the expected one by @tomaarsen in #20109\n* Fix AutoTokenizer with subfolder passed by @sgugger in #20110\n* [Audio Processor] Only pass sr to feat extractor by @sanchit-gandhi in #20022\n* Update github pr docs actions by @mishig25 in #20125\n* Adapt has\\_labels test when no labels were found by @sgugger in #20113\n* Improve tiny model creation script by @ydshieh in #20119\n* Remove BertConfig inheritance from RobertaConfig by @Saad135 in #20124\n* [Swin] Add Swin SimMIM checkpoints by @NielsRogge in #20034\n* Update `CLIPSegModelTester` by @ydshieh in #20134\n* Update SwinForMaskedImageModeling doctest values by @amyeroberts in #20139\n* Attempting to test automatically the `_keys_to_ignore`. by @Narsil in #20042\n* Generate: move generation\\_*.py src files into generation/*.py by @gante in #20096\n* add cv + audio labels by @stevhliu in #20114\n* Update VisionEncoderDecoder to use an image processor by @amyeroberts in #20137\n* [CLIPSeg] Add resources by @NielsRogge in #20118\n* Make DummyObject more robust by @mariosasko in #20146\n* Add `RoCBertTokenizer` to `TOKENIZER_MAPPING_NAMES` by @ydshieh in #20141\n* Adding support for LayoutLMvX variants for `object-detection`. by @Narsil in #20143\n* Add doc tests by @NielsRogge in #20158\n* doc comment fix: Args was in wrong place by @hollance in #20164\n* Update `OnnxConfig.generate_dummy_inputs` to check `ImageProcessingMixin` by @ydshieh in #20157\n* Generate: fix TF doctests by @gante in #20159\n* Fix arg names for our models by @Rocketknight1 in #20166\n* [processor] Add 'model input names' property by @sanchit-gandhi in #20117\n* Fix object-detection bug (height, width inversion). by @Narsil in #20167\n* [OWL-ViT] Make model consistent with CLIP by @NielsRogge in #20144\n* Fix type - update any PIL.Image.Resampling by @amyeroberts in #20172\n* Fix tapas scatter by @Bearnardd in #20149\n* Update README.md by @code-with-rajeev in #19530\n* Proposal Remove the weird `inspect` in ASR pipeline and make WhisperEncoder just nice to use. by @Narsil in #19571\n* Pytorch type hints by @IMvision12 in #20112\n* Generate: TF sample doctest result update by @gante in #20208\n* [ROC\\_BERT] Make CI happy by @younesbelkada in #20175\n* add \\_keys\\_to\\_ignore\\_on\\_load\\_unexpected = [r\"pooler\"] by @ArthurZucker in #20210\n* docs: translated index page to korean by @wonhyeongseo in #20180\n* feat: add i18n issue template by @wonhyeongseo in #20199\n* [Examples] Generalise Seq2Seq ASR to handle Whisper by @sanchit-gandhi in #19519\n* mark `test_save_load_fast_init_from_base` as `is_flaky` by @ydshieh in #20200\n* Update README.md by @Nietism in #20188\n* Downgrade log warning -> info by @amyeroberts in #20202\n* Generate: add Bloom fixes for contrastive search by @gante in #20213\n* Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm. by @Narsil in #20104\n* [docs] set overflowing image width to auto-scale by @wonhyeongseo in #20197\n* Update tokenizer\\_summary.mdx by @bofenghuang in #20135\n* Make `ImageSegmentationPipelineTests` less flaky by @ydshieh in #20147\n* update relative positional embedding by @ArthurZucker in #20203\n* [WHISPER] Update modeling tests by @ArthurZucker in #20162\n* Add `accelerate` support for `ViT` family by @younesbelkada in #20174\n* Add param\\_name to size\\_dict logs & tidy by @amyeroberts in #20205\n* Add object detection + segmentation transforms by @amyeroberts in #20003\n* Typo on doctring in ElectraTokenizer by @FacerAin in #20192\n* Remove `authorized_missing_keys`in favor of \\_keys\\_to\\_ignore\\_on\\_load\\_missing by @ArthurZucker in #20228\n* Add missing ESM autoclass by @Rocketknight1 in #20177\n* fix device issue by @ydshieh in #20227\n* fixed spelling error in testing.mdx by @kasmith11 in #20220\n* Fix `run_clip.py` by @ydshieh in #20234\n* Fix docstring of CLIPTokenizer(Fast) by @TilmannR in #20233\n* Fix MaskformerFeatureExtractor by @NielsRogge in #20100\n* New logging support to \"Trainer\" Class (ClearML Logger) by @skinan in #20184\n* Enable PyTorch 1.13 by @sgugger in #20168\n* [CLIP] allow loading projection layer in vision and text model by @patil-suraj in #18962\n* Slightly alter Keras dummy loss by @Rocketknight1 in #20232\n* Add to DeBERTa resources by @Saad135 in #20155\n* Add clip resources to the transformers documentation by @ambujpawar in #20190\n* Update reqs to include min gather\\_for\\_metrics Accelerate version by @muellerzr in #20242\n* Allow trainer to return eval. loss for CLIP-like models by @ydshieh in #20214\n* Adds image-guided object detection support to OWL-ViT by @alaradirik in #20136\n* Adding `audio-classification` example in the doc. by @Narsil in #20235\n* Updating the doctest for conversational. by @Narsil in #20236\n* Adding doctest for `fill-mask` pipeline. by @Narsil in #20241\n* Adding doctest for `feature-extraction`. by @Narsil in #20240\n* Adding ASR pipeline example. by @Narsil in #20226\n* Adding doctest for document-question-answering by @Narsil in #20239\n* Adding an example for `depth-estimation` pipeline. by @Narsil in #20237\n* Complete doc migration by @mishig25 in #20267\n* Fix result saving errors of pytorch examples by @li-plus in #20276\n* Adding a doctest for `table-question-answering` pipeline. by @Narsil in #20260\n* Adding doctest for `image-segmentation` pipeline. by @Narsil in #20256\n* Adding doctest for `text2text-generation` pipeline. by @Narsil in #20261\n* Adding doctest for `text-generation` pipeline. by @Narsil in #20264\n* Add TF protein notebook to notebooks doc by @Rocketknight1 in #20271\n* Rephrasing the link. by @Narsil in #20253\n* Add Chinese-CLIP implementation by @yangapku in #20368\n* Adding doctest example for `image-classification` pipeline. by @Narsil in #20254\n* Adding doctest for `zero-shot-image-classification` pipeline. by @Narsil in #20272\n* Adding doctest for `zero-shot-classification` pipeline. by @Narsil in #20268\n* Adding doctest for `visual-question-answering` pipeline. by @Narsil in #20266\n* Adding doctest for `text-classification` pipeline. by @Narsil in #20262\n* Adding doctest for `question-answering` pipeline. by @Narsil in #20259\n* [Docs] Add resources of OpenAI GPT by @shogohida in #20084\n* Adding doctest for `image-to-text` pipeline. by @Narsil in #20257\n* Adding doctest for `token-classification` pipeline. by @Narsil in #20265\n* remaining pytorch type hints by @IMvision12 in #20217\n* Data collator for token classification pads labels column when receives pytorch tensors by @markovalexander in #20244\n* [Doctest] Add configuration\\_deformable\\_detr.py by @Saad135 in #20273\n* Fix summarization script by @muellerzr in #20286\n* [DOCTEST] Fix the documentation of RoCBert by @ArthurZucker in #20142\n* [bnb] Let's warn users when saving 8-bit models by @younesbelkada in #20282\n* Adding `zero-shot-object-detection` pipeline doctest. by @Narsil in #20274\n* Adding doctest for `object-detection` pipeline. by @Narsil in #20258\n* Image transforms functionality used instead by @amyeroberts in #20278\n* TF: add test for `PushToHubCallback` by @gante in #20231\n* Generate: general TF XLA constrastive search are now slow tests by @gante in #20277\n* Fixing the doctests failures. by @Narsil in #20294\n* set the default cache\\_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast by @sywangyi in #20289\n* Add docstrings for canine model by @raghavanone in #19457\n* Add missing report button for Example test by @ydshieh in #20293\n* refactor test by @younesbelkada in #20300\n* [Tiny model creation] deal with `ImageProcessor` by @ydshieh in #20298\n* Fix blender bot missleading doc by @ArthurZucker in #20301\n* remove two tokens that should not be suppressed by @ArthurZucker in #20302\n* [ASR Examples] Update README for Whisper by @sanchit-gandhi in #20230\n* Add padding image transformation by @amyeroberts in #19838\n* Pin TensorFlow by @sgugger in #20313\n* Add AnyPrecisionAdamW optimizer by @atturaioe in #18961\n* [Proposal] Breaking change `zero-shot-object-detection` for improved consistency. by @Narsil in #20280\n* Fix flakey test with seed by @muellerzr in #20318\n* Pin TF 2.10.1 for Push CI by @ydshieh in #20319\n* Remove double brackets by @stevhliu in #20307\n* TF: future proof our keras imports by @gante in #20317\n* organize pipelines by modality by @stevhliu in #20306\n* Fix torch device issues by @ydshieh in #20304\n* Generate: add generation config class by @gante in #20218\n* translate zh quicktour by @bfss in #20095)\n* Add Spanish translation of serialization.mdx by @donelianc in #20245\n* Add LayerScale to NAT/DiNAT by @alihassanijr in #20325\n* [Switch Transformers] Fix failing slow test by @younesbelkada in #20346\n* fix: \"BigSicence\" typo in docs by @rajrajhans in #20331\n* Generate: `model_kwargs` can also be an input to `prepare_inputs_for_generation` by @gante in #20353\n* Update Special Language Tokens for PLBART by @jordiclive in #19980\n* Add resources by @NielsRogge in #20296\n* Enhance HfArgumentParser functionality and ease of use by @konstantinjdobler in #20323\n* Add inference section to task guides by @stevhliu in #18781\n* Fix toctree for Section 3 in Spanish Documentation by @donelianc in #20360\n* Generate: shorter XLA contrastive search tests by @gante in #20354\n* revert `keys_to_ignore` for M2M100 by @younesbelkada in #20381\n* add `accelerate` support for `ESM` by @younesbelkada in #20379\n* Fix nightly runs by @sgugger in #20352\n* Optimizes DonutProcessor token2json method for speed by @michaelnation26 in #20283\n* Indicate better minimal version of PyTorch in big model inference by @sgugger in #20385\n* Fix longformer onnx broken export by @fxmarty in #20292\n* Use tiny models for ONNX tests - text modality by @lewtun in #20333\n* [ESM] fix `accelerate` tests for esmfold by @younesbelkada in #20387\n* Generate: fix plbart generation tests by @gante in #20391\n* [bloom] convert script tweaks by @stas00 in #18593\n* Fix doctest file path by @ydshieh in #20400\n* [Image Transformers] to\\_pil fix float edge cases by @patrickvonplaten in #20406\n* make daily CI happy by @younesbelkada in #20410\n* fix nasty `bnb` bug by @younesbelkada in #20408\n* change the way sentinel tokens can retrived by @raghavanone in #20373\n* [BNB] Throw `ValueError` when trying to cast or assign by @younesbelkada in #20409\n* Use updated `model_max_length` when saving tokenizers by @ydshieh in #20401\n* Add Spanish translation of pr\\_checks.mdx by @donelianc in #20339\n* fix device in longformer onnx path by @fxmarty in #20419\n* Fix ModelOutput instantiation when there is only one tuple by @sgugger in #20416\n* `accelerate` support for `OwlViT` by @younesbelkada in #20411\n* [AnyPrecisionAdamW] test fix by @stas00 in #20454\n* fix `word_to_tokens` docstring format by @SaulLu in #20450\n* Fix typo in FSMT Tokenizer by @kamalkraj in #20456\n* Fix device issues in `CLIPSegModelIntegrationTest` by @ydshieh in #20467\n* Fix links for `contrastive_loss` by @ydshieh in #20455\n* Fix doctests for audio models by @ydshieh in #20468\n* Fix ESM checkpoints for tests by @Rocketknight1 in #20436\n* More TF int dtype fixes by @Rocketknight1 in #20384\n* make tensors in function build\\_relative\\_position created on proper device instead of always on cpu by @qq775294390 in #20434\n* update cpu related doc by @sywangyi in #20444\n* with pytorch cpu only version. without --no\\_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\" by @sywangyi in #20445\n* [CLIPTokenizer] Improve warning by @patrickvonplaten in #20458\n* Replace assertions with value errors on distilbert model by @JuheonChu in #20463\n* [Doctest] Add configuration\\_fsmt.py by @sha016 in #19936\n* Replace assertion with ValueError exceptions in run\\_image\\_captioning\\_flax.py by @katiele47 in #20365\n* [FLAX] Add dtype to embedding for bert/bart/opt/t5 by @merrymercy in #20340\n* fix both failing RoCBert tests by @ArthurZucker in #20469\n* Include image processor in add-new-model-like by @amyeroberts in #20439\n* chore: add link to the video cls notebook. by @sayakpaul in #20386\n* add timeout option for deepspeed engine by @henghuiz in #20443\n* [Maskformer] Add MaskFormerSwin backbone by @NielsRogge in #20344\n* Extract warnings from CI artifacts by @ydshieh in #20474\n* Add Donut image processor by @amyeroberts in #20425\n* Fix torch meshgrid warnings by @fxmarty in #20475\n* Fix init import\\_structure sorting by @sgugger in #20477\n* extract warnings in GH workflows by @ydshieh in #20487\n* add in layer gpt2 tokenizer by @piEsposito in #20421\n* Replace assert statements with raise exceptions by @miyu386 in #20478\n* fixed small typo by @sandeepgadhwal in #20490\n* Fix documentation code to import facebook/detr-resnet-50 model by @JuanFKurucz in #20491\n* Fix disk offload for full safetensors checkpoints by @sgugger in #20497\n* [modelcard] Check for IterableDataset by @sanchit-gandhi in #20495\n* [modelcard] Set model name if empty by @sanchit-gandhi in #20496\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* remove `attention_mask` truncation in whisper by @ydshieh in #20488\n* Make `add_special_tokens` more clear by @ydshieh in #20424\n* [OPT/Galactica] Load large `galactica` models by @younesbelkada in #20390\n* Support extraction of both train and eval XLA graphs by @jeffhataws in #20492\n* fix ipex+fp32 jit trace error in ipex 1.13 by @sywangyi in #20504\n* Expected output for the test changed by @ArthurZucker in #20493\n* Fix TF nightly tests by @Rocketknight1 in #20507\n* Update doc examples feature extractor -> image processor by @amyeroberts in #20501\n* Fix Typo in Docs for GPU by @julianpollmann in #20509\n* Fix minimum version for device\\_map by @sgugger in #20489\n* Update `AutomaticSpeechRecognitionPipeline` doc example by @ydshieh in #20512\n* Add `natten` for CI by @ydshieh in #20511\n* Fix Data2VecTextForCasualLM example code documentation by @JuanFKurucz in #20510\n* Add some warning for Dynamo and enable TF32 when it's set by @sgugger in #20515\n* [modelcard] Update dataset tags by @sanchit-gandhi in #20506\n* Change Doctests CI launch time by @ydshieh in #20523\n* Fix `PLBart` doctest by @ydshieh in #20527\n* Fix `ConditionalDetrForSegmentation` doc example by @ydshieh in #20531\n* add doc for by @younesbelkada in #20525\n* Update `ZeroShotObjectDetectionPipeline` doc example by @ydshieh in #20528\n* update post\\_process\\_image\\_guided\\_detection by @fcakyon in #20521\n* QnA example: add speed metric by @sywangyi in #20522\n* Fix doctest by @NielsRogge in #20534\n* Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code by @JuanFKurucz in #20516\n* Fix link in pipeline device map by @stevhliu in #20517\n\n\nSignificant community contributions\nThe following contributors have made significant changes to the library over the last release:\n\n\n* @sww9370\n\t+ Add RocBert (#20013)\n* @IMvision12\n\t+ Pytorch type hints (#20112)\n\t+ remaining pytorch type hints (#20217)\n* @alihassanijr\n\t+ Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)\n\t+ Add LayerScale to NAT/DiNAT (#20325)\n* @bfss\n\t+ translate zh quicktour(#20095) (#20181)\n* @donelianc\n\t+ Add Spanish translation of serialization.mdx (#20245)\n\t+ Fix toctree for Section 3 in Spanish Documentation (#20360)\n\t+ Add Spanish translation of pr\\_checks.mdx (#20339)\n* @yangapku\n\t+ Add Chinese-CLIP implementation (#20368)",
+  "author": "Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.",
+  "author_list": [
+    {
+      "family": "Wolf",
+      "given": "Thomas"
+    },
+    {
+      "family": "Debut",
+      "given": "Lysandre"
+    },
+    {
+      "family": "Sanh",
+      "given": "Victor"
+    },
+    {
+      "family": "Chaumond",
+      "given": "Julien"
+    },
+    {
+      "family": "Delangue",
+      "given": "Clement"
+    },
+    {
+      "family": "Moi",
+      "given": "Anthony"
+    },
+    {
+      "family": "Cistac",
+      "given": "Perric"
+    },
+    {
+      "family": "Ma",
+      "given": "Clara"
+    },
+    {
+      "family": "Jernite",
+      "given": "Yacine"
+    },
+    {
+      "family": "Plu",
+      "given": "Julien"
+    },
+    {
+      "family": "Xu",
+      "given": "Canwen"
+    },
+    {
+      "family": "Le Scao",
+      "given": "Teven"
+    },
+    {
+      "family": "Gugger",
+      "given": "Sylvain"
+    },
+    {
+      "family": "Drame",
+      "given": "Mariama"
+    },
+    {
+      "family": "Lhoest",
+      "given": "Quentin"
+    },
+    {
+      "family": "Rush",
+      "given": "Alexander M."
+    }
+  ],
+  "day": 1,
+  "doi": "10.5281/zenodo.7391177",
+  "eprint": 7391177,
+  "license": "other-open",
+  "month": 10,
+  "note": "If you use this software, please cite it using these metadata.",
+  "pubstate": "published",
+  "revision": 3,
+  "title": "Transformers: State-of-the-Art Natural Language Processing",
+  "type": "software",
+  "url": "https://zenodo.org/api/records/7391177",
+  "version": "v4.25.1",
+  "year": 2020
+}
\ No newline at end of file
diff --git a/tests/test_zenodo.py b/tests/test_zenodo.py
index 618c81d2..ef21d304 100644
--- a/tests/test_zenodo.py
+++ b/tests/test_zenodo.py
@@ -4,19 +4,28 @@ from papis.testing import TemporaryLibrary, ResourceCache
 
 
 @pytest.mark.resource_setup(cachedir="resources/zenodo")
+@pytest.mark.parametrize("markdownify", [True, False])
 @pytest.mark.parametrize("zenodo_id", ["7391177", "10794563"])
 def test_zenodo_id_to_data(tmp_library: TemporaryLibrary,
                            resource_cache: ResourceCache,
                            monkeypatch: pytest.MonkeyPatch,
+                           markdownify: bool,
                            zenodo_id: str) -> None:
-    # NOTE: the functionality doesn't require markdownify, but the output files
-    # are formatted using it so the test would fail otherwise.
-    pytest.importorskip("markdownify")
-
     import papis.zenodo
 
-    infile = "{}.json".format(zenodo_id)
-    outfile = "{}_out.json".format(zenodo_id)
+    if markdownify:
+        from importlib.metadata import version
+
+        pytest.importorskip("markdownify")
+        v = "_" if version("markdownify") >= "0.14" else "_pre_0_14_"
+    else:
+        v = "_html_"
+        monkeypatch.setattr(
+            papis.zenodo, "_get_text_from_html",
+            lambda html: html)
+
+    infile = f"{zenodo_id}.json"
+    outfile = f"{zenodo_id}{v}out.json"
 
     monkeypatch.setattr(
         papis.zenodo, "_get_zenodo_response",
